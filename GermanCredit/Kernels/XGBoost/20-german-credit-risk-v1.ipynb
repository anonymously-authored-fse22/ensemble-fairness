{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_credit = pd.read_csv(\"../input/germany-credit/german_credit_data.csv\",index_col=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_credit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_credit[\"Credit_amount_log\"]=np.log(df_credit[\"Credit amount\"])\ndf_credit[\"Credit_amount_log\"].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nt1 = df_credit.groupby([\"Sex\",\"Risk\"])\nt2 = df_credit.groupby([\"Sex\"])\nt3 = t1[[\"Risk\"]].count()/t2[[\"Risk\"]].count()*100\nt3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t1 = df_credit.groupby([\"Job\",\"Risk\"])\nt2 = df_credit.groupby([\"Job\"])\nt3 = t1[[\"Risk\"]].count()/t2[[\"Risk\"]].count()*100\nt3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t1 = df_credit.groupby([\"Housing\",\"Risk\"])\nt2 = df_credit.groupby([\"Housing\"])\nt3 = t1[[\"Risk\"]].count()/t2[[\"Risk\"]].count()*100\nt3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t1 = df_credit.groupby([\"Checking account\",\"Risk\"])\nt2 = df_credit.groupby([\"Checking account\"])\nt3 = t1[[\"Risk\"]].count()/t2[[\"Risk\"]].count()*100\nt3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def drawscatter(df_train,x,y):\n    import matplotlib.pyplot as plt\n    fig, ax = plt.subplots()\n    ax.scatter(x = df_train[x], y = df_train[y])\n    plt.ylabel('y', fontsize=13)\n    plt.xlabel('x', fontsize=13)\n    plt.show()\n\ndef drawHist(df_train,x,y):\n    df_train[x].hist(by=df_credit[y])\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drawHist(df_credit,\"Age\",\"Risk\")\ndrawHist(df_credit,\"Sex\",\"Risk\")\ndrawHist(df_credit,\"Job\",\"Risk\")\ndrawHist(df_credit,\"Housing\",\"Risk\")\ndrawHist(df_credit,\"Saving accounts\",\"Risk\")\ndrawHist(df_credit,\"Purpose\",\"Risk\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drawHist(df_credit,\"Credit amount\",\"Risk\")\n#drawHist(df_credit,\"Duration\",\"Risk\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_credit_2=df_credit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check null values\ndef check_nulls(df_train):\n    # check distinct values for the columns that contains null value\n    # remove nan values that is total count less than 5 percent\n    for d in df_train.columns:\n        if  df_train[d].isnull().values.any():\n            print(\"column \"+d)\n            if df_train[d].dtype.kind in 'bifc':\n                df_train[d].fillna(0,inplace = True)\n            else:\n                print(\"column \"+d)\n                df_train[d].fillna(\"NULL_VALUE\", inplace = True)\n                \n                \ncheck_nulls(df_credit_2)\n\n#check \nnan_cols = [i for i in df_credit_2.columns if df_credit_2[i].isnull().any()]\nprint(nan_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check extreme values for numeric inputs\nfor d in df_credit_2.columns:\n    if df_credit_2[d].dtype.kind in 'bifc':\n        print(d)\n        print(df_credit_2[d].describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_credit_2.rename(columns = {'Credit amount':'Credit_amount'}, inplace = True) \ndf_credit_2.rename(columns = {'Saving accounts':'Saving_accounts'}, inplace = True) \ndf_credit_2.rename(columns = {'Checking account':'Checking_account'}, inplace = True) \n\n\n# According to results above  focus on Credit amount\ndrawscatter(df_credit_2,\"Credit_amount\",\"Risk\")\n# bad: lets take values less than 15000\ndrawscatter(df_credit_2.query(\"(Credit_amount<14000 and Risk=='bad') or (Credit_amount<12600 and Risk=='good') \"),\"Credit_amount\",\"Risk\")\n\n\n#check skewness\ndf_credit_2[[\"Credit_amount\",\"Risk\"]].query(\"(Credit_amount<14000 and Risk=='bad') or (Credit_amount<12600 and Risk=='good')\").hist()\n# try to make it more normally distributed\ndf_credit_2[\"Credit_amount_log\"]=np.log(df_credit[\"Credit_amount\"])\ndf_credit_2[[\"Credit_amount_log\"]].hist()\n\n#  for model Credit_amount_log will be used for predicition","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_credit_2.Age.unique()\nbins = [0, 10, 18, 25, 30, 45, 60, 120]\nlabels = [1,2,3,4,5,6,7]\ndf_credit_2['Age_group'] = pd.cut(df_credit_2['Age'], bins=bins, labels=labels)\n\ndf_credit_2[[\"Age_group\",\"Age\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_credit_2.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in df_credit_2.columns:\n    if df_credit_2[col].dtype.kind  in 'bifc':\n        print(\"** \"+col)\n\n# correlation for  numeric and vs Risk ( categorical variable) \n\ndef One_way_ANOVA(df_train):\n    lister12 = []\n    for col in df_train.columns:\n        if df_train[col].dtype.kind  in 'bifc':\n            import statsmodels.api as sm\n            from statsmodels.formula.api import ols\n            model = ols(col+' ~ Risk',data=df_train).fit()\n            table = sm.stats.anova_lm(model, typ=2)\n            #print(col)\n            print(table[\"PR(>F)\"][0])\n            if table[\"PR(>F)\"][0] < 0.05:\n                lister12.append(col)\n    return lister12\n\nOne_way_ANOVA(df_credit_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# correlation for  categorical values vs Risk ( categorical variable)  Chi-square Test of Independence\ndef cat_corr(df_train):\n    lister12 = []\n    for col in df_train.columns:\n        if df_train[col].dtype.kind not in 'bifc':\n            print(col)\n            import pandas as pd\n            confusion_matrix = pd.crosstab(df_train[col], df_train[\"Risk\"])\n            from scipy import stats\n            print(stats.chi2_contingency(confusion_matrix))\n    \n            \ncat_corr(df_credit_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(df_credit_2[\"Purpose\"], df_credit_2[\"Risk\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(df_credit_2[\"Checking_account\"], df_credit_2[\"Risk\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"pd.crosstab(df_credit_2[\"Duration\"], df_credit_2[\"Risk\"]).apply(lambda r: r/r.sum(), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# label encoder\ndef  labeler(df):\n    from sklearn.preprocessing import LabelEncoder\n    for col in df.columns:\n        if df[col].dtype.kind not in 'bifc':\n                lb_make = LabelEncoder()\n                df[col] = lb_make.fit_transform(df[col])\n                \nlabeler(df_credit_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_credit_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_credit_2.Risk.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\nenc = OneHotEncoder(handle_unknown='ignore')\n\nenc_df = pd.DataFrame(enc.fit_transform(df_credit_2[['Sex']]).toarray())\nenc_df.columns = enc.get_feature_names(['Sex'])\n# merge with main df bridge_df on key values\ndf_credit_2 = df_credit_2.join(enc_df)\n\n\nenc_df = pd.DataFrame(enc.fit_transform(df_credit_2[['Housing']]).toarray())\nenc_df.columns = enc.get_feature_names(['Housing'])\n# merge with main df bridge_df on key values\ndf_credit_2 = df_credit_2.join(enc_df)\n\nenc_df = pd.DataFrame(enc.fit_transform(df_credit_2[['Saving_accounts']]).toarray())\nenc_df.columns = enc.get_feature_names(['Saving_accounts'])\n# merge with main df bridge_df on key values\ndf_credit_2 = df_credit_2.join(enc_df)\n\nenc_df = pd.DataFrame(enc.fit_transform(df_credit_2[['Checking_account']]).toarray())\nenc_df.columns = enc.get_feature_names(['Checking_account'])\n# merge with main df bridge_df on key values\ndf_credit_2 = df_credit_2.join(enc_df)\n\n\n\n\ndf_credit_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_credit_2.Duration.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_credit_2.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# gather cont. and categorical variable that have a strong relation with Risk\n\ndf_credit_3=df_credit_2[['Age', 'Duration', 'Credit_amount_log',       \n        'Sex_0', 'Sex_1', 'Housing_0', 'Housing_1', 'Housing_2',\n       'Saving_accounts_0', 'Saving_accounts_1', 'Saving_accounts_2',\n       'Saving_accounts_3', 'Saving_accounts_4', 'Checking_account_0',\n       'Checking_account_1', 'Checking_account_2', 'Checking_account_3',\"Age_group\"]]\ndf_credit_4=df_credit_2[\"Risk\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using cross-validation for splitting train data into two sets\n\nfrom sklearn.model_selection import KFold # import KFold\n\ncv = KFold(n_splits=5, random_state=42, shuffle=True)\n\nfor train_index, test_index in cv.split(df_credit_3):\n    X_train, X_test, y_train, y_test = df_credit_3.iloc[train_index], df_credit_3.iloc[test_index], df_credit_4.iloc[train_index], df_credit_4.iloc[test_index]\n    \n    import xgboost as xgb\n    from sklearn.metrics import mean_squared_error\n    from math import sqrt\n    from sklearn.metrics import r2_score\n    \n    params= {\n    # Parameters that we are going to tune.\n    'n_estimators':100,\n    'max_depth':6, #Result of tuning with CV\n    'eta':0.01\n    }\n    \n    \n    model = xgb.XGBClassifier()\n    \n    model.fit(X_train, y_train)\n    xgb_pred = (model.predict(X_test))\n    \n    #print(xgb_pred)\n    \n\n    \n    \n    from sklearn.metrics import accuracy_score\n    accuracy=round(accuracy_score( y_test , xgb_pred ) * 100, 2)            \n    print(accuracy)\n   ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}