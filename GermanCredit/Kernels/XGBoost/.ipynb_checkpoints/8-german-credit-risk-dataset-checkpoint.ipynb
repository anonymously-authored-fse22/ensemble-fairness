{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict, Counter\n",
    "from matplotlib.pyplot import plot\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#names of 21 columns\n",
    "cols = ['existingchecking', 'duration', 'credithistory', 'purpose', 'creditamount', \n",
    "         'savings', 'employmentsince', 'installmentrate', 'statussex', 'otherdebtors', \n",
    "         'residencesince', 'property', 'age', 'otherinstallmentplans', 'housing', \n",
    "         'existingcredits', 'job', 'peopleliable', 'telephone', 'foreignworker', 'classification']\n",
    "\n",
    "#load the dataset\n",
    "data = pd.read_csv('/kaggle/input/germancreditdata/german.data', names = cols, delimiter=' ')\n",
    "\n",
    "# preprocess numerical features\n",
    "num_features = ['creditamount', 'duration', 'installmentrate', 'residencesince', 'age', \n",
    "           'existingcredits', 'peopleliable']\n",
    "\n",
    "# standardization\n",
    "data[num_features] = StandardScaler().fit_transform(data[num_features])\n",
    "\n",
    "#preprocess categorical features\n",
    "cat_features = ['existingchecking', 'credithistory', 'purpose', 'savings', 'employmentsince',\n",
    "           'statussex', 'otherdebtors', 'property', 'otherinstallmentplans', 'housing', 'job', \n",
    "           'telephone', 'foreignworker']\n",
    "\n",
    "# one-hot encoding each of every categorical features\n",
    "data = pd.get_dummies(data, columns = cat_features)\n",
    "\n",
    "# features and target set\n",
    "x = data.drop('classification', axis = 1)\n",
    "# replace targets with 1=good, 0=bad\n",
    "data.classification.replace([1,2], [1,0], inplace=True)\n",
    "y = data.classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split features and target\n",
    "x = data.drop('classification', axis = 1)\n",
    "y = data.classification\n",
    "print('x.shape:', x.shape, '\\ny.shape:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "f = plt.figure(figsize=(20, 20))\n",
    "plt.matshow(x.corr(), fignum=f.number)\n",
    "plt.xticks(range(x.shape[1]), fontsize=14, rotation=45)\n",
    "plt.yticks(range(x.shape[1]), fontsize=14)\n",
    "cb = plt.colorbar()\n",
    "cb.ax.tick_params(labelsize=14)\n",
    "plt.title('Correlation Matrix', fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principal component analysis\n",
    "cov_mat = np.cov(x.T)\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "tot = sum(eig_vals)\n",
    "var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\n",
    "print(\"1. Variance Explained\\n\", var_exp)\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "print(\"\\n\\n2. Cumulative Variance Explained by the first 50 PC\\n\", cum_var_exp[0:51])\n",
    "print(\"\\n\\n3. Percentage of Variance Explained by the first 46 PC together sums up to:\", cum_var_exp[46])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensional reduction from 61 to 46\n",
    "pca = PCA(n_components=46)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "x_pca = pd.DataFrame(data = principalComponents)\n",
    "x_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train, test set\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x_pca, y,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since its a classification problem, its important to know if data is balanced or not\n",
    "print(ytrain.value_counts())\n",
    "ytrain.value_counts().plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply resampling\n",
    "sm = SMOTE()\n",
    "xtrain_res, ytrain_res = sm.fit_sample(xtrain, ytrain)\n",
    "# Print number of 'good' credits and 'bad credits, should be fairly balanced now\n",
    "print(\"Before SMOTE\")\n",
    "unique, counts = np.unique(ytrain, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "print(\"After SMOTE\")\n",
    "unique, counts = np.unique(ytrain_res, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classifiers=[LogisticRegression(),\n",
    "             DecisionTreeClassifier(),\n",
    "             RandomForestClassifier(),\n",
    "             GradientBoostingClassifier(),\n",
    "             AdaBoostClassifier(),\n",
    "             ExtraTreesClassifier(),\n",
    "             KNeighborsClassifier(),\n",
    "             SVC(),\n",
    "             GaussianNB()]\n",
    "\n",
    "pipelines = []\n",
    "for classifier in Classifiers:\n",
    "    pipeline = make_pipeline(classifier)\n",
    "    pipelines.append(pipeline)\n",
    "\n",
    "cv_acc = []\n",
    "model_names = ['LogisticRegression','DecisionTreeClassifier','RandomForestClassifier','GradientBoostingClassifier','AdaBoostClassifier','ExtraTreesClassifier','KNeighborsClassifier','SVC','GaussianNB']\n",
    "for name, pipeline in zip(model_names,pipelines):\n",
    "    pipeline.fit(xtrain_res, ytrain_res) \n",
    "    pred = pipeline.predict(xtest)\n",
    "    cv_accuracies = cross_val_score(estimator=pipeline, X=xtrain_res, y=ytrain_res, cv=5)    \n",
    "    cv_acc.append(cv_accuracies.mean())\n",
    "    print(name)\n",
    "    print('Train acc: ', pipeline.score(xtrain_res, ytrain_res))\n",
    "    print('Test acc: ', pipeline.score(xtest, ytest))\n",
    "    print(f'CV acc: {cv_accuracies.mean()}')\n",
    "    print(classification_report(ytest, pred))\n",
    "    print('Confusion_matrix:')\n",
    "    print(f'{confusion_matrix(ytest, pred)}')\n",
    "    print('*'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter values at which cross-validation performance is maximum\n",
    "best_acc = np.argmax(cv_acc)\n",
    "best_classifier = Classifiers[best_acc]\n",
    "print(\"Best classifier: {},\\n Train accuracy: {}, Cv accuracy: {}, Test accuracy: {}\".format(best_classifier, best_classifier.score(xtrain_res, ytrain_res), cv_acc[best_acc], best_classifier.score(xtest, ytest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    'bootstrap': [False, True],\n",
    "    'class_weight': ['balanced', 'balanced_subsample', None],\n",
    "    'max_depth': [None, 3, 5, 7],\n",
    "    'max_features': ['auto', 'sqrt', 'log2', None],\n",
    "    'n_estimators': [50, 100, 200, 300, 400, 500],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [3, 5, 7]}\n",
    "\n",
    "clf = ExtraTreesClassifier()\n",
    "\n",
    "rs = GridSearchCV(clf, params, cv=5, scoring= 'accuracy')\n",
    "rs.fit(xtrain_res, ytrain_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = rs.best_estimator_\n",
    "preds = best_model.predict(xtest)\n",
    "cv_accuracies = cross_val_score(estimator=best_model, X=xtrain_res, y=ytrain_res, cv=5)    \n",
    "cv_acc = cv_accuracies.mean()\n",
    "test_acc = accuracy_score(ytest, preds)\n",
    "print(best_model)\n",
    "print('CV accuracy:', cv_acc, 'Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame(ytest)\n",
    "a.insert(1,'predictions', preds)\n",
    "a\n",
    "pd.merge(data, a.predictions, left_index=True, right_index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
