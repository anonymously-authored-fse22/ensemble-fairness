{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predicting the credit risk using Logistic Regression - \n\nThis notebook help you in predicting the credit risk or in simpler terms will help the companies to predict bad loans. Dataset contains 21 variables namely - \n1. \"age.in.years\"                                             \n2. \"credit.amount\"                                           \n3. \"credit.history\" \n4. \"creditability\"  \n5. \"duration.in.month\" \n6. \"foreign.worker\"\n7. \"housing\"  \n8. \"installment.rate.in.percentage.of.disposable.income\"\n9. \"job\"   \n10. \"number.of.existing.credits.at.this.bank\"\n11. \"number.of.people.being.liable.to.provide.maintenance.for\" \n12. \"other.debtors.or.guarantors\"\n13. \"other.installment.plans\" \n14. \"personal.status.and.sex\"\n15. \"present.employment.since\" \n16. \"present.residence.since\"\n17. \"property\"    \n18. \"purpose\"\n19. \"savings.account.and.bonds\" \n20. \"status.of.existing.checking.account\"\n21. \"telephone\"           \n\nThese are the 21 variables. Each affecting the credibility of the loan taken by the customers. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"library(scorecard)\ndata(germancredit)\nhead(germancredit)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First we have deployed the **library scorecard**, which contains the dataset named germancredit.\n\nThen using the **data()** function we called that dataset into the picture. \n\nThen using the **head()** which is used for understanding the variable and its contents. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ls(germancredit)\nstr(germancredit)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the ls() and str() command, we find the what are our variables and how it is divided into different factors. \n\nFor Example - credibility has two factors good and bad / foreign worker has two factors yes and no.\n\nUsing he view() comamnd you can see the whole dataset. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"DataFiltered <- germancredit[-which(germancredit$property ==\"unknown / no property\"),]\nls(DataFiltered)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we filter the data, as person having unknown and no property, there are always has high chances of defaulting the EMI. Therefore can be biased. \n\nUsing -which we remove the factor unknown/ no property. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ai <- woebin(DataFiltered,\"creditability\") #w.r.t bin\nai","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# What is WOE and IV? \n\nLogistic regression model is one of the most commonly used statistical technique for solving binary classification problem. It is an acceptable technique in almost all the domains. These two concepts - weight of evidence (WOE) and information value (IV) evolved from the same logistic regression technique. \n\n**What is Weight of evidence?**\nThe weight of evidence tells the predictive power of an independent variable in relation to the dependent variable. Since it evolved from credit scoring world, it is generally described as a measure of the separation of good and bad customers. \"Bad Customers\" refers to the customers who defaulted on a loan. and \"Good Customers\" refers to the customers who paid back loan.\nWeight of Evidence (WOE) helps to transform a continuous independent variable into a set of groups or bins based on similarity of dependent variable distribution i.e. number of events and non-events.\n\nWOE = ln(Distribution of goods / Distribution of bads)\n\n**How to calculate WOE?**\n* For a continuous variable, split data into 10 parts (or lesser depending on the distribution).\n* Calculate the number of goods and bads in each group (bin)\n* Calculate the % of goods and % of bads in each group.\n* Calculate WOE by taking natural log of division of % of non-events and % of events\nNote : For a categorical variable, you do not need to split the data (Ignore Step 1 and follow the remaining steps)\n\n**Fine Classing -** \nCreate 10/20 bins/groups for a continuous independent variable and then calculates WOE and IV of the variable\n**Coarse Classing -**\nCombine adjacent categories with similar WOE scores\n\n**Benefits of WOE -**\n* It can treat outliers. Suppose you have a continuous variable such as annual salary and extreme values are more than 500 million dollars. These values would be grouped to a class of (let's say 250-500 million dollars). Later, instead of using the raw values, we would be using WOE scores of each classes.\n\n* It can handle missing values as missing values can be binned separately.\n\n* Since WOE Transformation handles categorical variable so there is no need for dummy variables.\n\n* WoE transformation helps you to build strict linear relationship with log odds. Otherwise it is not easy to accomplish linear relationship using other transformation methods such as log, square-root etc. In short, if you would not use WOE transformation, you may have to try out several transformation methods to achieve this.\n\n\n**What is Information Value (IV)**\nInformation value is one of the most useful technique to select important variables in a predictive model. It helps to rank variables on the basis of their importance. The IV is calculated using the following formula :\n\nIV = Summation(%Bads - %Goods) X WOE \n\nIV                  Variable Predictiveness \nLess than 0.02      Not useful for prediction\n0.02-0.1            Weak predictive power \n0.1-0.3             Medium predictive power \n0.3-0.5             Strong predictive power \n>0.5                Too good to be true \n\n**Important Points**\n* Information value increases as bins / groups increases for an independent variable. Be careful when there are more than 20 bins as some bins may have a very few number of events and non-events.\n\n* Information value is not an optimal feature (variable) selection method when you are building a classification model other than binary logistic regression (for eg. random forest or SVM) as conditional log odds (which we predict in a logistic regression model) is highly related to the calculation of weight of evidence. In other words, it's designed mainly for binary logistic regression model. Also think this way - Random forest can detect non-linear relationship very well so selecting variables via Information Value and using them in random forest model might not produce the most accurate and robust predictive model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"woeVal <- woebin_ply(DataFiltered,ai) #cumulative WOE wrt variable\nIv_Value <- iv(woeVal,\"creditability\",positive = \"bad|1\") #Information Value\n\nIv_Value","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we filter the all the variables which has IV values less than 0.05 ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sc.dat <- var_filter(woeVal,\"creditability\",iv_limit = 0.05)\nls(sc.dat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"table(sc.dat$creditability)\ncoeff= matrix(NA,100,12)\npre<- matrix(NA,100,1)\ndat1<-sc.dat[which(sc.dat$creditability==1),]\ndat0<-sc.dat[which(sc.dat$creditability==0),]\n\nfor (i in 1:100) {\n  cal=sample.int(613,233,replace=FALSE)\n  temp<-dat0[cal,]\n  BalData<-rbind(dat1,temp)\n  # \".\" wont take dependent \n  m<-glm(BalData$creditability~.,family = \"binomial\",data = BalData)\n  coeff[i,] <- m$coefficients\n  # pre[i]<-predict.glm(m)\n}\nco.b=colMeans(coeff)\nlp.o = co.b[1]\nlp.o","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here what I have done is - \n\nFirst I have created two matrix coeff with 100 rows and 12 columns, as only 12 variable remains in the dataset after filtering the variables based on IV. \n\nThen I divided the data into two data frames i.e. dat1 which contains all the records of bad loans, and dat0 which contains all the records of good loans. \n\nThen I take out 233 records from the dat0 and put them into dat1 and call this dataset BalData as contains equal number of good and bad. Then I do logistics regression, with credibility as dependent variable and other 11 variables as independent variable. \n\nI run this 100 times by putting them in loop. So that I have 100 coefficents of 11 variable and 100 intercept, and I store all of them in coeff matrix that I have created. \n\nThen I find out all the means of the coefficients and store them in co.b. Then I find out the intercept mean and store it into lp.o. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#null Model\npo.b = exp(lp.o)/(1+exp(lp.o))\n\n#full Model\n\n# lpf = co.b[1]+ BalData[,1:12]\n\n#Predicted linear\npre <- as.numeric(co.b[1]+as.matrix(sc.dat[,1:11]) %*% as.matrix(co.b[2:12]))\n\np1b <-exp(pre)/(1+exp(pre))\n\n#loglikelyhood function \nLL.0=sc.dat$creditability*log(po.b) +( 1-sc.dat$creditability)*(log((1-po.b)))\nLL.F=sc.dat$creditability*log(p1b) +( 1-sc.dat$creditability)*(log((1-p1b)))\nR2 <- 1-mean(LL.F)/mean(LL.0)\nR2\n\nscore.var <- 911.7 - 72.13*pre\n# score= -911.7 + 72.13*log(p1b/(1-p1b))\n# length(score)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let me tell you what I have done. \n\nWe have prepared two models, null model with no variables and full model with all the model included and compared both models to to calculate the efficeincy of the full model. \n\nThen we prepared bins, and find out that higher the score more the number of people in good i.e. 0 section and less number of people in bad loans type. This is what companies means when they talk about the credit score, they find out the credit score based on the model, higher the number, higher are the chances that they fall in good loans and hence can easily get loan at good interest rate. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# install.packages(\"OneR\")\nlibrary(OneR)\n\nbin.score <- bin(score.var,nbins=10)\n# bin.scor <- bin(score.var,nbins = 10)\ntable(bin.score,sc.dat$creditability)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"3.6.3"}},"nbformat":4,"nbformat_minor":4}