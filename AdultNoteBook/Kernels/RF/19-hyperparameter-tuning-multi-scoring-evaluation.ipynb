{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Multi scoring Hyperparemeter Tuning using GCU(Generic classifier Utility)\n\nWhile finetuning hyperparameter for a specific scoring strategy there is a chances other scores dropping.\ne.g: Accuracy might be increasing but Precision or Recall or ROC dropping.\nIn order to balance the lossess in other metrics we need multiple scoring evaluation in a single graph.\n\nFor this I developed a <b><u>Generic classifier Utility</u></b> library which will display multiple scoring for different Tree based Boosting technique.\ni.e: A single library can accomodate multiple Tree Boosting technique along with multiple scoring.\n\nSource code available [here](https://github.com/KeshavShetty/kesh-utils/tree/master/KUtils/classifier) and PyPi package [here](https://pypi.org/project/kesh-utils/)\n\nFor this demo I used Adult Census Income dataset."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nprint(os.listdir(\"../input\"))\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, make_scorer, recall_score, precision_score, f1_score, roc_auc_score\n\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install statsmodels==0.10.0rc2 --pre  # Statsmodel has sme problem with factorial in latest lib","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Install the Library (Refer: https://pypi.org/project/kesh-utils/ )\n!pip install kesh-utils","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ignore the warnings if any\nimport warnings  \nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the dataset \nadult_income_df = pd.read_csv('../input/adult.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"adult_income_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Quick known cleanup for this dataset\nadult_income_df['workclass']=adult_income_df['workclass'].replace('?','Unknown') # Treat ? workclass as unknown\nadult_income_df = adult_income_df[adult_income_df['occupation'] != '?'] # Remove rows with occupation =?\nadult_income_df['native.country']=adult_income_df['native.country'].replace('?', adult_income_df['native.country'].mode()[0]) # Replace ? with mode\nadult_income_df['fnlwgt']=np.log(adult_income_df['fnlwgt']) # Convert to antural log\nadult_income_df.loc[adult_income_df['native.country']!='United-States','native.country'] = 'non_usa' # Two many category level, convert just US and Non-US","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will use Label encoder for all categorical variables\nfrom sklearn import preprocessing\n\n# encode categorical variables using Label Encoder\n# select all categorical variables\ndf_categorical = adult_income_df.select_dtypes(include=['object'])\ndf_categorical.head()\n\n# apply Label encoder to df_categorical\nle = preprocessing.LabelEncoder()\ndf_categorical = df_categorical.apply(le.fit_transform)\ndf_categorical.head()\n\n# concat df_categorical with original df\nadult_income_df = adult_income_df.drop(df_categorical.columns, axis=1)\nadult_income_df = pd.concat([adult_income_df, df_categorical], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scale the numerical features using StandardScalar\nfrom sklearn.preprocessing import StandardScaler\nnumerical_column_names = ['age','fnlwgt','education.num', 'capital.gain', 'capital.loss', 'hours.per.week']\nscaler = StandardScaler()\n\nadult_income_df[numerical_column_names] = scaler.fit_transform(\n    adult_income_df[numerical_column_names])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Final cleaned dataset \nadult_income_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare the data for model building and evaluation\nX = adult_income_df.drop('income', axis=1)\ny = adult_income_df['income'] \nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=43)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# GCU in Action\n### The method used is KUtils.classifier.single_hyperparameter_multiple_scoring_tuning()"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the custom library\nfrom KUtils.classifier import generic_classifier_utils as gcu","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We will use\n- DecisionTreeClassifier\n- RandomForestClassifier\n- XGBClassifier\n- LGBMClassifier\n\nFor scoring we will use\n\nmodel_scoring = {'F1': make_scorer(f1_score),\n    'AUC': make_scorer(roc_auc_score),\n    'Accuracy': make_scorer(accuracy_score)\n}\n\nAt a time you can send single hyper parameter and multiple scoring for hyperparameter tuning."},{"metadata":{},"cell_type":"markdown","source":"# 1. DecisionTreeClassifier() and Hyperparameter 'max_depth' with range range(3, 21, 3)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\nscores = gcu.single_hyperparameter_multiple_scoring_tuning(\n    X_train, y_train,\n    cv_folds=5, \n    hyper_parameter_name='max_depth',\n    hyper_parameter_range = range(3, 21, 3),\n    model_scoring = {'F1': make_scorer(f1_score),\n                     'AUC': make_scorer(roc_auc_score),\n                     'Accuracy': make_scorer(accuracy_score)        #  'Accuracy': make_scorer(accuracy_score),\n                    },\n    refit='AUC',\n    classifier_algo=DecisionTreeClassifier())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### In a single chart you can see which scoring is improving and which one deteriorating\n\nIn the above chart at max_depth=9 all three (AUC, Accuracy, F1) are at its best**"},{"metadata":{},"cell_type":"markdown","source":"# 2. RandonForestClassifier() and Hyperparameter 'n_estimator' with range range(5, 200, 25)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nscores = gcu.single_hyperparameter_multiple_scoring_tuning(\n    X_train, y_train,\n    cv_folds=10, \n    hyper_parameter_name='n_estimators',\n    hyper_parameter_range =range(5, 200, 25),   \n    model_scoring = {'F1': make_scorer(f1_score),\n                     'AUC': make_scorer(roc_auc_score),\n                     'Accuracy': make_scorer(accuracy_score)        #  'Accuracy': make_scorer(accuracy_score),\n                    },\n    refit='AUC',\n    classifier_algo=RandomForestClassifier(max_depth=4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. XGBClassifier() and Hyperparameter 'learning_rate' with values [0.1, 0.2, 0.3, 0.4, 0.5, 0.9]"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost.sklearn import XGBClassifier\n\nscores = gcu.single_hyperparameter_multiple_scoring_tuning(\n    X_train, y_train,\n    cv_folds=10, \n    hyper_parameter_name='learning_rate',\n    hyper_parameter_range = [0.1, 0.2, 0.3, 0.4, 0.5, 0.9],\n    model_scoring = {'F1': make_scorer(f1_score),\n                     'AUC': make_scorer(roc_auc_score),\n                     'Accuracy': make_scorer(accuracy_score)        #  'Accuracy': make_scorer(accuracy_score),\n                    },\n    refit='AUC',\n    classifier_algo=XGBClassifier(objective= 'binary:logistic'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Accuracy best at learning rate 0.3, however F1 best at 0.4"},{"metadata":{},"cell_type":"markdown","source":"# 4. lightgbm - LGBMClassifier() and Hyperparameter 'num_leaves' with values [2, 5, 10, 50, 100, 200]"},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\n\nscores = gcu.single_hyperparameter_multiple_scoring_tuning(\n    X_train, y_train,\n    cv_folds=10,\n    hyper_parameter_name='num_leaves',\n    hyper_parameter_range = [2, 5, 10, 50, 100, 200],\n    model_scoring = {'F1': make_scorer(f1_score),\n                     'AUC': make_scorer(roc_auc_score),\n                     'Accuracy': make_scorer(accuracy_score)        #  'Accuracy': make_scorer(accuracy_score),\n                    },\n    refit='Accuracy',\n    classifier_algo=lgb.LGBMClassifier(n_jobs=-1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### I will stop here. This is just a demo how to use the library for using multiple Classifer with different scoring.\n\n#### You can try to finetune for different classifier with different scoring and different hyperparamaters.\n\n### Check other methods in the library.\n"},{"metadata":{},"cell_type":"markdown","source":"### Upvote if you liked the Kernel. Leave comments if any"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}