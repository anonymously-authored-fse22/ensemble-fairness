{"cells":[{"metadata":{"_uuid":"8567bb96db89e6f6ad48c92b93786de0e5b5458e"},"cell_type":"markdown","source":"# Adult Data Income Classification Notebook"},{"metadata":{"_uuid":"4db613427d0332a54e06ff2501a0c3f4efc67b19"},"cell_type":"markdown","source":"The Goal is to predict whether a person has an income of more than 50K a year or not. This is basically a binary classification problem where a person is classified into the >50K group or <=50K group. I have used Random Forests and Decision Tree to tackle this problem. \nThe dataset is taken from the UCI Machine Learning Repository. The link to the same is the following: https://archive.ics.uci.edu/ml/datasets/census+income\n### This Notebook covers the following aspects:\n   #### 1. Data Preprocessing and Visualization\n   #### 2. Classification Task\n   #### 3. Hyperparameter Tuning\n   #### 4. Building the Final Model\n   #### Appendix - Additional Information and graphs about hyperparameter tuning of Random Forests"},{"metadata":{"_uuid":"509bdc21752d3484f22aaad5b25cfd90931eee4e"},"cell_type":"markdown","source":"### 1. Data Preprocessing and Visualization"},{"metadata":{"trusted":true,"_uuid":"8a7a8c88ddec15f9b455829f0cd7259922b3d6e7"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\ndf = pd.read_csv(\"../input/adult.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2af57ed146a138d1e77cef04149eb961228a3535"},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d815ca24cb61bc47151efda852320e06a5941844"},"cell_type":"markdown","source":"Checking for null and/or missing values"},{"metadata":{"trusted":true,"_uuid":"2919dc9d440fe44822fb131bde8bf75c71c7d33f"},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e00b12c28fd3d0a22df87ca619bfe0c2ca5ca17"},"cell_type":"code","source":"df.columns.isna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2512bc1ea7a24d76f081df23f0ce19ef8068bdd7"},"cell_type":"code","source":"df.isin(['?']).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d4d40e0464c19cd529d71723685f7d1cd0d2893"},"cell_type":"code","source":"df = df.replace('?', np.NaN)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61a793f8137baa9bb09ee3d1fdb6dcdcf6bdbaf0"},"cell_type":"code","source":"df = df.dropna()\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7df15feb89a95ab4162d6efb75572c039801e09"},"cell_type":"markdown","source":"Mapping the income labels numerically"},{"metadata":{"trusted":true,"_uuid":"5e56ad89197580e5e06149345cdd18905ed77ef7"},"cell_type":"code","source":"df['income'] = df['income'].map({'<=50K':0, '>50K':1})\ndf.income.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7aa4db8e33fbc1e3c719a5233d15c0b94bf50202"},"cell_type":"code","source":"numerical_df = df.select_dtypes(exclude=['object'])\nnumerical_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"803df3b2539640a37e31a90a4a20594f0e2d0428"},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c28711e639273e4cf952e241a777e63a5a744f2f"},"cell_type":"code","source":"plt.hist(df['age'], edgecolor='black')\nplt.title('Age Histogram')\nplt.axvline(np.mean(df['age']), color='yellow', label='average age')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4219b903bc0f7c6c4dae140406b5d6d9e497598"},"cell_type":"code","source":"age50k = df[df['income']==1].age\nagel50k = df[df['income']==0].age\n\nfig, axs = plt.subplots(2, 1)\n\naxs[0].hist(age50k, edgecolor='black')\naxs[0].set_title('Distribution of Age for Income > 50K')\n\naxs[1].hist(agel50k, edgecolor='black')\naxs[1].set_title('Distribution of Age for Income <= 50K')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7ab83681082ab47600979909a072fd3ff72f4d9"},"cell_type":"markdown","source":"#### Inferences:\n\nFor Income > 50K, Age is almost normally distributed\n\nFor Income <=50K, Age is positively skewed. More people in the 20s and 30s have income <= 50K."},{"metadata":{"trusted":true,"_uuid":"c9a0e1ac601a7c62910d677a9c03659cd7fab6e7"},"cell_type":"code","source":"df['marital.status'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd4dfd4f036760da5850318ce6af95bc73a53d97"},"cell_type":"code","source":"ax = sns.countplot(df['marital.status'], hue=df['income'])\nax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f55e5ab5c495383e5cffe51c5476b079509c6018"},"cell_type":"markdown","source":"#### Converting marital.status to 2 categories\n\nIt seems better to reduce the number of categories for marital status to better visualize the effect of marital status on income. \nWe need to convert the following into 2 distinct categories namely, \"married\" and \"single\""},{"metadata":{"trusted":true,"_uuid":"7eaa75921ac26930e914049795546892c73fd499"},"cell_type":"code","source":"df['marital.status'] = df['marital.status'].replace(['Widowed', 'Divorced', 'Separated', 'Never-married'], 'single')\n\ndf['marital.status'] = df['marital.status'].replace(['Married-spouse-absent', 'Married-civ-spouse', 'Married-AF-spouse'], 'married')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0cf8fe8a2cd969a843a218027f665a6b9ae3cdb8"},"cell_type":"code","source":"categorical_df = df.select_dtypes(include=['object'])\ncategorical_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ddddede25ba0f9c60e38acea2cdb1a2a0776c89"},"cell_type":"code","source":"sns.countplot(df['marital.status'], hue=df['income'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6a7341004406bdb9e85d940ab914ae1c865d3b2"},"cell_type":"markdown","source":"#### Inference:\n\nMarried people are more likely to earn more than 50K as income"},{"metadata":{"_uuid":"d9ab8b0ba33b6ba0d7a828d83ab00526a03d24bb"},"cell_type":"markdown","source":"#### Encoding categorical variables numerically for classification "},{"metadata":{"trusted":true,"_uuid":"693f6db0f31dd8bdfe8125111bd2d0d639fc5273"},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nenc = LabelEncoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85a7f40e5cc85b83a133ad599605f98075a31731"},"cell_type":"code","source":"ax = sns.countplot(df['income'], hue=df['race'])\nax.set_title('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be168f1c1b26a29e5a422857b0a928d56882be66"},"cell_type":"code","source":"categorical_df = categorical_df.apply(enc.fit_transform)\ncategorical_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"462d8054bc8c8fbd74be13089a11bf34b6eb13c8"},"cell_type":"code","source":"df = df.drop(categorical_df.columns, axis=1)\ndf = pd.concat([df, categorical_df], axis=1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d01116aa1ed23f005e56c57867b516625a21ebe5"},"cell_type":"code","source":"sns.factorplot(data=df, x='education', y='hours.per.week', hue='income', kind='point')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82fdcb13a2034020532af4d5c894a2dd19fc4381"},"cell_type":"code","source":"sns.FacetGrid(data=df, hue='income', size=6).map(plt.scatter, 'age', 'hours.per.week').add_legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1225e3e322220253fd1ea8a6e52a8d678114272a"},"cell_type":"markdown","source":"#### Inferences: \n    1. Maximum people between the age of 25 to 80 earn more than 50K as income\n    2. Most people which work atleast 36 to 70 hours a week earn more than 50K\n    3. Most people under the age of 20 earn less than as m50K income"},{"metadata":{"trusted":true,"_uuid":"1b15a48aab1699f655e4b35778d6d7880a3a16a0"},"cell_type":"code","source":"plt.figure(figsize=(15,12))\ncor_map = df.corr()\nsns.heatmap(cor_map, annot=True, fmt='.3f', cmap='YlGnBu')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c939708384510da05592a465bdfda230dc63f94c"},"cell_type":"markdown","source":"### 2. Classification Task"},{"metadata":{"trusted":true,"_uuid":"bf863815b8ad0edadf9687045c3d84df3661281a"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = df.drop('income', axis=1)\ny = df['income']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=24)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d4e17c620fac99032c8f68cbb0e4416f776de04"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(n_estimators=100, random_state=24)\nclf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa7a9468383309b26addf0c1b8cdb01f3da00d71"},"cell_type":"code","source":"y_pred = clf.predict(X_test)\nfrom sklearn.metrics import accuracy_score\n\nprint(\"Random Forests accuracy\", accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"770f9b75b6638b2ddb2c6f65a22079495e87fc58"},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ndtree = DecisionTreeClassifier(criterion='gini', random_state=21, max_depth=10)\n\ndtree.fit(X_train, y_train)\ntree_pred = dtree.predict(X_test)\n\nprint(\"Decision Tree accuracy: \", accuracy_score(y_test, tree_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4a891f9aa807ad385396daf1a6e2e54b99aa97e"},"cell_type":"markdown","source":"Both the Random Forest and Decision Tree return similar prediction accuracy scores. \nHowever, Random Forest is marginally better and thus, it is the selected model. \n\nWe will now optimize the Random Forest Classifier by tuning the Hyperparameters."},{"metadata":{"_uuid":"ab04e0497bc17cbe30022742d96cf333422e9975"},"cell_type":"markdown","source":"### 3. Hyperparameter Tuning\n\nThe random forest hyperparameters we will tune are the following: \n\n1. n_estimators: represents the number of trees in the forest. More trees translates to better learning from the data, however at the cost of performance. Thus, a careful consideration must be placed on what is the optimal value.\n\n2. max_features: the number of features to consider before making a split. A high value causes overfitting. Thus, an optimized value must be found.\n\n3. min_samples_leaf: the minimum number of samples needed for a node to be considered a leaf node. Increasing this value can cause underfitting. \n\nMore about this in the Appendix."},{"metadata":{"_uuid":"627401c54ad114e247b315879fb0bfc8b05505b5"},"cell_type":"markdown","source":"#### Methodology\nFirst we do a Randomized Search to narrow down the possibilites and then perform a Grid Search to further optimize the model. This approach is more suited since directly running a Grid Search is computationally intensive.\n\nI found this article about Random and Grid Search particularly useful: https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search\n\n#### 1. Randomized Search  "},{"metadata":{"trusted":true,"_uuid":"1dc681af21ea1bc3ce8beff423f3f8027dc213d3"},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, KFold\n\nn_estimators = np.arange(100, 1000, 100)\nmax_features = np.arange(1, 10, 1)\nmin_samples_leaf = np.arange(2, 10, 1)\nkfold = KFold(n_splits = 3)\nstart_grid = {\n    'n_estimators': n_estimators,\n    'max_features': max_features,\n    'min_samples_leaf': min_samples_leaf,\n    }\n\nrf = RandomForestClassifier()\n\ntest_rf = RandomizedSearchCV(estimator=rf, param_distributions=start_grid, cv=kfold)\nprint(start_grid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59c0a7a7cbb2a365aa81c67a6d26d540fa04d3b8"},"cell_type":"code","source":"'''\nCommented out since takes a long time to run. \n\n------------------------------\nOPTIMIZED PARAMETERS:\nmax_features = 3\nmin_samples_leaf = 5\nn_estimators = 100\n------------------------------\n\n\ntest_rf.fit(X_train, y_train)\ntest_rf.best_params_\n'''","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"62cd7466c6621ec87bd64446acb76dee932d4832"},"cell_type":"markdown","source":"#### 2. Grid Search"},{"metadata":{"trusted":true,"_uuid":"4a9ca7e8f6686c037560e0d05ab1735c7e38cdf0"},"cell_type":"code","source":"'''\nCommented out since takes about 25 minutes to run\n----------------------------------\nOPTIMIZED HYPERPARAMETERS:\n\nmax_features = 3\nmin_samples_leaf = 3\nn_estimators = 450\n-----------------------------------\n\nkfold_gs = KFold(n_splits=3)\nn_estimators = np.arange(100, 500, 50)\nmax_features = np.arange(1, 5, 1)\nmin_samples_leaf = np.arange(2, 5, 1)\n\ngs_grid = {\n    'n_estimators': n_estimators,\n    'max_features': max_features,\n    'min_samples_leaf': min_samples_leaf\n}\n\ntest_grid = GridSearchCV(estimator = rf, param_grid=gs_grid, cv=kfold_gs)\nres = test_grid.fit(X_train, y_train)\nprint(res.best_params_)\nprint(res.best_score_)\n'''","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e06a3b9398cfca1cd1f2576c4b35be1520e3f873"},"cell_type":"markdown","source":"### 4. Building the Model"},{"metadata":{"trusted":true,"_uuid":"88c19c4f8fbcd129aab91998d24583b7bac284d2"},"cell_type":"code","source":"final_model = RandomForestClassifier(n_estimators=450, min_samples_leaf=3, max_features=3, random_state=24)\nfinal_model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7d64215f6d6dd05645c10c8799d8cd8c26b784c"},"cell_type":"code","source":"predictions = final_model.predict(X_test)\nprint(accuracy_score(y_test, predictions))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18281b19082961e71680d949f9e754e68aae0300"},"cell_type":"markdown","source":"The previous Random Forest Classifier without tuning gave an accuracy score of 0.851\nThe tuned model gives an accuracy score of 0.86\n\nBy tuning the model, we are able to get an improvement of 0.01 or 1%."},{"metadata":{"_uuid":"ce78fbc37086ff28ac8784a15cdd38822b1f349a"},"cell_type":"markdown","source":"### Appendix\n\nThe appendix has graphs for when the hyperparameters are under-fitting or over-fitting. \nThis can be used when determining the range of values for the hyperparameters. \n\nAUC (Area Under Curve) is used as the evaluation metric. For binary classification problems, AUC is a good evaluation metric.\n\n\nThis article explains how hyperparameters should be tuned for Random Forest:\nhttps://medium.com/all-things-ai/in-depth-parameter-tuning-for-random-forest-d67bb7e920d \n\n(I have used the code from this article to visualize overfitting and underfitting in training and testing case.)"},{"metadata":{"_uuid":"7fa280903313a92ec3131828da602abd0abf1604"},"cell_type":"markdown","source":"#### n_estimators"},{"metadata":{"trusted":true,"_uuid":"394c417ef55e379033080be9a8e761a1f1953c0a"},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nn_estimators = np.arange(100, 1000, 100)\n\ntrain_results = []\ntest_results = []\nfor n_est in n_estimators:\n   rf = RandomForestClassifier(n_estimators = n_est)\n   rf.fit(X_train, y_train)\n\n   train_pred = rf.predict(X_train)\n\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   train_results.append(roc_auc)\n\n   y_pred = rf.predict(X_test)\n\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   test_results.append(roc_auc)\n\nfrom matplotlib.legend_handler import HandlerLine2D\n\nline1, = plt.plot(n_estimators, train_results, 'b', label='Train AUC')\nline2, = plt.plot(n_estimators, test_results, 'r', label='Test AUC')\n\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n\nplt.ylabel('AUC score')\nplt.xlabel('n_estimators')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4927f778725cf2d03293b32357ad8e3782317a8b"},"cell_type":"markdown","source":"#### max_features\n\nA case of overfitting. It is quite unexpected that the model is over-fitting for all values of max_features. However, the scikit-learn documentation states that until a valid parition node is not found, the splitting does not stop even if it exceeds the value of max_features features."},{"metadata":{"trusted":true,"_uuid":"92e423e43fd1369ee412d55c1338523f61fa0a7c"},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nmax_features = np.arange(1, 10, 1)\n\ntrain_results = []\ntest_results = []\nfor max_f in max_features:\n   rf = RandomForestClassifier(max_features=max_f)\n   rf.fit(X_train, y_train)\n\n   train_pred = rf.predict(X_train)\n\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   train_results.append(roc_auc)\n\n   y_pred = rf.predict(X_test)\n\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   test_results.append(roc_auc)\n\nfrom matplotlib.legend_handler import HandlerLine2D\n\nline1, = plt.plot(max_features, train_results, 'b', label='Train AUC')\nline2, = plt.plot(max_features, test_results, 'r', label='Test AUC')\n\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n\nplt.ylabel('AUC score')\nplt.xlabel('max_features')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"403d545e59d319564441e26f29f2b0aeb365220d"},"cell_type":"markdown","source":"#### min_samples_leaf\n\nThis is a case of underfitting. Increasing this value can cause underfitting."},{"metadata":{"trusted":true,"_uuid":"19ea44611c4e908934e10b461d948ab80cfcac1c"},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nmin_samples_leafs = np.arange(2, 10, 1)\n\ntrain_results = []\ntest_results = []\nfor min_samples_leaf in min_samples_leafs:\n   rf = RandomForestClassifier(min_samples_leaf=min_samples_leaf)\n   rf.fit(X_train, y_train)\n\n   train_pred = rf.predict(X_train)\n\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   train_results.append(roc_auc)\n\n   y_pred = rf.predict(X_test)\n\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   test_results.append(roc_auc)\n\nfrom matplotlib.legend_handler import HandlerLine2D\n\nline1, = plt.plot(min_samples_leafs, train_results, 'b', label='Train AUC')\nline2, = plt.plot(min_samples_leafs, test_results, 'r', label='Test AUC')\n\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n\nplt.ylabel('AUC score')\nplt.xlabel('min samples leaf')\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}