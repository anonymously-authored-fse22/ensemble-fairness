{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **From Naïve to XGBoost and ANN: Adult Census Income**\n\nIn this kernel, we will use the Adult Census Income dataset from UCI Machine Learning in order to predict if a person earns more than 50k per year or not. Therefore, this will be a binary classification problem where input data is both continous and categorical. In order to make predictions we will develop the following models:\n\n- Logistic Regression\n- Categorical Naïve Bayes\n- Gaussian Naïve Bayes\n- K-Nearest Neighbors\n- Support Vector Machines\n- Decision Trees\n- Random Forest\n- XGBoost\n- Artificial Neural Networks\n\n\nAs usual, first we will have a look at the data in order to understand the different variables, then we will clean it in order to use it for prediction purposes, and finally we will develop the different models. These models will be tested with cross validation in order to pick the best ones and use them to develop an ensemble model with the purpose of achieving more accuracy than each member of it. Then, all the models will be tested in a test set in order to determine the best one.\n","metadata":{}},{"cell_type":"markdown","source":"# **Table of Contents**\n\n- [1. Descriptive analysis](#1)\n    - [1.1 Target variable](#1.1)\n    - [1.2 Categorical variables](#1.2)\n    - [1.3 Continous variables](#1.3)\n        - [1.3.1 Correlation matrix](#1.3.1)\n- [2. Cleaning data](#2)\n    - [2.1 Drop useless variables](#2.1)\n    - [2.2 Deal with missing data](#2.2)\n- [3. Split data and get dummies](#3)\n- [4. Proposed models](#4)\n    - [4.1 Logistic Regression](#4.1)\n    - [4.2 Categorical Naïve Bayes](#4.2)\n    - [4.3 Gaussian Naïve Bayes](#4.3)\n    - [4.4 K-Nearest Neighbors](#4.4)\n    - [4.5 Support Vector Machines](#4.5)\n    - [4.6 Decision Trees](#4.6)\n    - [4.7 Random Forest](#4.7)\n    - [4.8 XGBoost](#4.8)\n    - [4.9 Artificial Neural Networks](#4.9)\n    - [4.10 Ensembling](#4.10)\n","metadata":{}},{"cell_type":"markdown","source":"## **Initial Set-Up**","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing\n\n# libraries for visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\n# split into train and test, do grid search for hyperparameter optimization and\n# do cross validation with stratified data\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n\n# data scaling\nfrom sklearn.preprocessing import scale, MinMaxScaler\n# test data\nfrom sklearn.metrics import accuracy_score\n\n# models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import CategoricalNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import svm\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport xgboost as xgb\n\n# libraries for ANN\nfrom tensorflow import keras\nimport tensorflow as tf\nfrom keras.wrappers.scikit_learn import KerasClassifier\n","metadata":{"execution":{"iopub.status.busy":"2021-08-13T22:34:29.624684Z","iopub.execute_input":"2021-08-13T22:34:29.625249Z","iopub.status.idle":"2021-08-13T22:34:29.633651Z","shell.execute_reply.started":"2021-08-13T22:34:29.625215Z","shell.execute_reply":"2021-08-13T22:34:29.632317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **1. Descriptive analysis** <a class=\"anchor\" id=\"1\"></a>\n\nIn this section we will have a first look at the data and try to understand each variable in the dataset. First of all we read the data:","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/adult-census-income/adult.csv', sep=\",\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-13T22:34:30.604072Z","iopub.execute_input":"2021-08-13T22:34:30.604424Z","iopub.status.idle":"2021-08-13T22:34:30.708604Z","shell.execute_reply.started":"2021-08-13T22:34:30.604393Z","shell.execute_reply":"2021-08-13T22:34:30.707376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-13T22:34:30.971163Z","iopub.execute_input":"2021-08-13T22:34:30.971541Z","iopub.status.idle":"2021-08-13T22:34:30.9783Z","shell.execute_reply.started":"2021-08-13T22:34:30.971512Z","shell.execute_reply":"2021-08-13T22:34:30.97698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that we have both continous and categorical data. Now we are going to study each of the 15 variables in the dataset.","metadata":{}},{"cell_type":"markdown","source":"# **1.1 Target variable** <a class=\"anchor\" id=\"1.1\"></a>","metadata":{}},{"cell_type":"markdown","source":"## **Income**","metadata":{}},{"cell_type":"code","source":"# Income is the target variable. \ndf['income'].unique() # show unique values\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-13T22:34:32.686177Z","iopub.execute_input":"2021-08-13T22:34:32.686596Z","iopub.status.idle":"2021-08-13T22:34:32.694902Z","shell.execute_reply.started":"2021-08-13T22:34:32.686562Z","shell.execute_reply":"2021-08-13T22:34:32.694116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We substitute the values for ones and zeros and count the number of samples for each value.","metadata":{}},{"cell_type":"code","source":"df['income'].replace(['<=50K','>50K'],[0,1], inplace=True) # replace for 0 and 1\ndf['income'].value_counts() # show number of samples for each value\n","metadata":{"execution":{"iopub.status.busy":"2021-08-13T22:34:33.516591Z","iopub.execute_input":"2021-08-13T22:34:33.517147Z","iopub.status.idle":"2021-08-13T22:34:33.547582Z","shell.execute_reply.started":"2021-08-13T22:34:33.517114Z","shell.execute_reply":"2021-08-13T22:34:33.546914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we can see the percentage of people that has >50k:\nnp.mean(df['income'])\n","metadata":{"execution":{"iopub.status.busy":"2021-08-13T22:34:34.395586Z","iopub.execute_input":"2021-08-13T22:34:34.396139Z","iopub.status.idle":"2021-08-13T22:34:34.402887Z","shell.execute_reply.started":"2021-08-13T22:34:34.396105Z","shell.execute_reply":"2021-08-13T22:34:34.401823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that we have an imbalanced target variable, so we will use stratification when splitting data into train and test sets and when doing cross-validation.","metadata":{}},{"cell_type":"markdown","source":"# **1.2 Categorical variables** <a class=\"anchor\" id=\"1.2\"></a>","metadata":{}},{"cell_type":"markdown","source":"## **Workclass**","metadata":{}},{"cell_type":"code","source":"df['workclass'].value_counts() \n","metadata":{"execution":{"iopub.status.busy":"2021-08-13T22:34:36.868625Z","iopub.execute_input":"2021-08-13T22:34:36.869194Z","iopub.status.idle":"2021-08-13T22:34:36.885483Z","shell.execute_reply.started":"2021-08-13T22:34:36.86916Z","shell.execute_reply":"2021-08-13T22:34:36.884152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As wee see there are 1836 missing values.","metadata":{}},{"cell_type":"code","source":"# probability of belonging to the group with the highest income\nworkclass_income = df.groupby('workclass')['income'].mean() # there is correlation as spected\n\nplt.rcParams['axes.axisbelow'] = True # grid behind graphs bars\nplt.figure(figsize=(20, 8))\nplt.ylim(0,1) # values from 0 to 1 as there are probabilities\nplt.bar(workclass_income.index.astype(str), workclass_income,\n       color = 'SkyBlue' , edgecolor='black' )\nplt.ylabel('Probability', size=20)\nplt.xlabel('Workclass', size=20)\nplt.grid(axis='y')","metadata":{"execution":{"iopub.status.busy":"2021-08-13T22:34:43.811549Z","iopub.execute_input":"2021-08-13T22:34:43.812264Z","iopub.status.idle":"2021-08-13T22:34:44.049951Z","shell.execute_reply.started":"2021-08-13T22:34:43.812209Z","shell.execute_reply":"2021-08-13T22:34:44.049077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Probabilities are what we would expect.","metadata":{}},{"cell_type":"markdown","source":"## **Education**","metadata":{}},{"cell_type":"code","source":"df['education'].unique() \n","metadata":{"execution":{"iopub.status.busy":"2021-08-13T22:34:47.089539Z","iopub.execute_input":"2021-08-13T22:34:47.089884Z","iopub.status.idle":"2021-08-13T22:34:47.102472Z","shell.execute_reply.started":"2021-08-13T22:34:47.089856Z","shell.execute_reply":"2021-08-13T22:34:47.101306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are not null values.","metadata":{}},{"cell_type":"code","source":"# probability of belonging to the group with the highest income\neducation_income = df.groupby('education')['income'].mean() # there is correlation as spected\n\nplt.figure(figsize=(20, 8))\nplt.ylim(0,1)\nplt.xticks(rotation=30) # rotate axis text\nplt.bar(education_income.index.astype(str), education_income,\n       color = 'SkyBlue', edgecolor='black' )\nplt.ylabel('Probability of earning >50k', size=20)\nplt.xlabel('Education', size=20)\nplt.grid(axis='y')\n","metadata":{"execution":{"iopub.status.busy":"2021-08-13T22:35:14.283583Z","iopub.execute_input":"2021-08-13T22:35:14.284033Z","iopub.status.idle":"2021-08-13T22:35:14.595314Z","shell.execute_reply.started":"2021-08-13T22:35:14.283985Z","shell.execute_reply":"2021-08-13T22:35:14.594251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Probabilities are what we would expect: it increases with education.","metadata":{}},{"cell_type":"markdown","source":"## **Marital.status**","metadata":{}},{"cell_type":"code","source":"df['marital.status'].unique() \n","metadata":{"execution":{"iopub.status.busy":"2021-08-13T22:35:16.399671Z","iopub.execute_input":"2021-08-13T22:35:16.400077Z","iopub.status.idle":"2021-08-13T22:35:16.409832Z","shell.execute_reply.started":"2021-08-13T22:35:16.400043Z","shell.execute_reply":"2021-08-13T22:35:16.40902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are not null values.","metadata":{}},{"cell_type":"code","source":"# probability of belonging to the group with the highest income\nmarital_income = df.groupby('marital.status')['income'].mean()\n\nplt.figure(figsize=(20, 8))\nplt.ylim(0,1)\nplt.bar(marital_income.index.astype(str), marital_income,\n       color = 'SkyBlue', edgecolor='black' )\nplt.ylabel('Probability of earning >50k', size=20)\nplt.xlabel('Marital status', size=20)\nplt.grid(axis='y')","metadata":{"execution":{"iopub.status.busy":"2021-08-13T22:35:16.972924Z","iopub.execute_input":"2021-08-13T22:35:16.973322Z","iopub.status.idle":"2021-08-13T22:35:17.206399Z","shell.execute_reply.started":"2021-08-13T22:35:16.973291Z","shell.execute_reply":"2021-08-13T22:35:17.205109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Probabilities are what we would expect. Married people has more probability than the rest.","metadata":{}},{"cell_type":"markdown","source":"## **Occupation**","metadata":{}},{"cell_type":"code","source":"df['occupation'].value_counts() \n","metadata":{"execution":{"iopub.status.busy":"2021-08-13T22:35:19.776821Z","iopub.execute_input":"2021-08-13T22:35:19.777251Z","iopub.status.idle":"2021-08-13T22:35:19.796044Z","shell.execute_reply.started":"2021-08-13T22:35:19.777216Z","shell.execute_reply":"2021-08-13T22:35:19.794519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 1843 missing values. It must be correlated with workclass","metadata":{}},{"cell_type":"code","source":"# Show null values in common\nwork_ocupation = df.loc[df['workclass'] == df['occupation'],'workclass']\nwork_ocupation.value_counts()\n","metadata":{"execution":{"iopub.status.busy":"2021-08-13T22:35:20.538539Z","iopub.execute_input":"2021-08-13T22:35:20.5389Z","iopub.status.idle":"2021-08-13T22:35:20.556851Z","shell.execute_reply.started":"2021-08-13T22:35:20.538871Z","shell.execute_reply":"2021-08-13T22:35:20.555629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we see there are 1836 null values in common.","metadata":{}},{"cell_type":"code","source":"# probability of belonging to the group with the highest income\noccupation_income = df.groupby('occupation')['income'].mean()\n\nplt.figure(figsize=(25, 8))\nplt.ylim(0,1)\nplt.xticks(rotation=30) # rotate axis text\nplt.bar(occupation_income.index.astype(str), occupation_income,\n       color = 'SkyBlue', edgecolor='black' )\nplt.ylabel('Probability of earning >50k', size=20)\nplt.xlabel('Occupation', size=20)\nplt.grid(axis='y')","metadata":{"execution":{"iopub.status.busy":"2021-08-13T22:35:21.845372Z","iopub.execute_input":"2021-08-13T22:35:21.845711Z","iopub.status.idle":"2021-08-13T22:35:22.182472Z","shell.execute_reply.started":"2021-08-13T22:35:21.845683Z","shell.execute_reply":"2021-08-13T22:35:22.181119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Probabilities are what we would expect.","metadata":{}},{"cell_type":"markdown","source":"## **Relationship**","metadata":{}},{"cell_type":"code","source":"df['relationship'].value_counts() \n","metadata":{"execution":{"iopub.status.busy":"2021-08-13T22:35:24.810107Z","iopub.execute_input":"2021-08-13T22:35:24.810504Z","iopub.status.idle":"2021-08-13T22:35:24.828719Z","shell.execute_reply.started":"2021-08-13T22:35:24.810473Z","shell.execute_reply":"2021-08-13T22:35:24.82742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are not null values.","metadata":{}},{"cell_type":"code","source":"# probability of belonging to the group with the highest income\nrelationship_income = df.groupby('relationship')['income'].mean()\n\nplt.figure(figsize=(20, 8))\nplt.ylim(0,1)\nplt.bar(relationship_income.index.astype(str), relationship_income,\n       color = 'SkyBlue', edgecolor='black')\nplt.ylabel('Probability of earning >50k', size=20)\nplt.xlabel('Relationship', size=20)\nplt.grid(axis='y')","metadata":{"execution":{"iopub.status.busy":"2021-08-13T22:35:25.691348Z","iopub.execute_input":"2021-08-13T22:35:25.691698Z","iopub.status.idle":"2021-08-13T22:35:25.895163Z","shell.execute_reply.started":"2021-08-13T22:35:25.691669Z","shell.execute_reply":"2021-08-13T22:35:25.893813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we see being married increases the probability of earning more than 50k.","metadata":{}},{"cell_type":"markdown","source":"## **Race**","metadata":{}},{"cell_type":"code","source":"df['race'].value_counts()\n","metadata":{"execution":{"iopub.status.busy":"2021-08-13T22:35:26.829297Z","iopub.execute_input":"2021-08-13T22:35:26.82971Z","iopub.status.idle":"2021-08-13T22:35:26.846787Z","shell.execute_reply.started":"2021-08-13T22:35:26.829674Z","shell.execute_reply":"2021-08-13T22:35:26.84577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"race_income = df.groupby('race')['income'].mean()\n\nplt.figure(figsize=(20, 8))\nplt.ylim(0,1)\nplt.bar(race_income.index.astype(str), race_income,\n       color = 'SkyBlue', edgecolor='black')\nplt.ylabel('Probability of earning >50k', size=20)\nplt.xlabel('Race', size=20)\nplt.grid(axis='y')","metadata":{"execution":{"iopub.status.busy":"2021-08-13T22:35:26.98566Z","iopub.execute_input":"2021-08-13T22:35:26.98613Z","iopub.status.idle":"2021-08-13T22:35:27.18862Z","shell.execute_reply.started":"2021-08-13T22:35:26.986093Z","shell.execute_reply":"2021-08-13T22:35:27.187591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that white people and Asian-Pac-Islander have the higher probabilities.","metadata":{}},{"cell_type":"markdown","source":"## **Sex**","metadata":{}},{"cell_type":"code","source":"df['sex'].value_counts()\n","metadata":{"execution":{"iopub.status.busy":"2021-08-13T22:35:33.546235Z","iopub.execute_input":"2021-08-13T22:35:33.546657Z","iopub.status.idle":"2021-08-13T22:35:33.563437Z","shell.execute_reply.started":"2021-08-13T22:35:33.546621Z","shell.execute_reply":"2021-08-13T22:35:33.56233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sex_income = df.groupby('sex')['income'].mean()\n\nplt.figure(figsize=(20, 8))\nplt.ylim(0,1)\nplt.bar(sex_income.index.astype(str), sex_income,\n       color = 'SkyBlue', edgecolor='black')\nplt.ylabel('Probability of earning >50k', size=20)\nplt.xlabel('Sex', size=20)\nplt.grid(axis='y')","metadata":{"execution":{"iopub.status.busy":"2021-08-13T22:35:34.34297Z","iopub.execute_input":"2021-08-13T22:35:34.343425Z","iopub.status.idle":"2021-08-13T22:35:34.510122Z","shell.execute_reply.started":"2021-08-13T22:35:34.34339Z","shell.execute_reply":"2021-08-13T22:35:34.50879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we see male have higher probabilities than female.","metadata":{}},{"cell_type":"markdown","source":"## **native.country**","metadata":{}},{"cell_type":"code","source":"df['native.country'].unique() \n","metadata":{"execution":{"iopub.status.busy":"2021-08-13T22:35:39.202266Z","iopub.execute_input":"2021-08-13T22:35:39.202605Z","iopub.status.idle":"2021-08-13T22:35:39.212306Z","shell.execute_reply.started":"2021-08-13T22:35:39.202578Z","shell.execute_reply":"2021-08-13T22:35:39.211232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that there are missing data.","metadata":{}},{"cell_type":"code","source":"# Show number of missing values\ndf.loc[df['native.country'] == '?', 'native.country'].count() \n","metadata":{"execution":{"iopub.status.busy":"2021-08-13T22:35:40.097876Z","iopub.execute_input":"2021-08-13T22:35:40.098293Z","iopub.status.idle":"2021-08-13T22:35:40.111308Z","shell.execute_reply.started":"2021-08-13T22:35:40.098262Z","shell.execute_reply":"2021-08-13T22:35:40.110265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show if missing values have something to do with occupation missing data\ndf.loc[df['native.country'] == 'occupation','occupation' ].count()","metadata":{"execution":{"iopub.status.busy":"2021-08-13T22:35:40.532472Z","iopub.execute_input":"2021-08-13T22:35:40.533094Z","iopub.status.idle":"2021-08-13T22:35:40.545465Z","shell.execute_reply.started":"2021-08-13T22:35:40.533041Z","shell.execute_reply":"2021-08-13T22:35:40.544392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show if missing values have something to do with workclass missing data\ndf.loc[df['native.country'] == 'workclass','workclass' ].count()","metadata":{"execution":{"iopub.status.busy":"2021-08-13T22:35:41.121592Z","iopub.execute_input":"2021-08-13T22:35:41.122035Z","iopub.status.idle":"2021-08-13T22:35:41.135567Z","shell.execute_reply.started":"2021-08-13T22:35:41.121984Z","shell.execute_reply":"2021-08-13T22:35:41.13405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we see missing data from native.country as nothing to do with missing data from occupation and workclass.  ","metadata":{}},{"cell_type":"markdown","source":"# **1.3 Continous variables** <a class=\"anchor\" id=\"1.3\"></a>","metadata":{}},{"cell_type":"markdown","source":" ## **1.3.1 Correlation matrix** <a class=\"anchor\" id=\"1.3.1\"></a>","metadata":{}},{"cell_type":"code","source":"df.info() # Show continoues variables \n","metadata":{"execution":{"iopub.status.busy":"2021-08-13T22:35:43.51664Z","iopub.execute_input":"2021-08-13T22:35:43.517262Z","iopub.status.idle":"2021-08-13T22:35:43.559265Z","shell.execute_reply.started":"2021-08-13T22:35:43.517225Z","shell.execute_reply":"2021-08-13T22:35:43.558459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Group all continous variables \ndf_continous = df[['income', 'age', 'fnlwgt', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week']]\n# Correlation matrix\nplt.figure(figsize=(15, 8))\nsns.heatmap(data=df_continous.corr(), annot=True, vmin=-1, vmax=1)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-13T22:35:44.283296Z","iopub.execute_input":"2021-08-13T22:35:44.283832Z","iopub.status.idle":"2021-08-13T22:35:44.970696Z","shell.execute_reply.started":"2021-08-13T22:35:44.283798Z","shell.execute_reply":"2021-08-13T22:35:44.968116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we see fnlwgt does not have a high correlation with income so we will drop it.","metadata":{}},{"cell_type":"markdown","source":"## **Age**","metadata":{}},{"cell_type":"code","source":"df['age'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-08-13T22:35:47.500248Z","iopub.execute_input":"2021-08-13T22:35:47.500613Z","iopub.status.idle":"2021-08-13T22:35:47.509081Z","shell.execute_reply.started":"2021-08-13T22:35:47.500581Z","shell.execute_reply":"2021-08-13T22:35:47.508229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot histogram\nplt.figure(figsize=(20, 8))\nplt.hist(df['age'],density=True, bins=20, color = 'SkyBlue')\nplt.ylabel('Probability', size=20)\nplt.xlabel('Age', size=20)\nplt.grid(axis='y')","metadata":{"execution":{"iopub.status.busy":"2021-08-13T22:35:47.832657Z","iopub.execute_input":"2021-08-13T22:35:47.833188Z","iopub.status.idle":"2021-08-13T22:35:48.100026Z","shell.execute_reply.started":"2021-08-13T22:35:47.833155Z","shell.execute_reply":"2021-08-13T22:35:48.098965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show average age by income\ndf.groupby(\"income\")[\"age\"].mean() \n","metadata":{"execution":{"iopub.status.busy":"2021-08-13T22:35:52.128625Z","iopub.execute_input":"2021-08-13T22:35:52.12905Z","iopub.status.idle":"2021-08-13T22:35:52.140092Z","shell.execute_reply.started":"2021-08-13T22:35:52.129018Z","shell.execute_reply":"2021-08-13T22:35:52.139279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"People with more than 50k are on average older.","metadata":{}},{"cell_type":"code","source":"\n# divide age into groups\nage_range = pd.cut(df['age'], bins = [20,30,40,50,60,70,80,90])\n\n# show probability of belonging to the group with the highest income\nage_income = df.groupby(age_range)['income'].mean()\n\n# barplot showing probability of belonging to the group with the highest income per age range\nplt.figure(figsize=(20, 8))\nplt.ylim(0,1)\nplt.bar(age_income.index.astype(str), age_income, color = 'SkyBlue',\n       edgecolor='black')\nplt.ylabel('Probability of earning >50k', size=20)\nplt.xlabel('Age range', size=20)\nplt.grid(axis='y')","metadata":{"execution":{"iopub.status.busy":"2021-08-13T22:35:53.458413Z","iopub.execute_input":"2021-08-13T22:35:53.45904Z","iopub.status.idle":"2021-08-13T22:35:53.6715Z","shell.execute_reply.started":"2021-08-13T22:35:53.45899Z","shell.execute_reply":"2021-08-13T22:35:53.670476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results match with what we would expect.","metadata":{}},{"cell_type":"markdown","source":"## **fnlwgt**","metadata":{}},{"cell_type":"code","source":"df.loc[df['fnlwgt'] == '?'] \n","metadata":{"execution":{"iopub.status.busy":"2021-08-13T22:35:57.829326Z","iopub.execute_input":"2021-08-13T22:35:57.829686Z","iopub.status.idle":"2021-08-13T22:35:57.846011Z","shell.execute_reply.started":"2021-08-13T22:35:57.829657Z","shell.execute_reply":"2021-08-13T22:35:57.844738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It doesn't have null values.","metadata":{}},{"cell_type":"markdown","source":"## **education.num**\nThis is an ordinal variable for education.","metadata":{}},{"cell_type":"code","source":"df['education.num'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-08-13T22:35:59.861177Z","iopub.execute_input":"2021-08-13T22:35:59.86153Z","iopub.status.idle":"2021-08-13T22:35:59.869359Z","shell.execute_reply.started":"2021-08-13T22:35:59.861501Z","shell.execute_reply":"2021-08-13T22:35:59.868535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['education'].value_counts() # we can see that it has the same number of values","metadata":{"execution":{"iopub.status.busy":"2021-08-13T22:36:00.146421Z","iopub.execute_input":"2021-08-13T22:36:00.14695Z","iopub.status.idle":"2021-08-13T22:36:00.162117Z","shell.execute_reply.started":"2021-08-13T22:36:00.146919Z","shell.execute_reply":"2021-08-13T22:36:00.161221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **capital.gain**","metadata":{}},{"cell_type":"code","source":"df['capital.gain'].unique()\n","metadata":{"execution":{"iopub.status.busy":"2021-08-13T22:36:01.011823Z","iopub.execute_input":"2021-08-13T22:36:01.012212Z","iopub.status.idle":"2021-08-13T22:36:01.020955Z","shell.execute_reply.started":"2021-08-13T22:36:01.012168Z","shell.execute_reply":"2021-08-13T22:36:01.019687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **capital.loss**","metadata":{}},{"cell_type":"code","source":"df['capital.loss'].unique()\n","metadata":{"execution":{"iopub.status.busy":"2021-08-13T22:36:02.279649Z","iopub.execute_input":"2021-08-13T22:36:02.280098Z","iopub.status.idle":"2021-08-13T22:36:02.287712Z","shell.execute_reply.started":"2021-08-13T22:36:02.28006Z","shell.execute_reply":"2021-08-13T22:36:02.28672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **hours.per.week**","metadata":{}},{"cell_type":"code","source":"df['hours.per.week'].unique()\n","metadata":{"execution":{"iopub.status.busy":"2021-08-13T22:36:03.265454Z","iopub.execute_input":"2021-08-13T22:36:03.265958Z","iopub.status.idle":"2021-08-13T22:36:03.273954Z","shell.execute_reply.started":"2021-08-13T22:36:03.265912Z","shell.execute_reply":"2021-08-13T22:36:03.273145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot histogram\nplt.figure(figsize=(15, 8))\nplt.hist(df['hours.per.week'],density=True, bins=10,  color = 'SkyBlue')\nplt.ylabel('Probability', size=20)\nplt.xlabel('hours per week', size=20)\nplt.grid(axis='y')","metadata":{"execution":{"iopub.status.busy":"2021-08-13T22:36:03.979373Z","iopub.execute_input":"2021-08-13T22:36:03.979871Z","iopub.status.idle":"2021-08-13T22:36:04.194806Z","shell.execute_reply.started":"2021-08-13T22:36:03.979824Z","shell.execute_reply":"2021-08-13T22:36:04.193558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **2. Cleaning data** <a class=\"anchor\" id=\"2\"></a>","metadata":{}},{"cell_type":"markdown","source":"## **2.1 Drop useless variables** <a class=\"anchor\" id=\"2.1\"></a>\n\nWe have seen that fnlwgt variable has a really small correlation with the target variable so we can drop it. We can also drop education.num as if we don't do that it will be a multicollinearity problem with education.\n","metadata":{}},{"cell_type":"code","source":"df = df.drop('fnlwgt', axis=1)\ndf = df.drop('education.num', axis=1)\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-13T22:36:08.914029Z","iopub.execute_input":"2021-08-13T22:36:08.914456Z","iopub.status.idle":"2021-08-13T22:36:08.931214Z","shell.execute_reply.started":"2021-08-13T22:36:08.914423Z","shell.execute_reply":"2021-08-13T22:36:08.929722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **2.2 Deal with missing data** <a class=\"anchor\" id=\"2.2\"></a>\nAs we have a lot of data and the missing values is just a small part of the dataset, we drop rows with missing values.","metadata":{}},{"cell_type":"code","source":"df = df.loc[ (df['workclass'] != '?') & (df['occupation'] != '?') & (df['native.country']!= '?')]\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-09T16:04:44.772312Z","iopub.execute_input":"2021-08-09T16:04:44.772689Z","iopub.status.idle":"2021-08-09T16:04:44.801461Z","shell.execute_reply.started":"2021-08-09T16:04:44.772658Z","shell.execute_reply":"2021-08-09T16:04:44.800426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **3. Split data and get dummies** <a class=\"anchor\" id=\"3\"></a>\nFirst we will split the data into dependent and independent variables, and then we will split the dependent variables into continous variables and categorical variables.","metadata":{}},{"cell_type":"code","source":"# Split into dependend and independent variables\nX = df.drop('income', axis=1)\ny = df['income']","metadata":{"execution":{"iopub.status.busy":"2021-08-09T16:04:44.802754Z","iopub.execute_input":"2021-08-09T16:04:44.803036Z","iopub.status.idle":"2021-08-09T16:04:44.80961Z","shell.execute_reply.started":"2021-08-09T16:04:44.803009Z","shell.execute_reply":"2021-08-09T16:04:44.808706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split X into continous variables and categorical variables\n\nX_continous  = X[['age', 'capital.gain', 'capital.loss', 'hours.per.week']]\n\nX_categorical = X[['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race',\n                   'sex', 'native.country']]\n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T16:04:44.810801Z","iopub.execute_input":"2021-08-09T16:04:44.811073Z","iopub.status.idle":"2021-08-09T16:04:44.832707Z","shell.execute_reply.started":"2021-08-09T16:04:44.811046Z","shell.execute_reply":"2021-08-09T16:04:44.831921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can get the dummies from the categorical variables and concatenate both continous and categorical datasets.","metadata":{}},{"cell_type":"code","source":"# Get the dummies\nX_encoded = pd.get_dummies(X_categorical)\n# Concatenate both continous and encoded sets:\nX = pd.concat([X_continous, X_encoded],axis=1)\nX","metadata":{"execution":{"iopub.status.busy":"2021-08-09T16:04:44.833953Z","iopub.execute_input":"2021-08-09T16:04:44.834487Z","iopub.status.idle":"2021-08-09T16:04:44.91168Z","shell.execute_reply.started":"2021-08-09T16:04:44.83442Z","shell.execute_reply":"2021-08-09T16:04:44.91051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **4. Proposed models** <a class=\"anchor\" id=\"4\"></a>\n\nIn this section we will develop the predictive models. We will stratify the data and use a specific random state so all the models have the same target values.\nIt is worth mentioning that in most of the models (the complex ones) I have done hyperparameter optimization with GridSearchCV. As this process takes a lot of time, I have commented the lines where I fit the GridSearch and I comment the results that I had when running it. ","metadata":{}},{"cell_type":"markdown","source":"# **4.1 Logistic Regression** <a class=\"anchor\" id=\"4.1\"></a>","metadata":{}},{"cell_type":"code","source":"# Prepare the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 1/3,\n                                                    stratify=y,random_state=10 )\n\n# MODEL\nlogit = LogisticRegression(max_iter=10000)\nlogit = logit.fit(X_train, y_train)\n\n# CROSS VALIDATION\ncv = StratifiedKFold(n_splits=3) # we make 3 splits\nval_logit = cross_val_score(logit, X_train, y_train, cv=cv).mean()\nval_logit # show validation set score","metadata":{"execution":{"iopub.status.busy":"2021-08-09T16:04:44.913267Z","iopub.execute_input":"2021-08-09T16:04:44.913732Z","iopub.status.idle":"2021-08-09T16:05:12.240293Z","shell.execute_reply.started":"2021-08-09T16:04:44.91369Z","shell.execute_reply":"2021-08-09T16:05:12.239271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PREDICTIONS\nlogit_predictions = logit.predict(X_test)\nacc_logit = accuracy_score(y_test,logit_predictions)\nacc_logit # show test set score\n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T16:05:12.241827Z","iopub.execute_input":"2021-08-09T16:05:12.242452Z","iopub.status.idle":"2021-08-09T16:05:12.270776Z","shell.execute_reply.started":"2021-08-09T16:05:12.242404Z","shell.execute_reply":"2021-08-09T16:05:12.269762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **4.2 Categorical Naïve Bayes** <a class=\"anchor\" id=\"4.2\"></a>","metadata":{}},{"cell_type":"code","source":"# Prepare the data. We only use categorical independent variables \nX_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size= 1/3, \n                                                    stratify=y, random_state=10 )\n\n# MODEL\ncnb = CategoricalNB()\ncnb = cnb.fit(X_train, y_train)\n\n# PREDICTIONS\ncnb_predictions = cnb.predict(X_test)\nacc_cnb = accuracy_score(y_test,cnb_predictions)\nacc_cnb # show test set score","metadata":{"execution":{"iopub.status.busy":"2021-08-09T16:05:12.272244Z","iopub.execute_input":"2021-08-09T16:05:12.272838Z","iopub.status.idle":"2021-08-09T16:05:12.457377Z","shell.execute_reply.started":"2021-08-09T16:05:12.272792Z","shell.execute_reply":"2021-08-09T16:05:12.456411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **4.3 Gaussian Naïve Bayes** <a class=\"anchor\" id=\"4.3\"></a>","metadata":{}},{"cell_type":"code","source":"# Prepare the data. We only use continous independent variables \nX_train, X_test, y_train, y_test = train_test_split(X_continous, y, test_size= 1/3,\n                                                    stratify=y, random_state=10)\n\n# MODEL\ngnb = GaussianNB()\ngnb = gnb.fit(X_train, y_train)\n\n# CROSS VALIDATION\ncv = StratifiedKFold(n_splits=3)\nval_cnb = cross_val_score(gnb, X_train, y_train, cv=cv).mean()\nval_cnb # validation set score\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T16:05:12.458567Z","iopub.execute_input":"2021-08-09T16:05:12.458888Z","iopub.status.idle":"2021-08-09T16:05:12.534155Z","shell.execute_reply.started":"2021-08-09T16:05:12.458855Z","shell.execute_reply":"2021-08-09T16:05:12.53312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PREDICTIONS\ngnb_predictions = gnb.predict(X_test)\nacc_gnb = accuracy_score(y_test,gnb_predictions)\nacc_gnb # test set score","metadata":{"execution":{"iopub.status.busy":"2021-08-09T16:05:12.535629Z","iopub.execute_input":"2021-08-09T16:05:12.535939Z","iopub.status.idle":"2021-08-09T16:05:12.553677Z","shell.execute_reply.started":"2021-08-09T16:05:12.535907Z","shell.execute_reply":"2021-08-09T16:05:12.552669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **4.4 K-Nearest Neighbors** <a class=\"anchor\" id=\"4.4\"></a>","metadata":{}},{"cell_type":"code","source":"# Prepare the data. We scale the data as this algorithm is distance-based\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 1/3, \n                                                    stratify=y, random_state=10)\n# scale data in a range of (0,1)\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaler = scaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T16:05:12.555564Z","iopub.execute_input":"2021-08-09T16:05:12.555994Z","iopub.status.idle":"2021-08-09T16:05:12.662625Z","shell.execute_reply.started":"2021-08-09T16:05:12.55595Z","shell.execute_reply":"2021-08-09T16:05:12.66175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# HYPERPARAMETERS OPTIMIZATION\n\n# set the hyperparameters we want to test\nparam_grid = {'n_neighbors' : [40, 60, 70]}\n\ncv = StratifiedKFold(n_splits=3)\n\noptimal_params = GridSearchCV(\n    estimator = KNeighborsClassifier(),\n    param_grid = param_grid,\n    scoring = 'accuracy',\n    verbose = 2,\n    cv = cv,\n)\n\n#>>> optimal_params.fit(X_train, y_train)\n\n#>>> optimal_params.best_params_\n# {'n_neighbors': 40}","metadata":{"execution":{"iopub.status.busy":"2021-08-09T16:05:12.663841Z","iopub.execute_input":"2021-08-09T16:05:12.664451Z","iopub.status.idle":"2021-08-09T16:05:12.671663Z","shell.execute_reply.started":"2021-08-09T16:05:12.664397Z","shell.execute_reply":"2021-08-09T16:05:12.670189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MODEL\n\nknn = KNeighborsClassifier(n_neighbors=40)\nknn = knn.fit(X_train, y_train)\n\n# CROSS VALIDATION\ncv = StratifiedKFold(n_splits=3)\nval_knn = cross_val_score(knn, X_train, y_train, cv=cv).mean()\nval_knn # validation score\n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T16:05:12.673445Z","iopub.execute_input":"2021-08-09T16:05:12.673934Z","iopub.status.idle":"2021-08-09T16:06:19.331329Z","shell.execute_reply.started":"2021-08-09T16:05:12.673888Z","shell.execute_reply":"2021-08-09T16:06:19.330283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PREDICTIONS\nknn_predictions = knn.predict(X_test)\nacc_knn = accuracy_score(y_test,knn_predictions)\nacc_knn # test score","metadata":{"execution":{"iopub.status.busy":"2021-08-09T16:06:19.332545Z","iopub.execute_input":"2021-08-09T16:06:19.33283Z","iopub.status.idle":"2021-08-09T16:07:15.687303Z","shell.execute_reply.started":"2021-08-09T16:06:19.332803Z","shell.execute_reply":"2021-08-09T16:07:15.686287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **4.5 Support Vector Machines** <a class=\"anchor\" id=\"4.5\"></a>","metadata":{}},{"cell_type":"code","source":"# Prepare the data. We scale the data as this algorithm is distance-based\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 1/3,\n                                                    stratify=y, random_state=10)\n# scale the data (mean=0 and sd=1)\nX_train = scale(X_train)\nX_test = scale(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T16:07:15.692706Z","iopub.execute_input":"2021-08-09T16:07:15.693051Z","iopub.status.idle":"2021-08-09T16:07:15.847793Z","shell.execute_reply.started":"2021-08-09T16:07:15.693017Z","shell.execute_reply":"2021-08-09T16:07:15.846641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# HYPERPARAMETERS OPTIMIZATION\n\n# 1 ROUND\nparam_grid = {\n    'kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n}\n\ncv = StratifiedKFold(n_splits=3)\n\noptimal_params = GridSearchCV(\n    estimator = svm.SVC(),\n    param_grid = param_grid,\n    scoring = 'accuracy',\n    verbose = 2,\n    cv = cv\n)\n\n#>>> optimal_params.fit(X_train, y_train,)\n\n\n#>>> optimal_params.best_params_\n# {'kernel': 'linear'}\n\n# As linear kernels are the simplest ones the result is unexpected.\n# wW can see how better linear kernels are with respect to the rest.\n\n#>>> opt = optimal_params.cv_results_\n#>>> opt = pd.DataFrame.from_dict(opt)\n#>>> opt[['params', 'mean_test_score']]\n#                   params  mean_test_score\n# 0   {'kernel': 'linear'}         0.843346\n# 1     {'kernel': 'poly'}         0.818580\n# 2      {'kernel': 'rbf'}         0.839865\n# 3  {'kernel': 'sigmoid'}         0.830316\n\n\n# As we can tune parameters from the rbf kernel, we will try to improve\n# its performance\n\n\n# 2 ROUND\n\nparam_grid = {\n    'kernel': ['rbf'],\n    'C' : [1, 2, 3, 4, 5],\n}\n\ncv = StratifiedKFold(n_splits=3)\n\noptimal_params = GridSearchCV(\n    estimator = svm.SVC(),\n    param_grid = param_grid,\n    scoring = 'accuracy',\n    verbose = 2,\n    cv = cv\n)\n\n#>>> optimal_params.fit(X_train, y_train,)\n\n#>>> optimal_params.best_params_\n#{'C': 2, 'kernel': 'rbf'}\n\n#>>> optimal_params.best_score_\n# 0.8398648211769877\n\n# as we see rbf is worse than linear, so linear will be used\n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T16:07:15.850498Z","iopub.execute_input":"2021-08-09T16:07:15.850999Z","iopub.status.idle":"2021-08-09T16:07:15.861061Z","shell.execute_reply.started":"2021-08-09T16:07:15.850947Z","shell.execute_reply":"2021-08-09T16:07:15.859041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MODEL\nsuppvm = svm.SVC(kernel='linear')\nsuppvm = suppvm.fit(X_train, y_train)\n\n# CROSS VALIDATION\ncv = StratifiedKFold(n_splits=3)\nval_suppvm = cross_val_score(suppvm, X_train, y_train, cv=cv).mean()\nval_suppvm # validation score","metadata":{"execution":{"iopub.status.busy":"2021-08-09T16:07:15.862712Z","iopub.execute_input":"2021-08-09T16:07:15.863072Z","iopub.status.idle":"2021-08-09T16:09:33.185799Z","shell.execute_reply.started":"2021-08-09T16:07:15.863038Z","shell.execute_reply":"2021-08-09T16:09:33.184561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PREDICTIONS\nsuppvm_predictions = suppvm.predict(X_test)\nacc_suppvm = accuracy_score(y_test,suppvm_predictions)\nacc_suppvm # test score","metadata":{"execution":{"iopub.status.busy":"2021-08-09T16:09:33.187201Z","iopub.execute_input":"2021-08-09T16:09:33.187535Z","iopub.status.idle":"2021-08-09T16:09:42.368332Z","shell.execute_reply.started":"2021-08-09T16:09:33.187502Z","shell.execute_reply":"2021-08-09T16:09:42.367081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **4.6 Decision Trees** <a class=\"anchor\" id=\"4.6\"></a>","metadata":{}},{"cell_type":"code","source":"# Prepare the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 1/3, \n                                                    stratify=y, random_state=10)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T16:09:42.370135Z","iopub.execute_input":"2021-08-09T16:09:42.370734Z","iopub.status.idle":"2021-08-09T16:09:42.422614Z","shell.execute_reply.started":"2021-08-09T16:09:42.370685Z","shell.execute_reply":"2021-08-09T16:09:42.421523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# HYPERPARAMETERS OPTIMIZATION\nparam_grid = {\n'max_depth' : [2,4,6,7,8,9,10,11,12,16,20]\n}\n\ncv = StratifiedKFold(n_splits=3)\n\noptimal_params = GridSearchCV(\n    estimator = DecisionTreeClassifier(),\n    param_grid = param_grid,\n    scoring = 'accuracy',\n    verbose = 2,\n    cv = cv\n)\n\n#>>> optimal_params.fit(X_train, y_train,)\n\n#>>> optimal_params.best_params_\n# {'max_depth': 11}","metadata":{"execution":{"iopub.status.busy":"2021-08-09T16:09:42.424211Z","iopub.execute_input":"2021-08-09T16:09:42.424626Z","iopub.status.idle":"2021-08-09T16:09:42.431688Z","shell.execute_reply.started":"2021-08-09T16:09:42.424591Z","shell.execute_reply":"2021-08-09T16:09:42.430565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MODEL\ntree = DecisionTreeClassifier(max_depth=11)\ntree = tree.fit(X_train, y_train)\n\n# CROSS VALIDATION\ncv = StratifiedKFold(n_splits=3)\nval_tree = cross_val_score(tree, X_train, y_train, cv=cv).mean()\nval_tree # validation score","metadata":{"execution":{"iopub.status.busy":"2021-08-09T16:09:42.433063Z","iopub.execute_input":"2021-08-09T16:09:42.433386Z","iopub.status.idle":"2021-08-09T16:09:42.925883Z","shell.execute_reply.started":"2021-08-09T16:09:42.433327Z","shell.execute_reply":"2021-08-09T16:09:42.924708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PREDICTIONS\ntree_predictions = tree.predict(X_test)\nacc_tree = accuracy_score(y_test,tree_predictions)\nacc_tree # test score","metadata":{"execution":{"iopub.status.busy":"2021-08-09T16:09:42.927341Z","iopub.execute_input":"2021-08-09T16:09:42.927652Z","iopub.status.idle":"2021-08-09T16:09:42.942848Z","shell.execute_reply.started":"2021-08-09T16:09:42.927623Z","shell.execute_reply":"2021-08-09T16:09:42.941749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **4.7 Random Forest** <a class=\"anchor\" id=\"4.7\"></a>","metadata":{}},{"cell_type":"code","source":"# Prepare the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 1/3,\n                                                    stratify=y, random_state=10)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T16:15:38.652825Z","iopub.execute_input":"2021-08-09T16:15:38.653265Z","iopub.status.idle":"2021-08-09T16:15:38.707499Z","shell.execute_reply.started":"2021-08-09T16:15:38.653228Z","shell.execute_reply":"2021-08-09T16:15:38.706521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# HYPERPARAMETERS OPTIMIZATION\nparam_grid = {\n'max_depth' : [8,10,12,16,18,20],\n'n_estimators': [50,100,200],\n'max_samples': [1,0.8,0.6]\n}\n\ncv = StratifiedKFold(n_splits=3)\n\noptimal_params = GridSearchCV(\n    estimator = RandomForestClassifier(),\n    param_grid = param_grid,\n    scoring = 'accuracy',\n    verbose = 2,\n    cv = cv\n)\n\n#>>> optimal_params.fit(X_train, y_train,)\n\n#>>> optimal_params.best_params_\n#{'max_depth': 16, 'max_samples': 0.6, 'n_estimators': 200}\n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T16:09:43.00062Z","iopub.execute_input":"2021-08-09T16:09:43.000968Z","iopub.status.idle":"2021-08-09T16:09:43.007417Z","shell.execute_reply.started":"2021-08-09T16:09:43.000936Z","shell.execute_reply":"2021-08-09T16:09:43.006172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MODEL\nRforest = RandomForestClassifier(max_depth=16,max_samples=0.6, n_estimators=200)\nRforest = Rforest.fit(X_train, y_train)\n\n# CROSS VALIDATION\ncv = StratifiedKFold(n_splits=3)\nval_Rforest = cross_val_score(Rforest, X_train, y_train, cv=cv).mean()\nval_Rforest # validation score","metadata":{"execution":{"iopub.status.busy":"2021-08-09T16:09:43.008835Z","iopub.execute_input":"2021-08-09T16:09:43.009153Z","iopub.status.idle":"2021-08-09T16:09:53.538805Z","shell.execute_reply.started":"2021-08-09T16:09:43.009126Z","shell.execute_reply":"2021-08-09T16:09:53.537724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PREDICTIONS\nRforest_predictions = Rforest.predict(X_test)\nacc_Rforest = accuracy_score(y_test,Rforest_predictions)\nacc_Rforest # test score","metadata":{"execution":{"iopub.status.busy":"2021-08-09T16:09:53.540145Z","iopub.execute_input":"2021-08-09T16:09:53.540504Z","iopub.status.idle":"2021-08-09T16:09:53.94066Z","shell.execute_reply.started":"2021-08-09T16:09:53.540433Z","shell.execute_reply":"2021-08-09T16:09:53.939728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **4.8 XGBoost** <a class=\"anchor\" id=\"4.8\"></a>","metadata":{}},{"cell_type":"code","source":"# Prepare the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 1/3,\n                                                    stratify=y, random_state=10)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T16:09:53.942101Z","iopub.execute_input":"2021-08-09T16:09:53.942437Z","iopub.status.idle":"2021-08-09T16:09:53.991945Z","shell.execute_reply.started":"2021-08-09T16:09:53.942407Z","shell.execute_reply":"2021-08-09T16:09:53.991168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# HYPERPARAMETER OPTIMIZATION\n\n# ROUND 1\n\nparam_grid = {\n    'max_depth': [3, 5, 7],\n    'learning_rate': [0.3, 0.1, 0.05],\n    'gamma': [0, 1, 10],\n    'reg_lambda': [0, 1, 10]\n}\n\n\ncv = StratifiedKFold(n_splits=3)\n\noptimal_params = GridSearchCV(\n    estimator=xgb.XGBClassifier(objective='binary:logistic', #for binary classification\n                                eval_metric=\"logloss\",\n                                use_label_encoder=False), #avoid warning (since we have done encoding)\n    param_grid=param_grid,\n    scoring='accuracy',\n    verbose=2,\n    cv = cv\n)\n\n#>>> optimal_params.fit(X_train, y_train,)\n\n#>>> optimal_params.best_params_\n#{'gamma': 0, 'learning_rate': 0.3, 'max_depth': 5, 'reg_lambda': 10}\n\n\n\n# ROUND 2\n\n\nparam_grid = {\n    'max_depth': [4, 5, 6],\n    'learning_rate': [0.3, 0.5],\n    'subsample': [1, 0.8, 0.6, 0.4],\n    'gamma' : [10, 50, 100]\n}\n\n\n\ncv = StratifiedKFold(n_splits=3)\n\noptimal_params = GridSearchCV(\n    estimator=xgb.XGBClassifier(objective='binary:logistic', #for binary classification\n                                eval_metric=\"logloss\",\n                                learning_rate= 0.1,\n                                reg_lambda=0,\n                                use_label_encoder=False), #avoid warning (since we have done encoding)\n    param_grid=param_grid,\n    scoring='accuracy',\n    verbose=2,\n    cv = cv\n)\n\n#>>> optimal_params.fit(X_train, y_train,)\n\n#>>> optimal_params.best_params_\n#{'learning_rate': 0.3, 'max_depth': 5, 'reg_lambda': 10, 'subsample': 1}\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T16:09:53.993293Z","iopub.execute_input":"2021-08-09T16:09:53.993924Z","iopub.status.idle":"2021-08-09T16:09:54.006287Z","shell.execute_reply.started":"2021-08-09T16:09:53.993875Z","shell.execute_reply":"2021-08-09T16:09:54.00491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MODEL\nxgbm = xgb.XGBClassifier(eval_metric=\"logloss\",\n                        learning_rate= 0.3,\n                        reg_lambda=10,\n                        use_label_encoder=False, # as we have done encoding\n                        max_depth=5,\n                        subsample=1)\n\nxgbm = xgbm.fit(X_train, y_train)\n\n# CROSS VALIDATION\ncv = StratifiedKFold(n_splits=3)\nval_xgbm = cross_val_score(xgbm, X_train, y_train, cv=cv).mean()\nval_xgbm","metadata":{"execution":{"iopub.status.busy":"2021-08-09T16:09:54.007524Z","iopub.execute_input":"2021-08-09T16:09:54.007937Z","iopub.status.idle":"2021-08-09T16:10:03.192762Z","shell.execute_reply.started":"2021-08-09T16:09:54.007882Z","shell.execute_reply":"2021-08-09T16:10:03.191841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PREDICTIONS\nxgbm_predictions = xgbm.predict(X_test)\nacc_xgbm = accuracy_score(y_test,xgbm_predictions)\nacc_xgbm","metadata":{"execution":{"iopub.status.busy":"2021-08-09T16:10:03.197173Z","iopub.execute_input":"2021-08-09T16:10:03.199878Z","iopub.status.idle":"2021-08-09T16:10:03.233649Z","shell.execute_reply.started":"2021-08-09T16:10:03.197714Z","shell.execute_reply":"2021-08-09T16:10:03.232724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save predictions with probabilities in order to later make the ensembling\nxgbm_predictions_prob = xgbm.predict_proba(X_test)\nxgbm_predictions_prob = xgbm_predictions_prob[:,1]","metadata":{"execution":{"iopub.status.busy":"2021-08-09T16:10:03.238249Z","iopub.execute_input":"2021-08-09T16:10:03.240119Z","iopub.status.idle":"2021-08-09T16:10:03.265093Z","shell.execute_reply.started":"2021-08-09T16:10:03.238808Z","shell.execute_reply":"2021-08-09T16:10:03.264219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **4.9 Artifical Neural Networks** <a class=\"anchor\" id=\"4.9\"></a>","metadata":{}},{"cell_type":"code","source":"# Prepare the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 1/3, \n                                                    stratify=y, random_state=10)\n# scale the data (mean=0, sd=1)\nX_train = scale(X_train)\nX_test = scale(X_test)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T16:10:03.269644Z","iopub.execute_input":"2021-08-09T16:10:03.270219Z","iopub.status.idle":"2021-08-09T16:10:03.420313Z","shell.execute_reply.started":"2021-08-09T16:10:03.270182Z","shell.execute_reply":"2021-08-09T16:10:03.419495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# HYPERPARAMETERS OPTIMIZATION\n\n# ROUND 1\n\n# first we need to define the model \ndef ANN_1(neurons=10, hidden_layers=0, dropout_rate=0, learn_rate= 0.1):\n    # model\n    model = keras.Sequential()\n    model.add(keras.layers.Dense(neurons, input_shape = (X_train.shape[1], ), activation='relu'))\n    for i in range(hidden_layers):\n        # Add one hidden layer\n        model.add(keras.layers.Dense(neurons, activation='relu'))\n        model.add(keras.layers.Dropout(dropout_rate))\n    model.add(keras.layers.Dense(1, activation='sigmoid'))\n    # Compile model\n    optimizer = keras.optimizers.SGD(lr=learn_rate, momentum = 0)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    return model\n\n# we will do the grid search with KerasClassifier\nann = KerasClassifier(build_fn=ANN_1, batch_size=30)\n\n\nparam_grid = {\n    'neurons': [10, 30, 60, 100, 200],\n    'hidden_layers': [0, 1, 2],\n    'dropout_rate': [0, 0.1, 0.2, 0.4],\n    'epochs': [8,15],\n    'learn_rate': [0.1, 0.03]\n}\n\ncv = StratifiedKFold(n_splits=3)\n\noptimal_params = GridSearchCV(estimator=ann, param_grid=param_grid, verbose=2, cv=cv)\n\n#>>> optimal_params.fit(X_train, y_train,)\n\n#>>> optimal_params.best_params_\n# {'dropout_rate': 0.2, 'epochs': 15, 'hidden_layers': 1, 'learn_rate': 0.1, 'neurons': 10}\n\n\n\n# ROUND 2\n\ndef ANN_2(init_mode='uniform', activation='relu'):\n    # model\n    model = keras.Sequential()\n    model.add(keras.layers.Dense(10,kernel_initializer=init_mode,\n                                 input_shape = (X_train.shape[1], ), activation=activation))\n    model.add(keras.layers.Dense(10, kernel_initializer=init_mode,activation=activation))\n    model.add(keras.layers.Dropout(0.2))\n    model.add(keras.layers.Dense(1,kernel_initializer=init_mode, activation='sigmoid'))\n    # Compile model\n    optimizer = keras.optimizers.SGD(lr=0.1, momentum = 0)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    return model\n\n\nann = KerasClassifier(build_fn=ANN_2, epochs= 15,  batch_size=30)\n\n\nparam_grid = {\n    'init_mode': ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal',\n                  'glorot_uniform', 'he_normal', 'he_uniform'],\n    'activation': ['softmax','relu', 'tanh', 'sigmoid']\n}\n\n\ncv = StratifiedKFold(n_splits=3)\n\n\noptimal_params = GridSearchCV(estimator=ann, param_grid=param_grid, verbose=2, cv=cv)\n\n#>>> optimal_params.fit(X_train, y_train)\n\n#>>> optimal_params.best_params_\n# {'activation': 'relu', 'init_mode': 'uniform'}\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T16:10:03.421563Z","iopub.execute_input":"2021-08-09T16:10:03.42197Z","iopub.status.idle":"2021-08-09T16:10:03.438909Z","shell.execute_reply.started":"2021-08-09T16:10:03.421939Z","shell.execute_reply":"2021-08-09T16:10:03.43783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MODEL\n\ndef ANN_():\n    model = keras.Sequential()\n    model.add(keras.layers.Dense(10,kernel_initializer='uniform',\n                                 input_shape = (X_train.shape[1], ), activation='relu'))\n    model.add(keras.layers.Dense(10, kernel_initializer='uniform',activation='relu'))\n    model.add(keras.layers.Dropout(0.2))\n    model.add(keras.layers.Dense(1,kernel_initializer='uniform', activation='sigmoid'))\n    # Compile model\n    optimizer = keras.optimizers.SGD(lr=0.1, momentum = 0)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    return model\n\n# we define a learning rate schedule in order to decrease the learning rate\n# as we epoch increases.\ndef scheduler(epoch, lr):\n  if epoch < 10:\n    return lr\n  if epoch < 15:\n\t  return 0.05\n  else:\n      return 0.01\n\n\n# Early stopping: stop the learning when it has 3 consecutive epoch without improvement\ncallback2 = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n# Learning rate schedule\ncallback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n\nann = KerasClassifier(build_fn=ANN_, epochs= 15,  batch_size=30, verbose=0)\n\n\n# CROSS VALIDATION\ncv = StratifiedKFold(n_splits=3)\nval_ann= cross_val_score(ann, X_train, y_train,\n                         cv=cv, fit_params={'callbacks': [callback,callback2]}).mean()\nval_ann # validation score","metadata":{"execution":{"iopub.status.busy":"2021-08-09T16:10:03.44056Z","iopub.execute_input":"2021-08-09T16:10:03.440985Z","iopub.status.idle":"2021-08-09T16:10:25.174014Z","shell.execute_reply.started":"2021-08-09T16:10:03.440941Z","shell.execute_reply":"2021-08-09T16:10:25.172746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Neural Networks Ensembling\n# we will make 10 Neural Networks and then join its predictions by averaging. \n\n\nn_members = 10\nann_dict = {} # dictionary where we will store the predictions\n\nfor i in range(n_members):\n    ann = ANN_()\n    ann.fit(X_train, y_train, epochs=15, batch_size=30, \n            verbose=0, callbacks=[callback, callback2])\n    ann_predictions = ann.predict(X_test)\n    ann_predictions = ann_predictions.reshape(ann_predictions.shape[0], )\n    ann_dict[\"ann%s\" %i] = ann_predictions\n\n# create a pandas DataFrame from the dictionary\nann_dataframe = pd.DataFrame.from_dict(ann_dict)\n\nann_mean_prob = ann_dataframe.mean(axis=1) #averaging all the ANN predictions for each row\nann_mean = np.where(ann_mean_prob > 0.5, 1, 0) # transforem probabilities to a binary variable\nacc_ann = accuracy_score(y_test, ann_mean)\nacc_ann # test score","metadata":{"execution":{"iopub.status.busy":"2021-08-09T16:10:25.175492Z","iopub.execute_input":"2021-08-09T16:10:25.175801Z","iopub.status.idle":"2021-08-09T16:12:14.734787Z","shell.execute_reply.started":"2021-08-09T16:10:25.175771Z","shell.execute_reply":"2021-08-09T16:12:14.733547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **4.10 Ensembling** <a class=\"anchor\" id=\"4.10\"></a>","metadata":{}},{"cell_type":"code","source":"# Select the models by looking at the validation score\n\nnp.array([val_logit, val_cnb, val_knn, val_suppvm, val_tree, val_Rforest, val_xgbm, val_ann])","metadata":{"execution":{"iopub.status.busy":"2021-08-09T16:12:14.736417Z","iopub.execute_input":"2021-08-09T16:12:14.736828Z","iopub.status.idle":"2021-08-09T16:12:14.745136Z","shell.execute_reply.started":"2021-08-09T16:12:14.73679Z","shell.execute_reply":"2021-08-09T16:12:14.743578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we see XGBoost outperforms the rest of the models. It it worth mentioning that the validation score for the ANN is for just one ANN, so when doing the Neural Networks Ensemble we expect it to be higher. Thus, we can do the ensembling with XGBoost and ANN. As XGBoost seems to perform better, we will give it a higher weight.","metadata":{}},{"cell_type":"code","source":"# Create dataset with the best predictions\nbest_predictions = pd.DataFrame(data= {'ann':ann_mean_prob, \n                                       'xgb':xgbm_predictions_prob})\n\n# We give a higher weight to XGBoost\nensembling = best_predictions['ann']* 0.4 + best_predictions['xgb']*0.6\n\n# Probabilities to binary\nensembling_binary = np.where(ensembling > 0.5, 1, 0)\nacc_ensembling = accuracy_score(y_test, ensembling_binary)\nacc_ensembling # test score","metadata":{"execution":{"iopub.status.busy":"2021-08-09T16:21:50.338432Z","iopub.execute_input":"2021-08-09T16:21:50.338856Z","iopub.status.idle":"2021-08-09T16:21:50.352246Z","shell.execute_reply.started":"2021-08-09T16:21:50.338822Z","shell.execute_reply":"2021-08-09T16:21:50.350978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **5. Conclusion**\n\nWe can represent the different accuracy results:\n\n","metadata":{}},{"cell_type":"code","source":"# make a dictionary with all the results\nresults = {'Logistic Regression': acc_logit, \n           'Categorical Naive Bayes': acc_cnb,\n           'Gaussian Naive Bayes': acc_gnb,\n           'K-Nearest Neighbors': acc_knn ,\n           'Support Vector Machines': acc_suppvm,\n           'Decision Trees':acc_tree ,\n           'Random Forest': acc_Rforest,\n           'XGBoost':acc_xgbm ,\n           'Artificial Neural Networks':acc_ann ,\n           'XGBoost-ANN Ensembling': acc_ensembling,         \n          }\n\nresults_dataframe = pd.DataFrame.from_dict(results, orient='index', \n                                           columns=['Accuracy'])\nresults_dataframe","metadata":{"execution":{"iopub.status.busy":"2021-08-09T16:37:33.293631Z","iopub.execute_input":"2021-08-09T16:37:33.294032Z","iopub.status.idle":"2021-08-09T16:37:33.30897Z","shell.execute_reply.started":"2021-08-09T16:37:33.294Z","shell.execute_reply":"2021-08-09T16:37:33.307748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we have seen, the XGBoost-ANN ensembling model has the best accuracy, but it is not remarkably better than the single XGBoost model. We can conclude that XGBoost model outperforms all the other models. ","metadata":{}}]}