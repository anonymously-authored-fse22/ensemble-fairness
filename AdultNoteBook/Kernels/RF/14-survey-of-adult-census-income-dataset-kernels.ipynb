{"cells":[{"metadata":{"id":"szDuN0TZUjqH"},"cell_type":"markdown","source":"\n\n# Classification A Machine Learning Perspective: Survey \n\n","execution_count":null},{"metadata":{"id":"IXXk9tS_10aE"},"cell_type":"markdown","source":"**Abstract:** In this work  the Adults Income Census dataset in Kaggle website is selected as subject for applying diverse classification techniques. This data was extracted from the 1994 Census bureau database by Ronny Kohavi and Barry Becker (Data Mining and Visualization, Silicon Graphics). A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1) && (HRSWK>0)). The prediction task is to determine whether a person makes over 50K dollars a year.\n247 coding literary works were written on this dataset in Kaggle in different computing languages, in this work we consider only the top notebooks written in python .The task   is to predict whether the particular adult earns more or less than $50000, by finding patterns in the independent variables.\n\n**Keywords**—Kaggle, Data Scientists, Machine Learning Engineers,Python\n\n# I. INTRODUCTION\n\n\nHuman dependence on data insights in society has increased over the past two decades. With the emerging technologies there is a huge demand for machine learning which has its applications in all types of the industries. By Machine learning, automated decisions will be predicted based on sample data inputs.. Problems relating to significant domains of social life, retail sector and public safety have been addressed by using machine learning techniques. These three domains play a very important role in daily human life, bringing machine learning techniques to these domains can bring significant change in the life style of humans. The economic status play an important role in determining the social life of an individual, there is a significant interest in these days from government to standardize these social survey platforms in their country and there is a tremendous scope for machine learning techniques to be implement in these survey to obtain interesting insights on social and economic life of citizens.\nRaw data is like crude oil, by processing we can get desired products. Similarly by preprocessing the data we can draw insights on what factors the target variable depends on. Our dataset contains 14 independent variables and one target variable with 32561 samples. \n\n# II. DATA DESCRIPTION\n\nThis dataset is extracted from 1994 Census in such way to to focus on adults, to study their income.This dataset contains 32516 samples and 15 variables in which 14 are independent but Income variable is a target or dependent variable as it depends upon these independent variables.\n\nincome: >50K, <=50K \n\nage: age of a person\n\nworkclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked\n\nfnlwgt: The weight given by the census board\n\neducation_num: Categorical variable of the education\n\n\n\neducation: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool education-num: continuous\nmarital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse\n\noccupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspect, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces\n\nrelationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried\nrace: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black\n\nsex: Female, Male\n\ncapital-gain: gain in capital\n\ncapital-loss: loss in capital\n\nhours-per-week: chorus the person worked for a week\n\nnative-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands\n\n# III. DATA PREPROCESSING\n\nBefore going into data preprocessing, I would like to mention about the libraries that were imported and reading the dataset. Different notebooks did it in different style, one can import all the libraries in single step like in [2][3][6], or multiple steps whenever they required like in [1][4][5]. All the notebooks read_csv function in pandas to read the data.\n\nMost popular preprocessing libraries written in python computing language are Pandas, Numpy, Sklearn. \n\nUsing .info or .describe can help us understand more about the data [1][5].\n\n## A. Handling Missing values\n\nMost of the datasets will have null values, we have to process them before building a model. The ways of treating these  missing values is to drop the samples that have missing values, replace them with mean or median or mode , treat all the missing values as a new class(possible only in few cases).In this dataset null values are in the in the form of ‘?’ so replace it with ‘np.nan’.\n\n\n\nWe can also try by merging different categories in a particular categorical column or columns  like in [2][3][6].\n\n## B. Label Encoding\nGenerally independent variables are of two kinds of datatypes which is Numerical and String, most of the algorithms works better with numerical values so in order to draw insights from  the string variables, we need to encode them .This can be done by the label encoder or onehot encoder from sklearn library like in [4][5][6].\n\n## C. Scaling\nThe variables might be in different ranges, the huge difference in magnitude of variables might effect the prediction of  the target variable. .MinMaxScaler, StandardScaler,  RobustScaler and Normalizer can help us to bring entire data onto a uniform scaled range of values. \n\n## D. Correlation\nCorrelation, is a statistical technique to determines how one variables varies with the other variable, this gives the idea on degree of the relationship of the two variables. It’s a bi-variate analysis measure which describes the association between different variables. In most of the analysis works it is useful to express one variable in terms of  others.\n\n\nAs we observe there are various data types across the independent variables of dataset. To maintain uniformity the columns [ workclass, education, marital_status, occupation, relationship, race, sex , native_country] were encoded into categorical variables. The target variable ‘income’ is also categorized to form ‘Two’ categories for more than 50k dollars/year or less than 50k dollars year. The dataset is made split into train and test datasets with 70:30 ratio and scaling of independent variables is made using standard scaling techniques to obtain data of zero mean and unit variance.\n\n# IV.  MODEL BUILDING\n\nDifferent Classifaction algorithms are applied on the dataset and different results yie'lded.\n\n## A. Logistic Regression:\nLogistic Regression is a Machine Learning algorithm primarily used for the classification; this is a predictive analysis algorithm which is based on probability concept.\n\nLogistic Regression uses a Sigmoid or logistic function as cost function.\nThe hypothesis of logistic regression tends to limit the cost function between 0 and 1. Therefore linear functions fail to represent it as it can have a value greater than 1 or less than 0 which is not possible as per the hypothesis of logistic regression.\n\n**Logistic Regression hyper parameters:**\n\n*Solver* in [‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’]\nRegularization (penalty) can sometimes be helpful.\n\n*Penalty* in [‘none’, ‘l1’, ‘l2’, ‘elasticnet’]\n\nNote: not all solvers support all regularization terms.\n\nThe *C parameter* controls the penalty strength, which can also be effective.\nC in [100, 10, 1.0, 0.1, 0.01]\n\n## B. KNN:\nIn k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\n\n**KNN hyperparameters:-**\n\n*n_neighbors* in [1 to 21]\n\n*metric* in [‘euclidean’, ‘manhattan’, ‘minkowski’]\n\n*weights* in [‘uniform’, ‘distance’]\n\n## C. SVM:\n\n“Support Vector Machine” (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. However,  it is mostly used in classification problems. In the SVM algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiates the two classes very well.\n\nIn the SVM classifier, it is easy to have a linear hyper-plane between these two classes. But, another burning question which arises is, should we need to add this feature manually to have a hyper-plane. No, the SVM  algorithm has a technique called the kernel trick. The SVM kernel is a function that takes low dimensional input space and transforms it to a higher dimensional space i.e. it converts not separable problem to separable problem. It is mostly useful in non-linear separation problem. Simply put, it does some extremely complex data transformations, then finds out the process to separate the data based on the labels or outputs you’ve defined.\n\n**SVM Hyperparameters:-**\n\n*C parameter*: It handles the tradeoff between the two goals below.\n\nIncrease the distance of decision boundary to classes (or support vectors)\n\nMaximize the number of points that are correctly classified in the training set\n\n*Kernel*: Linear, RBF, Poly\n\n*Gamma*: One of the commonly used kernel functions is radial basis function (RBF). Gamma parameter of RBF controls the distance of influence of a single training point. Low values of gamma indicates a large similarity radius which results in more points being grouped together. For high values of gamma, the points need to be very close to each other in order to be considered in the same group (or class). Therefore, models with very large gamma values tend to overfit.\n\nAs the gamma decreases, the regions separating different classes get more generalized. Very large gamma values result in too specific class regions (overfitting).\n\n## D. Decision Tree:\nDecision tree is a type of supervised learning algorithm (having a pre-defined target variable) that is mostly used in classification problems. It works for both categorical and continuous input and output variables. In this technique, we split the population or sample into two or more homogeneous sets (or sub-populations) based on most significant splitter / differentiator in input variables.\n\n**Decision Tree Hyperparameters**\n\n*ROOT Node*: It represents entire population or sample and this further gets divided into two or more homogeneous sets.\n\n*SPLITTING*: It is a process of dividing a node into two or more sub-nodes.\nDecision Node: When a sub-node splits into further sub-nodes, then it is called decision node.\n\n*Leaf/ Terminal Node*: Nodes do not split is called Leaf or Terminal node.\nPruning: When we remove sub-nodes of a decision node, this process is called pruning. You can say opposite process of splitting.\n\n*Branch / Sub-Tree*: A sub section of entire tree is called branch or sub-tree\nParent and Child Node: A node, which is divided into sub-nodes is called parent node of sub-nodes where as sub-nodes are the child of parent node.\n\n## E. Random Forest:\nRandom forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set.\n\nRandom forest is like bootstrapping algorithm with Decision tree (CART) model. Say, we have 1000 observation in the complete population with 10 variables. Random forest tries to build multiple CART models with different samples and different initial variables. For instance, it will take a random sample of 100 observation and 5 randomly chosen initial variables to build a CART model. It will repeat the process (say) 10 times and then make a final prediction on each observation. Final prediction is a function of each prediction. This final prediction can simply be the mean of each prediction.\n\nFor a Random Forest Classifier, there are several different hyperparameters that can be adjusted.But the following four parameters are most important\n\n**Random Forest Hyperparameters:-**\n\n*n_estimators*: The n_estimators parameter specifies the number of trees in the forest of the model. The default value for this parameter is 10, which means that 10 different decision trees will be constructed in the random forest.\n\n*max_depth*: The max_depth parameter specifies the maximum depth of each tree. The default value for max_depth is None, which means that each tree will expand until every leaf is pure. A pure leaf is one where all of the data on the leaf comes from the same class.\n\n*min_samples_split*: The min_samples_split parameter specifies the minimum number of samples required to split an internal leaf node. The default value for this parameter is 2, which means that an internal node must have at least two samples before it can be split to have a more specific classification.\n\n*min_samples_leaf*: The min_samples_leaf parameter specifies the minimum number of samples required to be at a leaf node. The default value for this parameter is 1, which means that every leaf must have at least 1 sample that it classifies.\n\n## F. XGBOOST: \nBoosting is used to create a collection of predictors. In this technique, learners are learned sequentially with early learners fitting simple models to the data and then analysing data for errors. Consecutive trees (random sample) are fit and at every step, the goal is to improve the accuracy from the prior tree. When an input is misclassified by a hypothesis, its weight is increased so that next hypothesis is more likely to classify it correctly. This process converts weak learners into better performing model.\n\nThe XGBoost library implements the gradient boosting decision tree algorithm.\n\nGradient boosting is an approach where new models are created that predict the residuals or errors of prior models and then added together to make the final prediction. It is called gradient boosting because it uses a gradient descent algorithm to minimize the loss when adding new models.\n\nThis approach supports both regression and classification predictive modelling problems.\n\n**XGBoost Hyperparameters:-**\n\n*max_depth*:The maximum depth of a tree\nmin_child_weight:Defines the minimum sum of weights of all observations required in a child.\n\n*gamma*:A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.\n\n*subsample*:   Denotes the fraction of observations to be randomly chosen samples for each tree.\n\n*colsample_bytree*:  Denotes the fraction of columns to be randomly chosen  samples for each tree.\n\n*reg_alpha*:     Regularization parameter\n\n*reg_lamda*:  Regularization parameter\n\n*learning rate*:  Parameter in Gradient Descent\n\n## G. CATBOOST:\nIt yields state-of-the-art results without extensive data training typically required by other machine learning methods, and provides powerful out-of-the-box support for the more descriptive data formats that accompany many business problems.\n\n“CatBoost” name comes from two words “Category” and “Boosting”.\n\nIt can work with multiple Categories of data, such as audio, text, image including historical data.\n\n“Boost” comes from gradient boosting machine learning algorithm as this library is based on gradient boosting library. Gradient boosting is a powerful machine learning algorithm that is widely applied to multiple types of business challenges like fraud detection, recommendation items, forecasting and it performs well also. It can also return very good result with relatively less data, unlike DL models that need to learn from a massive amount of data.\n\n**Advantages of CatBoost Library**\n\n*Performance*: CatBoost provides state of the art results and it is competitive with any leading machine learning algorithm on the performance front.\n\n*Handling Categorical features automatically*: We can use CatBoost without any explicit pre-processing to convert categories into numbers. CatBoost converts categorical values into numbers using various statistics on combinations of categorical features and combinations of categorical and numerical features. You can read more about it here.\n\n*Robust*: It reduces the need for extensive hyper-parameter tuning and lower the chances of overfitting also which leads to more generalized models. Although, CatBoost has multiple parameters to tune and it contains parameters like the number of trees, learning rate, regularization, tree depth, fold size, bagging temperature and others. You can read about all these parameters here.\n\n*Easy-to-use*: You can use CatBoost from the command line, using an user-friendly API for both Python and R.\n\n\n# V TUNING HYPERPARAMETERS\n\n\n## A. Grid Search:- \n Grid Search is particularly used for tuning hyperparameters. Now before going into deep lets know what is a hyperparameter.\n\nA model hyperparameter is a characteristic of a model that is external to the model and whose value cannot be estimated from data. The value of the hyperparameter has to be set before the learning process begins. For example k in k-Nearest Neighbours, the number of hidden layers in Neural Networks.\n\nIn contrast, a parameter is an internal characteristic of the model and its value can be estimated from data. Example, beta coefficients of linear/logistic regression or support vectors in Support Vector Machines.\n\nGrid-search is used to find the optimal hyperparameters of a model which results in the most ‘accurate’ predictions.\n\nTrying with different values of hyperparameters and checking the metrics is a long process. Grid Search does the work for us by iterating with different values that we specifically give and results the optimal one from the given.\n\n*estimator*: estimator object you created\n\n*params_grid*: the dictionary object that holds the hyperparameters you want to try\n\n*scoring*: evaluation metric that you want to use, you can simply pass a valid string/ object of evaluation metric\n\n*cv*: number of cross-validation you have to try for each selected set of hyperparameters\n\n*verbose*: you can set it to 1 to get the detailed print out while you fit the data to GridSearchCV\n\n*n_jobs*: number of processes you wish to run in parallel for this task if it -1 it will use all available processors.\n\n## B. Random Search:\nRandom search is a technique where random combinations of the hyperparameters are used to find the best solution for the built model. It tries random combinations of a range of values. To optimise with random search, the function is evaluated at some number of random configurations in the parameter space.\n\nThe chances of finding the optimal parameter are comparatively higher in random search because of the random search pattern where the model might end up being trained on the optimised parameters without any aliasing. Random search works best for lower dimensional data since the time taken to find the right set is less with less number of iterations. Random search is the best parameter search technique when there are less number of dimensions. In the paper Random Search for Hyper-Parameter Optimization by Bergstra and Bengio, the authors show empirically and theoretically that random search is more efficient for parameter optimization than grid search.\n\n## C. Bayesian Optimization\nBayesian approaches, in contrast to random or grid search, keep track of past evaluation results which they use to form a probabilistic model mapping hyperparameters to a probability of a score on the objective function.\n\nIn the literature, this model is called a “surrogate” for the objective function and is represented as p(y | x). The surrogate is much easier to optimize than the objective function and Bayesian methods work by finding the next set of hyperparameters to evaluate on the actual objective function by selecting hyperparameters that perform best on the surrogate function. In other words:\n\n1.Build a surrogate probability model of the objective function\n\n2.Find the hyperparameters that perform best on the surrogate\n\n3.Apply these hyperparameters to the true objective function\n\n4.Update the surrogate model incorporating the new results\n\n5.Repeat steps 2–4 until max iterations or time is reached\n\nThe aim of Bayesian reasoning is to become “less wrong” with more data which these approaches do by continually updating the surrogate probability model after each evaluation of the objective function.\n\nAt a high-level, Bayesian optimization methods are efficient because they choose the next hyperparameters in an informed manner. The basic idea is: spend a little more time selecting the next hyperparameters in order to make fewer calls to the objective function. In practice, the time spent selecting the next hyperparameters is inconsequential compared to the time spent in the objective function. By evaluating hyperparameters that appear more promising from past results, Bayesian methods can find better model settings than random search in fewer iterations.\n\n# VI CURSE OF DIMENSIONALITY\n\n## Principal Component Analysis\n\nWhen we have too many features to train in the model then there might be a chance of over fitting, so two options\n\n**Feature Elimination**: We shall drop few features and train the model with features that contribute most towards target variable. This can be done by RFE(Recursive Feature Elimination) or RFECV(RFE with Cross Validation).They can give how much each feature is important , like a list, so we can select the important features. But the only problem is we lose the information whatever the dropped features were contributing to the target variable.  So\n\n**Feature Extraction**:  PCA is a feature extraction method. In feature extraction, we create “new” independent variables, where each “new” independent variable is a combination of each of the all “old” independent variables. However, we create these new independent variables in a specific way and order these new variables by how well they predict our dependent variable.\n\nWe keep as many of the new independent variables as we want, but we drop the “least important ones.” Because we ordered the new variables by how well they predict our dependent variable, we know which variable is the most important and least important. Because these new independent variables are combinations of our old ones, we’re still keeping the most valuable parts of our old variables, even when we drop one or more of these “new” variables\n\n**Pros:**\nWhen we want to reduce the number of features but don’t know what to drop, PCA can help us\nNew variables are independent off each other\n\n**Cons:**\nNew features are less interpretable\n\n# VII   EVALUATION METRICS\n\n## A. ROC\nMany machine learning models were performed on this dataset that includes Logistic and Decision Tree algorithms. The metrics which were used to determine the efficiency includes sensitivity or Recall, specificity and accuracy and also using Receiver operating charecteristics curve. The area under curve value reveals that Logistic Regression performed better than Decision Tree. \n\n\n## B. Confusion Matrix\nPerformance measurement for machine learning classification problem where output can be two or more classes. It is a table with 4 different combinations of predicted and actual values.\n\n\nIt is extremely useful for measuring Recall, Precision, Specificity, Accuracy and most importantly AUC-ROC Curve.\n\n## C. Classification Report\n\nA Classification report is used to measure the quality of predictions from a classification algorithm. How many predictions are True and how many are False. More specifically, True Positives, False Positives, True negatives and False Negatives are used to predict the metrics of a classification report\n\nThe report shows the main classification metrics precision, recall and f1-score on a per-class basis. The metrics are calculated by using true and false positives, true and false negatives. Positive and negative in this case are generic names for the predicted classes. There are four ways to check if the predictions are right or wrong:\n\nTN / True Negative: when a case was negative and predicted negative\n\nTP / True Positive: when a case was positive and predicted positive\n\nFN / False Negative: when a case was positive but predicted negative\n\nFP / False Positive: when a case was negative but predicted positive\n\n## D. Precision\nAn ability of a classifier not to label positive to the negatives\nWhen you say a male is pregnant, which is not possible yet, then this would be detected under this precision score.\n(Number of true positive cases) / (Number of all the positive cases)\n*all the positive classes = true positive + false positive\n\n## E. Recall\n\nAn ability of a classifier to find all positive instances. So, only corrected measured instances, which are true-positive and false-negatives, are concerned.\n(Number of true positives) / (# of true positives + # of false negatives)\n*# signifies ‘number’\nBy far, we can tell both Precision and Recall focus on true-positive cases in different perspectives.\n\n## F. F1-score\n\nThis is a weighted harmonic mean value using both Precision and Recall. This measure is pretty useful when the dataset has an imbalanced distribution of different labels.\n{(Precision * Recall) * 2} / (Precision + Recall)\n\n\n\n\n\n# VIII CONCLUSION\nPrimarily in this work classification is performed using machine learning techniques. Various techniques deployed were compared and contrasted in terms of evaluation metrics. Most of the classification tasks will suit to be processed in the pipline discussed in above sections. In future this work can extend to explore the  potential of Neural Networks for classification tasks. \n\n# IX REFERENCES\n\n1.   [EDA + Logistic Regression + PCA by *Prashant Banerjee*](https://www.kaggle.com/prashant111/eda-logistic-regression-pca)\n\n1.   [Income Prediction (84.369% Accuracy) by *IPByrne*](https://www.kaggle.com/ipbyrne/income-prediction-84-369-accuracy)\n2.   [Multiple ML Techniques and Analysis of Dataset by *Matt Green* ](https://www.kaggle.com/bananuhbeatdown/multiple-ml-techniques-and-analysis-of-dataset)\n\n2.   [Catboost and other class.algos with 88% accuracy by *Kanav Anand*](https://www.kaggle.com/kanav0183/catboost-and-other-class-algos-with-88-accuracy)\n\n2.   [EDA and Income predictions (86.75 % accuracy) by *Sumit Mishra*](https://www.kaggle.com/sumitm004/eda-and-income-predictions-86-75-accuracy)\n\n1.   [Income prediction using Random Forest and XGBoost by *Nitineshwar*](https://www.kaggle.com/grayphantom/income-prediction-using-random-forest-and-xgboost)\n\n","execution_count":null},{"metadata":{"id":"XjOKMOoA10V5"},"cell_type":"markdown","source":"Now lets dive into the code and see how to solve the dataset.","execution_count":null},{"metadata":{"id":"8bf0AuDX10Rs"},"cell_type":"markdown","source":"##  Importing necessary Libraries\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Catboost algorithm might need installation, the below code is in the comment form. Run it if installation is required ","execution_count":null},{"metadata":{"id":"J9Rcvcdc33JP","trusted":false},"cell_type":"code","source":"#pip install catboost","execution_count":null,"outputs":[]},{"metadata":{"id":"TXb7HZfjA6-s","outputId":"72153f31-19cd-4c30-d73e-b7b8e7111bea","trusted":true},"cell_type":"code","source":"#Importing Libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.metrics import roc_curve, roc_auc_score,accuracy_score,f1_score,log_loss,confusion_matrix,classification_report,precision_score,recall_score\n","execution_count":null,"outputs":[]},{"metadata":{"id":"Un3kdwWlBNCY","outputId":"fcd7ae36-f4ea-4964-cba8-c6649ea2a0fa","trusted":true},"cell_type":"code","source":"#Reading Dataset\ndataset = pd.read_csv('../input/adult-census-income/adult.csv')\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"PzoHR5qUznrE"},"cell_type":"markdown","source":"We have two variables regarding Education, \"education\" and \"education.num\".\"education.num\" is the categorical version of \"education\" so we can drop the \"education\" variable","execution_count":null},{"metadata":{"id":"YFa38J90-q-i","trusted":true},"cell_type":"code","source":"#Dropping education since we have its categorical coloumn education.num\ndataset.drop(columns=['education'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"MFQ4ccmT8R8f","outputId":"15646796-25b2-4f33-d0a7-85e6aa817639","trusted":true},"cell_type":"code","source":"#data types of columns\ndataset.dtypes","execution_count":null,"outputs":[]},{"metadata":{"id":"RWxKLPbBBSL8","outputId":"630864e3-aa31-42b8-bed5-4de766c54a39","trusted":true},"cell_type":"code","source":"#List of columns present in Dataset\ndataset.columns","execution_count":null,"outputs":[]},{"metadata":{"id":"W4Nr99FPYcr_","outputId":"5e0da855-3c63-4422-942f-fecde665cabb","trusted":true},"cell_type":"code","source":"#Shape of Dataset\ndataset.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"cclCvZT50H32"},"cell_type":"markdown","source":"All these columns names consits of \".\", which cause problem while calling the columns, so replacing all the \".\" with \"_\" in the column names","execution_count":null},{"metadata":{"id":"F75XoTrkBU-H","trusted":true},"cell_type":"code","source":"#Renaming few columns\ndataset.rename(columns = {'education.num':'education_num', 'marital.status':'marital_status', 'capital.gain':'capital_gain',\n                          'capital.loss':'capital_loss','hours.per.week':'hours_per_week','native.country':'native_country'}, inplace = True) ","execution_count":null,"outputs":[]},{"metadata":{"id":"et11jQCSBbeI","outputId":"395b7845-a6eb-43d3-a40e-5f3f82677582","trusted":true},"cell_type":"code","source":"#Checking the change\ndataset.columns","execution_count":null,"outputs":[]},{"metadata":{"id":"PL95sW5p0dd4"},"cell_type":"markdown","source":"\"?\" represents null values , lets replace them with np.nan","execution_count":null},{"metadata":{"id":"cH9yKrQ1BfXR","trusted":true},"cell_type":"code","source":"#Replacing \"?\" with NAN\ndataset['workclass'].replace('?', np.nan, inplace= True)\ndataset['occupation'].replace('?', np.nan, inplace= True)\ndataset['native_country'].replace('?', np.nan, inplace= True)","execution_count":null,"outputs":[]},{"metadata":{"id":"6fk5BXJnB7Iy","outputId":"de199858-c145-4f7f-bf9d-b813403a1ede","trusted":true},"cell_type":"code","source":"#A detailed description of the datset\ndataset.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"id":"wNc6ujYx3Eoq"},"cell_type":"markdown","source":"## Handling the Missing values\n\nEvery datset have some missing values, lets find out in which cloumns they are?","execution_count":null},{"metadata":{"id":"bbmUEm8ICHE_","outputId":"bc8c5251-cf4d-4b92-9f18-1f7031eae5fa","trusted":true},"cell_type":"code","source":"#Number of null values in the dataset column wise\ndataset.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"HuAUmKFphLAk","outputId":"e0346320-4488-4070-ed23-d607c158defa","trusted":true},"cell_type":"code","source":"#Grouping Workclass\ndataset.groupby(['workclass']).size().plot(kind=\"bar\",fontsize=14)\nplt.xlabel('Work Class Categories')\nplt.ylabel('Count of People')\nplt.title('Barplot of Workclass Variable')","execution_count":null,"outputs":[]},{"metadata":{"id":"ut2qyixB0qbv"},"cell_type":"markdown","source":"All the null values in the \"workclass\" can be replaced by \"private\"","execution_count":null},{"metadata":{"id":"bSYgHh-nhadp","outputId":"42eeb826-b84e-402f-a60c-3a5a92aa372d","trusted":true},"cell_type":"code","source":"#Grouping Occupation\ndataset.groupby(['occupation']).size().plot(kind=\"bar\",fontsize=14)\nplt.xlabel('Occupation Categories')\nplt.ylabel('Count of People')\nplt.title('Barplot of Occupation Variable')","execution_count":null,"outputs":[]},{"metadata":{"id":"m_B_w5ZC01bk"},"cell_type":"markdown","source":"\"occupation\" null values cant be replaced by mode , its more or like equally distributed. So drop the null values in this column","execution_count":null},{"metadata":{"id":"g-v2IT0xharM","outputId":"65efba47-09b1-46df-9406-11da1bf6893c","trusted":true},"cell_type":"code","source":"#Grouping Native Country\ndataset.groupby(['native_country']).size().plot(kind=\"bar\",fontsize=10)\nplt.xlabel('Native Country Categories')\nplt.ylabel('Count of People')\nplt.title('Barplot of Native Country Variable')","execution_count":null,"outputs":[]},{"metadata":{"id":"bo5YcOol1J6D"},"cell_type":"markdown","source":"Its clear the null values of \"native_country\" could be easily replaced by mode.","execution_count":null},{"metadata":{"id":"jec4ZRu1CU_y","outputId":"52b43a18-bfcb-4aa9-903f-b0313f14547f","trusted":true},"cell_type":"code","source":"#Droping null values in occupation column\ndataset.dropna(subset=['occupation'],inplace=True)\ndataset.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"C-wOeq_i1X7i"},"cell_type":"markdown","source":"By dropping the null values in \"occupation\" we lose the null values in \"workclass\" also.","execution_count":null},{"metadata":{"id":"Y8mzPvcit2kL","trusted":true},"cell_type":"code","source":"#Imputing null values with Mode\n\ndataset['native_country'].fillna(dataset['native_country'].mode()[0], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"UKmNAHlqt2kb","outputId":"e50e0240-d336-450d-aef7-a03f81ed20a6","trusted":true},"cell_type":"code","source":"#Checking for null values\ndataset.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"5AE390fUt2kk","outputId":"466dd151-865f-441c-cf2a-57a785ec3ad0","trusted":true},"cell_type":"code","source":"#Confirming the Categorical Features\ncategorical_feature_mask = dataset.dtypes==object\ncategorical_feature_mask\n","execution_count":null,"outputs":[]},{"metadata":{"id":"VOwv-NSs3PMu"},"cell_type":"markdown","source":"## Label Encoding\n\nAll the categorical columns and the columns with text data are being Label Encodeded in this step.","execution_count":null},{"metadata":{"id":"xbh_SJKIt2kr","trusted":true},"cell_type":"code","source":"##Label encoding the all the categorical features\n\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ncat_list=['income','workclass','marital_status','occupation','relationship','race','sex','native_country']\ndataset[cat_list]=dataset[cat_list].apply(lambda x:le.fit_transform(x))","execution_count":null,"outputs":[]},{"metadata":{"id":"xX7bL3WxugnW","outputId":"46450ad6-ec91-4d80-a1aa-a830c1b5680f","trusted":true},"cell_type":"code","source":"#Number of categories in dataset\ndataset.nunique()","execution_count":null,"outputs":[]},{"metadata":{"id":"i_Z4aEFN3UJP"},"cell_type":"markdown","source":"## Correlation\n\nTo find out whether there is any relation between variables, in other terms multicollineariaty.\n\n","execution_count":null},{"metadata":{"id":"Nm2UEYdDucz7","outputId":"7e6f8912-d0cb-44bd-819e-cb69b1c4cc04","trusted":true},"cell_type":"code","source":"#Finding Correlation between variables\ncorr = dataset.corr()\nmask = np.zeros(corr.shape, dtype=bool)\nmask[np.triu_indices(len(mask))] = True\nplt.subplots(figsize=(10,7))\nsns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns,annot=True,cmap='RdYlGn',mask = mask)","execution_count":null,"outputs":[]},{"metadata":{"id":"E-uqMDaJ6t6Z","trusted":true},"cell_type":"code","source":"#Dropping \"sex\" variable since it is highly correlated with \"relationship\" variable \ndataset.drop(columns=['sex'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"eDA395Np-7Hh","trusted":true},"cell_type":"code","source":"#Slicing dataset into Independent(X) and Target(y) varibles\ny = dataset.pop('income')\nX = dataset\n","execution_count":null,"outputs":[]},{"metadata":{"id":"3eFvBUVw50SE","trusted":true},"cell_type":"code","source":"#Scaling the dependent variables\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX = sc.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"id":"kUzutCCU_Y3J","outputId":"146908f1-175c-4533-c938-2d09e082e51c","trusted":true},"cell_type":"code","source":"#Dividing dataset into test and train\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n\nprint(X_train.shape)\nprint(y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"3TMNoAaB4lZL","trusted":true},"cell_type":"code","source":"#Performing Recursive Feauture Elimation with Cross Validation\n#Using Random forest for RFE-CV and logloss as scoring\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import log_loss\nclf_rf=RandomForestClassifier(random_state=0)\nrfecv=RFECV(estimator=clf_rf, step=1,cv=5,scoring='neg_log_loss')\nrfecv=rfecv.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"n5oPAzaW4oTI","outputId":"aebb6d1d-e64d-404e-d35e-6d0b6f8a7cc7","trusted":true},"cell_type":"code","source":"#Optimal number of features\nX_train = pd.DataFrame(X_train)\nX_test = pd.DataFrame(X_test)\nprint('Optimal number of features :', rfecv.n_features_)\nprint('Best features :', X_train.columns[rfecv.support_])","execution_count":null,"outputs":[]},{"metadata":{"id":"ikq_JzbH5lg-","trusted":true},"cell_type":"code","source":"#Feauture Ranking\nclf_rf = clf_rf.fit(X_train,y_train)\nimportances = clf_rf.feature_importances_\n\nstd = np.std([tree.feature_importances_ for tree in clf_rf.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n","execution_count":null,"outputs":[]},{"metadata":{"id":"pLasDzFZ5syQ","trusted":true},"cell_type":"code","source":"#Selecting the Important Features\nX_train = X_train.iloc[:,X_train.columns[rfecv.support_]]\nX_test = X_test.iloc[:,X_test.columns[rfecv.support_]]","execution_count":null,"outputs":[]},{"metadata":{"id":"KKrXYvss5vx-","outputId":"ea47e266-c5ef-4a75-e7e0-07153006ed3a","trusted":true},"cell_type":"code","source":"#Creating anew dataframe with column names and feature importance\ndset = pd.DataFrame()\ndata1 = dataset\n\ndset['attr'] = data1.columns\n\n\ndset['importance'] = clf_rf.feature_importances_\n#Sorting with importance column\ndset = dset.sort_values(by='importance', ascending=True)\n\n#Barplot indicating Feature Importance\nplt.figure(figsize=(16, 14))\nplt.barh(y=dset['attr'], width=dset['importance'], color='#1976D2')\nplt.title('RFECV - Feature Importances', fontsize=20, fontweight='bold', pad=20)\nplt.xlabel('Importance', fontsize=14, labelpad=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"Hz7TTPdu3d6N"},"cell_type":"markdown","source":"# 3 CLASSIFICATION MODEL BUILDING","execution_count":null},{"metadata":{"id":"XEM_NOwp6CiY","trusted":true},"cell_type":"code","source":"classifier_lg = LogisticRegression(random_state=0)\nclassifier_dt = DecisionTreeClassifier(random_state=0)\nclassifier_nb = GaussianNB()\nclassifier_knn = KNeighborsClassifier()\nclassifier_rf = RandomForestClassifier(random_state=0)\nclassifier_xgb = XGBClassifier(random_state=0)\nclassifier_cgb = CatBoostClassifier(random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"id":"umKdtsku6Ir6"},"cell_type":"markdown","source":"## Training with didfferent Algorithms","execution_count":null},{"metadata":{"id":"1PLTrGpF_osJ","outputId":"6c2ea972-831a-42e4-9353-0538b136d7c1","trusted":true},"cell_type":"code","source":"\n\n# Instantiate the classfiers and make a list\nclassifiers = [classifier_lg,\n               classifier_dt,\n               classifier_nb,\n               classifier_knn,\n               classifier_rf,\n               classifier_xgb,\n               classifier_cgb]\n# Define a result table as a DataFrame\nresult_table = pd.DataFrame(columns=['classifiers', 'fpr','tpr','Roc Auc','Accuracy','f1 Score','logloss','Confusion Matrix','Precision','Recall'])\n\n# Train the models and record the results\nfor cls in classifiers:\n    model = cls.fit(X_train, y_train)\n    y_proba = model.predict_proba(X_test)[::,1]\n    y_pred = model.predict(X_test)\n    print(cls, '\\n','Confusion Matrix','\\n',confusion_matrix(y_test,  y_pred))\n    print('\\n','Classification Report','\\n',classification_report(y_test,  y_pred))\n    print('='*170)\n    fpr, tpr, _ = roc_curve(y_test,  y_proba)\n    auc = roc_auc_score(y_test, y_proba)\n    Accuracy = accuracy_score(y_test,y_pred)\n    f1score = f1_score(y_test,y_pred)\n    logloss = log_loss(y_test,y_proba)\n    cm = confusion_matrix(y_test,  y_pred)\n    precision = precision_score(y_test,  y_pred)\n    recall = recall_score(y_test,  y_pred)\n  \n    \n    result_table = result_table.append({'classifiers':cls.__class__.__name__,\n                                        'fpr':fpr, \n                                        'tpr':tpr, \n                                        'Roc Auc':auc,\n                                        'Accuracy':Accuracy,\n                                        'f1 Score':f1score,\n                                        'logloss':logloss,\n                                        'Confusion Matrix': cm,\n                                        'Precision':precision,\n                                        'Recall':recall}, ignore_index=True)\n\n# Set name of the classifiers as index labels\nresult_table.set_index('classifiers', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"bSCu66396XDp"},"cell_type":"markdown","source":"## Roc Plot","execution_count":null},{"metadata":{"id":"KqtUFsxL6ZBI","outputId":"5d0b8eb6-aa09-4ede-f22b-cdfe9e166120","trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(8,6))\n\nfor i in result_table.index:\n    plt.plot(result_table.loc[i]['fpr'], \n             result_table.loc[i]['tpr'], \n             label=\"{}, AUC={:.3f}\".format(i, result_table.loc[i]['Roc Auc']))\n    \nplt.plot([0,1], [0,1], color='orange', linestyle='--')\n\nplt.xticks(np.arange(0.0, 1.1, step=0.1))\nplt.xlabel(\"Flase Positive Rate\", fontsize=15)\n\nplt.yticks(np.arange(0.0, 1.1, step=0.1))\nplt.ylabel(\"True Positive Rate\", fontsize=15)\n\nplt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\nplt.legend(prop={'size':13}, loc='lower right')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"GH8Tw0qI_tbB","outputId":"35f990a5-6faf-4ac6-9811-418dc363c1a9","trusted":true},"cell_type":"code","source":"result_table[['Roc Auc','Accuracy','f1 Score','logloss','Confusion Matrix','Precision','Recall']]","execution_count":null,"outputs":[]},{"metadata":{"id":"eTb4EcqM8S8c"},"cell_type":"markdown","source":"CatBoost, XGBoost both performed well, both gave good accuracy, f1score, log_loss.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}