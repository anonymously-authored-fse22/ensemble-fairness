{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Load the census income dataset\ndata = pd.read_csv(\"../input/adult.csv\")","execution_count":2,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e809a2ca08aa588f5b65e26ea65da3203cf56b4f","collapsed":true},"cell_type":"code","source":"data.head()","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"3f811dfa91e304d01dcaea7443be82b0ea8819b9"},"cell_type":"markdown","source":"## Data Exploration\n#### A cursory investigation of the dataset will determine how many individuals fit into either group, and will tell us about the percentage of these individuals making more than $50,000."},{"metadata":{"trusted":true,"_uuid":"9536d638d078ef778312f1f53933e2fd6ad4ce80","collapsed":true},"cell_type":"code","source":"# TODO: Total number of records\nn_records = len(data)\n\n# TODO: Number of records where individual's income is more than $50,000\nn_greater_50k = len(data.query('income == \">50K\"'))\n\n# TODO: Number of records where individual's income is at most $50,000\nn_at_most_50k = len(data.query('income == \"<=50K\"'))\n\n# TODO: Percentage of individuals whose income is more than $50,000\ngreater_percent = (float(n_greater_50k) / n_records * 100)\n\n# Print the results\nprint(\"Total number of records: {}\".format(n_records))\nprint(\"Individuals making more than $50,000: {}\".format(n_greater_50k))\nprint(\"Individuals making at most $50,000: {}\".format(n_at_most_50k))\nprint(\"Percentage of individuals making more than $50,000: {:.2f}%\".format(greater_percent))","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"8a3307329f123f77f681ade30c2c9f3edf01ff13"},"cell_type":"markdown","source":"## Preparing the Data\n####  Before data can be used as input for machine learning algorithms, it often must be cleaned, formatted, and restructured â€” this is typically known as preprocessing. Fortunately, for this dataset, there are no invalid or missing entries we must deal with, however, there are some qualities about certain features that must be adjusted. This preprocessing can help tremendously with the outcome and predictive power of nearly all learning algorithms."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"bf4a7186781fc9bca837e0d7249724f2fff164fa","collapsed":true},"cell_type":"code","source":"# Drop this column since it is not a significant feature to build our model prediction\ndata.drop(labels='fnlwgt',axis=1,inplace=True)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"144c8d8908876d41ad239657e6aa226cc7c6170a","collapsed":true},"cell_type":"code","source":"data[data.isnull()].count()","execution_count":6,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fa068dcfa770c64479d97e21dbc7c3eb6fb862c8"},"cell_type":"markdown","source":"## Normalizing Numerical Features"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9c7b729ea44c880e9374faa30bcf7338e59eb105"},"cell_type":"code","source":"X = data.iloc[:,:-1]\nY = data.iloc[:,-1]","execution_count":7,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf09e1bd296bd66fb962b2f283c0b19ba221be0d","collapsed":true},"cell_type":"code","source":"# Check out the head of input features in X\nX.head()","execution_count":8,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ded3b53b44d2ac60378de08ef78a730889b7ce2d","collapsed":true},"cell_type":"code","source":"# Check out the head of output target in Y\nY.head()","execution_count":9,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fc0c8ad6e23c4f11a45f80d07f85312efbc7fc90"},"cell_type":"code","source":"# Import sklearn.preprocessing.StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Initialize a scaler, then apply it to the features\nscaler = MinMaxScaler() # default=(0, 1)\nnumerical = ['age', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week']\n\nX = pd.DataFrame(data = X)\nX[numerical] = scaler.fit_transform(X[numerical])","execution_count":12,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"58ebb2de5df38c6e1bec10843828e9b3073d00bc","collapsed":true},"cell_type":"code","source":"# Show an example of a record with scaling applied\ndisplay(X.head(n = 5))","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"f4270b672e198fe69430fde61a05a221e1f8dfbd"},"cell_type":"markdown","source":"## Data Preprocessing"},{"metadata":{"trusted":true,"_uuid":"ff944ad47e2fd472b450d8213a0f6163523e14e5","collapsed":true},"cell_type":"code","source":"# TODO: One-hot encode the 'features_log_minmax_transform' data using pandas.get_dummies()\nX_final = pd.get_dummies(X)\n\n# TODO: Encode the 'income_raw' data to numerical values\nY_final = [1 if x == '>50K' else 0 for x in Y ]\n\n# Print the number of features after one-hot encoding\nencoded = list(X_final.columns)\nprint(\"{} total features after one-hot encoding.\".format(len(encoded)))","execution_count":13,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2aaa67565d08edf1ff3ba70e1a1220097235f9f","collapsed":true},"cell_type":"code","source":"# Print the list of features which were encoded\nprint(encoded)","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"3ea21a812b0ca02eaffaa05047ba24d92fda95bf"},"cell_type":"markdown","source":"## Split the dataset into Training and Test set"},{"metadata":{"trusted":true,"_uuid":"c7de255f0510d718fb75306537eb4093741144d8","collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(X_final, Y_final, test_size=0.20, random_state=0)\n\n# Show the results of the split\nprint(\"Training set has {} samples.\".format(X_train.shape[0]))\nprint(\"Testing set has {} samples.\".format(X_test.shape[0]))","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"99c50e8741be45eeb065d06b6bbbc046b88e874c"},"cell_type":"markdown","source":"## Evaluating Model Performance"},{"metadata":{"trusted":true,"_uuid":"eca07b1d02f22e5e2c1e903526970dd1b5e7db1c","collapsed":true},"cell_type":"code","source":"# TODO: Calculate accuracy\naccuracy = float(n_greater_50k) / (n_greater_50k + n_at_most_50k) \n\n# TODO: Calculate F-score using the formula above for beta = 0.5\nfscore = 1.25 * (accuracy) / ( 0.25 * accuracy + 1)\nprint(\"Naive Predictor: [Accuracy score: {:.4f}, F-score: {:.4f}]\".format(accuracy, fscore))","execution_count":16,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"391e8b041bc4579df518b92a5f244c43ee7aff22","collapsed":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve, train_test_split, KFold\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\n# Params for Random Forest\nnum_trees = 100\nmax_features = 3\n\n#Spot Check 5 Algorithms (LR, LDA, KNN, CART, GNB, SVM)\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('RF', RandomForestClassifier(n_estimators=num_trees, max_features=max_features)))\n#models.append(('SVM', SVC()))\n# evalutate each model in turn\nresults = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=0)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","execution_count":17,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2c84d924f34652cb4b632386c45274347b8b19b","collapsed":true},"cell_type":"code","source":"# Model Comparison\nfig = plt.figure()\nfig.suptitle('Algorith Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","execution_count":20,"outputs":[]},{"metadata":{"_uuid":"85c86e1b7b1313dfa754b59736408e952b45f1b0"},"cell_type":"markdown","source":"## Final Model Evaluation"},{"metadata":{"trusted":true,"_uuid":"4b69c2cb835efcc372d31a4d8f6663087564287c","collapsed":true},"cell_type":"code","source":"random_forest = RandomForestClassifier(n_estimators=250,max_features=5)\nrandom_forest.fit(X_train, Y_train)\npredictions = random_forest.predict(X_test)\nprint(\"Accuracy: %s%%\" % (100*accuracy_score(Y_test, predictions)))\nprint(confusion_matrix(Y_test, predictions))\nprint(classification_report(Y_test, predictions))","execution_count":21,"outputs":[]},{"metadata":{"_uuid":"e59fb199474ee7fb519f08971d5daddfeffeacd2"},"cell_type":"markdown","source":"## Conclusion"},{"metadata":{"_uuid":"55f04e3f0f9ba24777b24da3c6d6ff672443b5fb"},"cell_type":"markdown","source":"#### Answer: The Random Forest optimized model's accuracy and F-score on testing data is 84.5% and 0.84 respectively. These scores are slightly better than the unoptimized model.\n#### There is a huge improvement from the naive predictor benchmarks (Accuracy: 0.2408 F-score: 0.2839), the optimized model gives almost 85 percent accurate results."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}