{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('../input/adult-census-income/adult.csv') #read file with data set\n# Let's see first 10 lines of our table just to know whith what we will work\ndataset.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can see that there are 32561 instances(lines) and 15 attributes(columns) in the data set.\ndataset.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This is all columns from data set\ndataset.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can see that our table has missing values \"?\"(for example line 0, line 2)\n# We have to correct it, because if not our program will think that person 0 and person 2 \n# have the same workclass which call \"?\" and have the same occupation also \"?\",\n# I guess it is not true so I just change all \"?\" to NaN value\ndataset[dataset == \"?\"] = np.nan\ndataset.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ok now Thre is NaN ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# In this part of code we change NaN with the most frequent value\nfor col in dataset.columns:\n    dataset[col].fillna(dataset[col].mode()[0], inplace=True)\ndataset.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Above we see all types of columns I think \"age\" don't need such big number like int64,\n# so I chage to smaller number int8\ndataset.astype({'age':'int8', 'education.num':'int8','hours.per.week':'int8'}).dtypes\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classify columns to numerical group (comparable values) and categorical group (non comparable group)\nCATEGORICAL = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country']\nNUMERICAL = ['age', 'fnlwgt', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We do this becouse it is not comfortable to work with string values and we will hash(encrypt) it to numvers,\n# the same values correspond to the same number, the difference values correspond to the different numbers\n# we use function fit_transform to do this","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in CATEGORICAL:\n    dataset[i] = LabelEncoder().fit_transform(dataset[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lts's see what happen, there are only numbers now\ndataset.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We do the same with 'income' but manually\ndataset['income'] = dataset['income'].apply(lambda x: 0 if(x == '<=50K') else 1)\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# look 'income' was changed\ndataset.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Divide the table into input and output,  \ny = dataset.pop('income') #output\nX = dataset.copy() #input\n# it is seems like function in math y(x), so we have x and must find y ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split data into separate training and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, train_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# METHOD 1 RANDOM FOREST\n# I decided to do this task with random forest, so I create the object \"random_forest\"\nmodel_random_forest = RandomForestClassifier()\nmodel_random_forest.fit(X_train, y_train) # Build a forest of trees from the training set (X_train, y_train).","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_random_forest = model_random_forest.predict(X_test) # Predict ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_prob_random_forest = model_random_forest.predict_proba(X_test) # Predict proba","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use Accuracy Score\nmetrics.accuracy_score(y_test, y_pred_random_forest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics.roc_auc_score(y_test, y_pred_prob_random_forest[:, 1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We also can use Confusion Matrix \nconf_matrix = metrics.confusion_matrix(y_test, y_pred_random_forest)\nconf_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# So we have confusion matrix which look like this:\n#            Predicted 0   Predicted 1\n#  Actual 0   18448           1334\n#  Actual 1    2439           3828\n\ntn = conf_matrix[0,0] # true negative\nfp = conf_matrix[0,1] # false positive\nfn = conf_matrix[1,0] # false negative\ntp = conf_matrix[1,1] # true positive","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# how often is the classifier correct\n(tn+tp)/(tn+fp+fn+tp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Aha! Have the same result. But the plus of confusion matrix is that we can calculate variety of metrix:\n# For exapmle:\n# When the actual value is positive, how often is the prediction correct?\nprint(tp/(tp+fn))\n# When the actual value is negative, how often is the prediction correct?\nprint(tn/(tn+fp))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We have the same value with accuracy_score and Confussion_matrix\n# But let's see how many 1 and 0 in test set (y_test)\ny_test.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# So we have about 80% of 0, this is high imbalance \n# This means that even if we just print only 0 we will gave about 80% of correct answers\n# Maybe we should use other method ???","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's try roc_auc_score\n# AUC is useful even when there is high imbalance (unlike classification accuracy)\nmetrics.roc_auc_score(y_test, y_pred_random_forest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we see other result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# METHOD 2 XGB\n# Do the same as above but using another classifier algorithm\nmodel_xgb = xgb.XGBClassifier()\nmodel_xgb.fit(X_train, y_train)\ny_pred_xgb = model_xgb.predict(X_test)\ny_pred_prob_xgb = model_xgb.predict_proba(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics.accuracy_score(y_test, y_pred_xgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics.roc_auc_score(y_test, y_pred_prob_xgb[:, 1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Conclusions:\n# When we use random forest classifier we have accuracy about 90%\n# When we use XGBCLassifier we have accuracy about 92%\n# So I think better to use XGBCLassifier in this task","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"nbformat":4,"nbformat_minor":1}