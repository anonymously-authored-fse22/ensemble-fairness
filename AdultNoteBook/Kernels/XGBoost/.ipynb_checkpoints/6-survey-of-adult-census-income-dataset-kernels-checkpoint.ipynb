{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "szDuN0TZUjqH"
   },
   "source": [
    "\n",
    "\n",
    "# Classification A Machine Learning Perspective: Survey \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXXk9tS_10aE"
   },
   "source": [
    "**Abstract:** In this work  the Adults Income Census dataset in Kaggle website is selected as subject for applying diverse classification techniques. This data was extracted from the 1994 Census bureau database by Ronny Kohavi and Barry Becker (Data Mining and Visualization, Silicon Graphics). A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1) && (HRSWK>0)). The prediction task is to determine whether a person makes over 50K dollars a year.\n",
    "247 coding literary works were written on this dataset in Kaggle in different computing languages, in this work we consider only the top notebooks written in python .The task   is to predict whether the particular adult earns more or less than $50000, by finding patterns in the independent variables.\n",
    "\n",
    "**Keywords**—Kaggle, Data Scientists, Machine Learning Engineers,Python\n",
    "\n",
    "# I. INTRODUCTION\n",
    "\n",
    "\n",
    "Human dependence on data insights in society has increased over the past two decades. With the emerging technologies there is a huge demand for machine learning which has its applications in all types of the industries. By Machine learning, automated decisions will be predicted based on sample data inputs.. Problems relating to significant domains of social life, retail sector and public safety have been addressed by using machine learning techniques. These three domains play a very important role in daily human life, bringing machine learning techniques to these domains can bring significant change in the life style of humans. The economic status play an important role in determining the social life of an individual, there is a significant interest in these days from government to standardize these social survey platforms in their country and there is a tremendous scope for machine learning techniques to be implement in these survey to obtain interesting insights on social and economic life of citizens.\n",
    "Raw data is like crude oil, by processing we can get desired products. Similarly by preprocessing the data we can draw insights on what factors the target variable depends on. Our dataset contains 14 independent variables and one target variable with 32561 samples. \n",
    "\n",
    "# II. DATA DESCRIPTION\n",
    "\n",
    "This dataset is extracted from 1994 Census in such way to to focus on adults, to study their income.This dataset contains 32516 samples and 15 variables in which 14 are independent but Income variable is a target or dependent variable as it depends upon these independent variables.\n",
    "\n",
    "income: >50K, <=50K \n",
    "\n",
    "age: age of a person\n",
    "\n",
    "workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked\n",
    "\n",
    "fnlwgt: The weight given by the census board\n",
    "\n",
    "education_num: Categorical variable of the education\n",
    "\n",
    "\n",
    "\n",
    "education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool education-num: continuous\n",
    "marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse\n",
    "\n",
    "occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspect, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces\n",
    "\n",
    "relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried\n",
    "race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black\n",
    "\n",
    "sex: Female, Male\n",
    "\n",
    "capital-gain: gain in capital\n",
    "\n",
    "capital-loss: loss in capital\n",
    "\n",
    "hours-per-week: chorus the person worked for a week\n",
    "\n",
    "native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands\n",
    "\n",
    "# III. DATA PREPROCESSING\n",
    "\n",
    "Before going into data preprocessing, I would like to mention about the libraries that were imported and reading the dataset. Different notebooks did it in different style, one can import all the libraries in single step like in [2][3][6], or multiple steps whenever they required like in [1][4][5]. All the notebooks read_csv function in pandas to read the data.\n",
    "\n",
    "Most popular preprocessing libraries written in python computing language are Pandas, Numpy, Sklearn. \n",
    "\n",
    "Using .info or .describe can help us understand more about the data [1][5].\n",
    "\n",
    "## A. Handling Missing values\n",
    "\n",
    "Most of the datasets will have null values, we have to process them before building a model. The ways of treating these  missing values is to drop the samples that have missing values, replace them with mean or median or mode , treat all the missing values as a new class(possible only in few cases).In this dataset null values are in the in the form of ‘?’ so replace it with ‘np.nan’.\n",
    "\n",
    "\n",
    "\n",
    "We can also try by merging different categories in a particular categorical column or columns  like in [2][3][6].\n",
    "\n",
    "## B. Label Encoding\n",
    "Generally independent variables are of two kinds of datatypes which is Numerical and String, most of the algorithms works better with numerical values so in order to draw insights from  the string variables, we need to encode them .This can be done by the label encoder or onehot encoder from sklearn library like in [4][5][6].\n",
    "\n",
    "## C. Scaling\n",
    "The variables might be in different ranges, the huge difference in magnitude of variables might effect the prediction of  the target variable. .MinMaxScaler, StandardScaler,  RobustScaler and Normalizer can help us to bring entire data onto a uniform scaled range of values. \n",
    "\n",
    "## D. Correlation\n",
    "Correlation, is a statistical technique to determines how one variables varies with the other variable, this gives the idea on degree of the relationship of the two variables. It’s a bi-variate analysis measure which describes the association between different variables. In most of the analysis works it is useful to express one variable in terms of  others.\n",
    "\n",
    "\n",
    "As we observe there are various data types across the independent variables of dataset. To maintain uniformity the columns [ workclass, education, marital_status, occupation, relationship, race, sex , native_country] were encoded into categorical variables. The target variable ‘income’ is also categorized to form ‘Two’ categories for more than 50k dollars/year or less than 50k dollars year. The dataset is made split into train and test datasets with 70:30 ratio and scaling of independent variables is made using standard scaling techniques to obtain data of zero mean and unit variance.\n",
    "\n",
    "# IV.  MODEL BUILDING\n",
    "\n",
    "Different Classifaction algorithms are applied on the dataset and different results yie'lded.\n",
    "\n",
    "## A. Logistic Regression:\n",
    "Logistic Regression is a Machine Learning algorithm primarily used for the classification; this is a predictive analysis algorithm which is based on probability concept.\n",
    "\n",
    "Logistic Regression uses a Sigmoid or logistic function as cost function.\n",
    "The hypothesis of logistic regression tends to limit the cost function between 0 and 1. Therefore linear functions fail to represent it as it can have a value greater than 1 or less than 0 which is not possible as per the hypothesis of logistic regression.\n",
    "\n",
    "**Logistic Regression hyper parameters:**\n",
    "\n",
    "*Solver* in [‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’]\n",
    "Regularization (penalty) can sometimes be helpful.\n",
    "\n",
    "*Penalty* in [‘none’, ‘l1’, ‘l2’, ‘elasticnet’]\n",
    "\n",
    "Note: not all solvers support all regularization terms.\n",
    "\n",
    "The *C parameter* controls the penalty strength, which can also be effective.\n",
    "C in [100, 10, 1.0, 0.1, 0.01]\n",
    "\n",
    "## B. KNN:\n",
    "In k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\n",
    "\n",
    "**KNN hyperparameters:-**\n",
    "\n",
    "*n_neighbors* in [1 to 21]\n",
    "\n",
    "*metric* in [‘euclidean’, ‘manhattan’, ‘minkowski’]\n",
    "\n",
    "*weights* in [‘uniform’, ‘distance’]\n",
    "\n",
    "## C. SVM:\n",
    "\n",
    "“Support Vector Machine” (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. However,  it is mostly used in classification problems. In the SVM algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiates the two classes very well.\n",
    "\n",
    "In the SVM classifier, it is easy to have a linear hyper-plane between these two classes. But, another burning question which arises is, should we need to add this feature manually to have a hyper-plane. No, the SVM  algorithm has a technique called the kernel trick. The SVM kernel is a function that takes low dimensional input space and transforms it to a higher dimensional space i.e. it converts not separable problem to separable problem. It is mostly useful in non-linear separation problem. Simply put, it does some extremely complex data transformations, then finds out the process to separate the data based on the labels or outputs you’ve defined.\n",
    "\n",
    "**SVM Hyperparameters:-**\n",
    "\n",
    "*C parameter*: It handles the tradeoff between the two goals below.\n",
    "\n",
    "Increase the distance of decision boundary to classes (or support vectors)\n",
    "\n",
    "Maximize the number of points that are correctly classified in the training set\n",
    "\n",
    "*Kernel*: Linear, RBF, Poly\n",
    "\n",
    "*Gamma*: One of the commonly used kernel functions is radial basis function (RBF). Gamma parameter of RBF controls the distance of influence of a single training point. Low values of gamma indicates a large similarity radius which results in more points being grouped together. For high values of gamma, the points need to be very close to each other in order to be considered in the same group (or class). Therefore, models with very large gamma values tend to overfit.\n",
    "\n",
    "As the gamma decreases, the regions separating different classes get more generalized. Very large gamma values result in too specific class regions (overfitting).\n",
    "\n",
    "## D. Decision Tree:\n",
    "Decision tree is a type of supervised learning algorithm (having a pre-defined target variable) that is mostly used in classification problems. It works for both categorical and continuous input and output variables. In this technique, we split the population or sample into two or more homogeneous sets (or sub-populations) based on most significant splitter / differentiator in input variables.\n",
    "\n",
    "**Decision Tree Hyperparameters**\n",
    "\n",
    "*ROOT Node*: It represents entire population or sample and this further gets divided into two or more homogeneous sets.\n",
    "\n",
    "*SPLITTING*: It is a process of dividing a node into two or more sub-nodes.\n",
    "Decision Node: When a sub-node splits into further sub-nodes, then it is called decision node.\n",
    "\n",
    "*Leaf/ Terminal Node*: Nodes do not split is called Leaf or Terminal node.\n",
    "Pruning: When we remove sub-nodes of a decision node, this process is called pruning. You can say opposite process of splitting.\n",
    "\n",
    "*Branch / Sub-Tree*: A sub section of entire tree is called branch or sub-tree\n",
    "Parent and Child Node: A node, which is divided into sub-nodes is called parent node of sub-nodes where as sub-nodes are the child of parent node.\n",
    "\n",
    "## E. Random Forest:\n",
    "Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set.\n",
    "\n",
    "Random forest is like bootstrapping algorithm with Decision tree (CART) model. Say, we have 1000 observation in the complete population with 10 variables. Random forest tries to build multiple CART models with different samples and different initial variables. For instance, it will take a random sample of 100 observation and 5 randomly chosen initial variables to build a CART model. It will repeat the process (say) 10 times and then make a final prediction on each observation. Final prediction is a function of each prediction. This final prediction can simply be the mean of each prediction.\n",
    "\n",
    "For a Random Forest Classifier, there are several different hyperparameters that can be adjusted.But the following four parameters are most important\n",
    "\n",
    "**Random Forest Hyperparameters:-**\n",
    "\n",
    "*n_estimators*: The n_estimators parameter specifies the number of trees in the forest of the model. The default value for this parameter is 10, which means that 10 different decision trees will be constructed in the random forest.\n",
    "\n",
    "*max_depth*: The max_depth parameter specifies the maximum depth of each tree. The default value for max_depth is None, which means that each tree will expand until every leaf is pure. A pure leaf is one where all of the data on the leaf comes from the same class.\n",
    "\n",
    "*min_samples_split*: The min_samples_split parameter specifies the minimum number of samples required to split an internal leaf node. The default value for this parameter is 2, which means that an internal node must have at least two samples before it can be split to have a more specific classification.\n",
    "\n",
    "*min_samples_leaf*: The min_samples_leaf parameter specifies the minimum number of samples required to be at a leaf node. The default value for this parameter is 1, which means that every leaf must have at least 1 sample that it classifies.\n",
    "\n",
    "## F. XGBOOST: \n",
    "Boosting is used to create a collection of predictors. In this technique, learners are learned sequentially with early learners fitting simple models to the data and then analysing data for errors. Consecutive trees (random sample) are fit and at every step, the goal is to improve the accuracy from the prior tree. When an input is misclassified by a hypothesis, its weight is increased so that next hypothesis is more likely to classify it correctly. This process converts weak learners into better performing model.\n",
    "\n",
    "The XGBoost library implements the gradient boosting decision tree algorithm.\n",
    "\n",
    "Gradient boosting is an approach where new models are created that predict the residuals or errors of prior models and then added together to make the final prediction. It is called gradient boosting because it uses a gradient descent algorithm to minimize the loss when adding new models.\n",
    "\n",
    "This approach supports both regression and classification predictive modelling problems.\n",
    "\n",
    "**XGBoost Hyperparameters:-**\n",
    "\n",
    "*max_depth*:The maximum depth of a tree\n",
    "min_child_weight:Defines the minimum sum of weights of all observations required in a child.\n",
    "\n",
    "*gamma*:A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.\n",
    "\n",
    "*subsample*:   Denotes the fraction of observations to be randomly chosen samples for each tree.\n",
    "\n",
    "*colsample_bytree*:  Denotes the fraction of columns to be randomly chosen  samples for each tree.\n",
    "\n",
    "*reg_alpha*:     Regularization parameter\n",
    "\n",
    "*reg_lamda*:  Regularization parameter\n",
    "\n",
    "*learning rate*:  Parameter in Gradient Descent\n",
    "\n",
    "## G. CATBOOST:\n",
    "It yields state-of-the-art results without extensive data training typically required by other machine learning methods, and provides powerful out-of-the-box support for the more descriptive data formats that accompany many business problems.\n",
    "\n",
    "“CatBoost” name comes from two words “Category” and “Boosting”.\n",
    "\n",
    "It can work with multiple Categories of data, such as audio, text, image including historical data.\n",
    "\n",
    "“Boost” comes from gradient boosting machine learning algorithm as this library is based on gradient boosting library. Gradient boosting is a powerful machine learning algorithm that is widely applied to multiple types of business challenges like fraud detection, recommendation items, forecasting and it performs well also. It can also return very good result with relatively less data, unlike DL models that need to learn from a massive amount of data.\n",
    "\n",
    "**Advantages of CatBoost Library**\n",
    "\n",
    "*Performance*: CatBoost provides state of the art results and it is competitive with any leading machine learning algorithm on the performance front.\n",
    "\n",
    "*Handling Categorical features automatically*: We can use CatBoost without any explicit pre-processing to convert categories into numbers. CatBoost converts categorical values into numbers using various statistics on combinations of categorical features and combinations of categorical and numerical features. You can read more about it here.\n",
    "\n",
    "*Robust*: It reduces the need for extensive hyper-parameter tuning and lower the chances of overfitting also which leads to more generalized models. Although, CatBoost has multiple parameters to tune and it contains parameters like the number of trees, learning rate, regularization, tree depth, fold size, bagging temperature and others. You can read about all these parameters here.\n",
    "\n",
    "*Easy-to-use*: You can use CatBoost from the command line, using an user-friendly API for both Python and R.\n",
    "\n",
    "\n",
    "# V TUNING HYPERPARAMETERS\n",
    "\n",
    "\n",
    "## A. Grid Search:- \n",
    " Grid Search is particularly used for tuning hyperparameters. Now before going into deep lets know what is a hyperparameter.\n",
    "\n",
    "A model hyperparameter is a characteristic of a model that is external to the model and whose value cannot be estimated from data. The value of the hyperparameter has to be set before the learning process begins. For example k in k-Nearest Neighbours, the number of hidden layers in Neural Networks.\n",
    "\n",
    "In contrast, a parameter is an internal characteristic of the model and its value can be estimated from data. Example, beta coefficients of linear/logistic regression or support vectors in Support Vector Machines.\n",
    "\n",
    "Grid-search is used to find the optimal hyperparameters of a model which results in the most ‘accurate’ predictions.\n",
    "\n",
    "Trying with different values of hyperparameters and checking the metrics is a long process. Grid Search does the work for us by iterating with different values that we specifically give and results the optimal one from the given.\n",
    "\n",
    "*estimator*: estimator object you created\n",
    "\n",
    "*params_grid*: the dictionary object that holds the hyperparameters you want to try\n",
    "\n",
    "*scoring*: evaluation metric that you want to use, you can simply pass a valid string/ object of evaluation metric\n",
    "\n",
    "*cv*: number of cross-validation you have to try for each selected set of hyperparameters\n",
    "\n",
    "*verbose*: you can set it to 1 to get the detailed print out while you fit the data to GridSearchCV\n",
    "\n",
    "*n_jobs*: number of processes you wish to run in parallel for this task if it -1 it will use all available processors.\n",
    "\n",
    "## B. Random Search:\n",
    "Random search is a technique where random combinations of the hyperparameters are used to find the best solution for the built model. It tries random combinations of a range of values. To optimise with random search, the function is evaluated at some number of random configurations in the parameter space.\n",
    "\n",
    "The chances of finding the optimal parameter are comparatively higher in random search because of the random search pattern where the model might end up being trained on the optimised parameters without any aliasing. Random search works best for lower dimensional data since the time taken to find the right set is less with less number of iterations. Random search is the best parameter search technique when there are less number of dimensions. In the paper Random Search for Hyper-Parameter Optimization by Bergstra and Bengio, the authors show empirically and theoretically that random search is more efficient for parameter optimization than grid search.\n",
    "\n",
    "## C. Bayesian Optimization\n",
    "Bayesian approaches, in contrast to random or grid search, keep track of past evaluation results which they use to form a probabilistic model mapping hyperparameters to a probability of a score on the objective function.\n",
    "\n",
    "In the literature, this model is called a “surrogate” for the objective function and is represented as p(y | x). The surrogate is much easier to optimize than the objective function and Bayesian methods work by finding the next set of hyperparameters to evaluate on the actual objective function by selecting hyperparameters that perform best on the surrogate function. In other words:\n",
    "\n",
    "1.Build a surrogate probability model of the objective function\n",
    "\n",
    "2.Find the hyperparameters that perform best on the surrogate\n",
    "\n",
    "3.Apply these hyperparameters to the true objective function\n",
    "\n",
    "4.Update the surrogate model incorporating the new results\n",
    "\n",
    "5.Repeat steps 2–4 until max iterations or time is reached\n",
    "\n",
    "The aim of Bayesian reasoning is to become “less wrong” with more data which these approaches do by continually updating the surrogate probability model after each evaluation of the objective function.\n",
    "\n",
    "At a high-level, Bayesian optimization methods are efficient because they choose the next hyperparameters in an informed manner. The basic idea is: spend a little more time selecting the next hyperparameters in order to make fewer calls to the objective function. In practice, the time spent selecting the next hyperparameters is inconsequential compared to the time spent in the objective function. By evaluating hyperparameters that appear more promising from past results, Bayesian methods can find better model settings than random search in fewer iterations.\n",
    "\n",
    "# VI CURSE OF DIMENSIONALITY\n",
    "\n",
    "## Principal Component Analysis\n",
    "\n",
    "When we have too many features to train in the model then there might be a chance of over fitting, so two options\n",
    "\n",
    "**Feature Elimination**: We shall drop few features and train the model with features that contribute most towards target variable. This can be done by RFE(Recursive Feature Elimination) or RFECV(RFE with Cross Validation).They can give how much each feature is important , like a list, so we can select the important features. But the only problem is we lose the information whatever the dropped features were contributing to the target variable.  So\n",
    "\n",
    "**Feature Extraction**:  PCA is a feature extraction method. In feature extraction, we create “new” independent variables, where each “new” independent variable is a combination of each of the all “old” independent variables. However, we create these new independent variables in a specific way and order these new variables by how well they predict our dependent variable.\n",
    "\n",
    "We keep as many of the new independent variables as we want, but we drop the “least important ones.” Because we ordered the new variables by how well they predict our dependent variable, we know which variable is the most important and least important. Because these new independent variables are combinations of our old ones, we’re still keeping the most valuable parts of our old variables, even when we drop one or more of these “new” variables\n",
    "\n",
    "**Pros:**\n",
    "When we want to reduce the number of features but don’t know what to drop, PCA can help us\n",
    "New variables are independent off each other\n",
    "\n",
    "**Cons:**\n",
    "New features are less interpretable\n",
    "\n",
    "# VII   EVALUATION METRICS\n",
    "\n",
    "## A. ROC\n",
    "Many machine learning models were performed on this dataset that includes Logistic and Decision Tree algorithms. The metrics which were used to determine the efficiency includes sensitivity or Recall, specificity and accuracy and also using Receiver operating charecteristics curve. The area under curve value reveals that Logistic Regression performed better than Decision Tree. \n",
    "\n",
    "\n",
    "## B. Confusion Matrix\n",
    "Performance measurement for machine learning classification problem where output can be two or more classes. It is a table with 4 different combinations of predicted and actual values.\n",
    "\n",
    "\n",
    "It is extremely useful for measuring Recall, Precision, Specificity, Accuracy and most importantly AUC-ROC Curve.\n",
    "\n",
    "## C. Classification Report\n",
    "\n",
    "A Classification report is used to measure the quality of predictions from a classification algorithm. How many predictions are True and how many are False. More specifically, True Positives, False Positives, True negatives and False Negatives are used to predict the metrics of a classification report\n",
    "\n",
    "The report shows the main classification metrics precision, recall and f1-score on a per-class basis. The metrics are calculated by using true and false positives, true and false negatives. Positive and negative in this case are generic names for the predicted classes. There are four ways to check if the predictions are right or wrong:\n",
    "\n",
    "TN / True Negative: when a case was negative and predicted negative\n",
    "\n",
    "TP / True Positive: when a case was positive and predicted positive\n",
    "\n",
    "FN / False Negative: when a case was positive but predicted negative\n",
    "\n",
    "FP / False Positive: when a case was negative but predicted positive\n",
    "\n",
    "## D. Precision\n",
    "An ability of a classifier not to label positive to the negatives\n",
    "When you say a male is pregnant, which is not possible yet, then this would be detected under this precision score.\n",
    "(Number of true positive cases) / (Number of all the positive cases)\n",
    "*all the positive classes = true positive + false positive\n",
    "\n",
    "## E. Recall\n",
    "\n",
    "An ability of a classifier to find all positive instances. So, only corrected measured instances, which are true-positive and false-negatives, are concerned.\n",
    "(Number of true positives) / (# of true positives + # of false negatives)\n",
    "*# signifies ‘number’\n",
    "By far, we can tell both Precision and Recall focus on true-positive cases in different perspectives.\n",
    "\n",
    "## F. F1-score\n",
    "\n",
    "This is a weighted harmonic mean value using both Precision and Recall. This measure is pretty useful when the dataset has an imbalanced distribution of different labels.\n",
    "{(Precision * Recall) * 2} / (Precision + Recall)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# VIII CONCLUSION\n",
    "Primarily in this work classification is performed using machine learning techniques. Various techniques deployed were compared and contrasted in terms of evaluation metrics. Most of the classification tasks will suit to be processed in the pipline discussed in above sections. In future this work can extend to explore the  potential of Neural Networks for classification tasks. \n",
    "\n",
    "# IX REFERENCES\n",
    "\n",
    "1.   [EDA + Logistic Regression + PCA by *Prashant Banerjee*](https://www.kaggle.com/prashant111/eda-logistic-regression-pca)\n",
    "\n",
    "1.   [Income Prediction (84.369% Accuracy) by *IPByrne*](https://www.kaggle.com/ipbyrne/income-prediction-84-369-accuracy)\n",
    "2.   [Multiple ML Techniques and Analysis of Dataset by *Matt Green* ](https://www.kaggle.com/bananuhbeatdown/multiple-ml-techniques-and-analysis-of-dataset)\n",
    "\n",
    "2.   [Catboost and other class.algos with 88% accuracy by *Kanav Anand*](https://www.kaggle.com/kanav0183/catboost-and-other-class-algos-with-88-accuracy)\n",
    "\n",
    "2.   [EDA and Income predictions (86.75 % accuracy) by *Sumit Mishra*](https://www.kaggle.com/sumitm004/eda-and-income-predictions-86-75-accuracy)\n",
    "\n",
    "1.   [Income prediction using Random Forest and XGBoost by *Nitineshwar*](https://www.kaggle.com/grayphantom/income-prediction-using-random-forest-and-xgboost)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjOKMOoA10V5"
   },
   "source": [
    "Now lets dive into the code and see how to solve the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bf0AuDX10Rs"
   },
   "source": [
    "##  Importing necessary Libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Catboost algorithm might need installation, the below code is in the comment form. Run it if installation is required "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "J9Rcvcdc33JP"
   },
   "outputs": [],
   "source": [
    "#pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TXb7HZfjA6-s",
    "outputId": "72153f31-19cd-4c30-d73e-b7b8e7111bea"
   },
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.metrics import roc_curve, roc_auc_score,accuracy_score,f1_score,log_loss,confusion_matrix,classification_report,precision_score,recall_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.datasets import StandardDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "import matplotlib.patches as patches\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "#from packages import *\n",
    "#from ml_fairness import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Un3kdwWlBNCY",
    "outputId": "fcd7ae36-f4ea-4964-cba8-c6649ea2a0fa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education.num</th>\n",
       "      <th>marital.status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital.gain</th>\n",
       "      <th>capital.loss</th>\n",
       "      <th>hours.per.week</th>\n",
       "      <th>native.country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90</td>\n",
       "      <td>?</td>\n",
       "      <td>77053</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>?</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>Private</td>\n",
       "      <td>132870</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>18</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66</td>\n",
       "      <td>?</td>\n",
       "      <td>186061</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>?</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>Private</td>\n",
       "      <td>140359</td>\n",
       "      <td>7th-8th</td>\n",
       "      <td>4</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>Private</td>\n",
       "      <td>264663</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age workclass  fnlwgt     education  education.num marital.status  \\\n",
       "0   90         ?   77053       HS-grad              9        Widowed   \n",
       "1   82   Private  132870       HS-grad              9        Widowed   \n",
       "2   66         ?  186061  Some-college             10        Widowed   \n",
       "3   54   Private  140359       7th-8th              4       Divorced   \n",
       "4   41   Private  264663  Some-college             10      Separated   \n",
       "\n",
       "          occupation   relationship   race     sex  capital.gain  \\\n",
       "0                  ?  Not-in-family  White  Female             0   \n",
       "1    Exec-managerial  Not-in-family  White  Female             0   \n",
       "2                  ?      Unmarried  Black  Female             0   \n",
       "3  Machine-op-inspct      Unmarried  White  Female             0   \n",
       "4     Prof-specialty      Own-child  White  Female             0   \n",
       "\n",
       "   capital.loss  hours.per.week native.country income  \n",
       "0          4356              40  United-States  <=50K  \n",
       "1          4356              18  United-States  <=50K  \n",
       "2          4356              40  United-States  <=50K  \n",
       "3          3900              40  United-States  <=50K  \n",
       "4          3900              40  United-States  <=50K  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading Dataset\n",
    "dataset = pd.read_csv('../../Data/adult.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PzoHR5qUznrE"
   },
   "source": [
    "We have two variables regarding Education, \"education\" and \"education.num\".\"education.num\" is the categorical version of \"education\" so we can drop the \"education\" variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "YFa38J90-q-i"
   },
   "outputs": [],
   "source": [
    "#Dropping education since we have its categorical coloumn education.num\n",
    "dataset.drop(columns=['education'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MFQ4ccmT8R8f",
    "outputId": "15646796-25b2-4f33-d0a7-85e6aa817639"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                int64\n",
       "workclass         object\n",
       "fnlwgt             int64\n",
       "education.num      int64\n",
       "marital.status    object\n",
       "occupation        object\n",
       "relationship      object\n",
       "race              object\n",
       "sex               object\n",
       "capital.gain       int64\n",
       "capital.loss       int64\n",
       "hours.per.week     int64\n",
       "native.country    object\n",
       "income            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data types of columns\n",
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "RWxKLPbBBSL8",
    "outputId": "630864e3-aa31-42b8-bed5-4de766c54a39"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'workclass', 'fnlwgt', 'education.num', 'marital.status',\n",
       "       'occupation', 'relationship', 'race', 'sex', 'capital.gain',\n",
       "       'capital.loss', 'hours.per.week', 'native.country', 'income'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#List of columns present in Dataset\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "W4Nr99FPYcr_",
    "outputId": "5e0da855-3c63-4422-942f-fecde665cabb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32561, 14)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shape of Dataset\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cclCvZT50H32"
   },
   "source": [
    "All these columns names consits of \".\", which cause problem while calling the columns, so replacing all the \".\" with \"_\" in the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "F75XoTrkBU-H"
   },
   "outputs": [],
   "source": [
    "#Renaming few columns\n",
    "dataset.rename(columns = {'education.num':'education_num', 'marital.status':'marital_status', 'capital.gain':'capital_gain',\n",
    "                          'capital.loss':'capital_loss','hours.per.week':'hours_per_week','native.country':'native_country'}, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "et11jQCSBbeI",
    "outputId": "395b7845-a6eb-43d3-a40e-5f3f82677582"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'workclass', 'fnlwgt', 'education_num', 'marital_status',\n",
       "       'occupation', 'relationship', 'race', 'sex', 'capital_gain',\n",
       "       'capital_loss', 'hours_per_week', 'native_country', 'income'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the change\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PL95sW5p0dd4"
   },
   "source": [
    "\"?\" represents null values , lets replace them with np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "cH9yKrQ1BfXR"
   },
   "outputs": [],
   "source": [
    "#Replacing \"?\" with NAN\n",
    "dataset['workclass'].replace('?', np.nan, inplace= True)\n",
    "dataset['occupation'].replace('?', np.nan, inplace= True)\n",
    "dataset['native_country'].replace('?', np.nan, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6fk5BXJnB7Iy",
    "outputId": "de199858-c145-4f7f-bf9d-b813403a1ede"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>32561.000000</td>\n",
       "      <td>30725</td>\n",
       "      <td>3.256100e+04</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561</td>\n",
       "      <td>30718</td>\n",
       "      <td>32561</td>\n",
       "      <td>32561</td>\n",
       "      <td>32561</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>31978</td>\n",
       "      <td>32561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Private</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>22696</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14976</td>\n",
       "      <td>4140</td>\n",
       "      <td>13193</td>\n",
       "      <td>27816</td>\n",
       "      <td>21790</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29170</td>\n",
       "      <td>24720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.581647</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.897784e+05</td>\n",
       "      <td>10.080679</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1077.648844</td>\n",
       "      <td>87.303830</td>\n",
       "      <td>40.437456</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.640433</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.055500e+05</td>\n",
       "      <td>2.572720</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7385.292085</td>\n",
       "      <td>402.960219</td>\n",
       "      <td>12.347429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.228500e+04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.178270e+05</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.783560e+05</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.370510e+05</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.484705e+06</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>4356.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 age workclass        fnlwgt  education_num  \\\n",
       "count   32561.000000     30725  3.256100e+04   32561.000000   \n",
       "unique           NaN         8           NaN            NaN   \n",
       "top              NaN   Private           NaN            NaN   \n",
       "freq             NaN     22696           NaN            NaN   \n",
       "mean       38.581647       NaN  1.897784e+05      10.080679   \n",
       "std        13.640433       NaN  1.055500e+05       2.572720   \n",
       "min        17.000000       NaN  1.228500e+04       1.000000   \n",
       "25%        28.000000       NaN  1.178270e+05       9.000000   \n",
       "50%        37.000000       NaN  1.783560e+05      10.000000   \n",
       "75%        48.000000       NaN  2.370510e+05      12.000000   \n",
       "max        90.000000       NaN  1.484705e+06      16.000000   \n",
       "\n",
       "            marital_status      occupation relationship   race    sex  \\\n",
       "count                32561           30718        32561  32561  32561   \n",
       "unique                   7              14            6      5      2   \n",
       "top     Married-civ-spouse  Prof-specialty      Husband  White   Male   \n",
       "freq                 14976            4140        13193  27816  21790   \n",
       "mean                   NaN             NaN          NaN    NaN    NaN   \n",
       "std                    NaN             NaN          NaN    NaN    NaN   \n",
       "min                    NaN             NaN          NaN    NaN    NaN   \n",
       "25%                    NaN             NaN          NaN    NaN    NaN   \n",
       "50%                    NaN             NaN          NaN    NaN    NaN   \n",
       "75%                    NaN             NaN          NaN    NaN    NaN   \n",
       "max                    NaN             NaN          NaN    NaN    NaN   \n",
       "\n",
       "        capital_gain  capital_loss  hours_per_week native_country income  \n",
       "count   32561.000000  32561.000000    32561.000000          31978  32561  \n",
       "unique           NaN           NaN             NaN             41      2  \n",
       "top              NaN           NaN             NaN  United-States  <=50K  \n",
       "freq             NaN           NaN             NaN          29170  24720  \n",
       "mean     1077.648844     87.303830       40.437456            NaN    NaN  \n",
       "std      7385.292085    402.960219       12.347429            NaN    NaN  \n",
       "min         0.000000      0.000000        1.000000            NaN    NaN  \n",
       "25%         0.000000      0.000000       40.000000            NaN    NaN  \n",
       "50%         0.000000      0.000000       40.000000            NaN    NaN  \n",
       "75%         0.000000      0.000000       45.000000            NaN    NaN  \n",
       "max     99999.000000   4356.000000       99.000000            NaN    NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A detailed description of the datset\n",
    "dataset.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNc6ujYx3Eoq"
   },
   "source": [
    "## Handling the Missing values\n",
    "\n",
    "Every datset have some missing values, lets find out in which cloumns they are?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "bbmUEm8ICHE_",
    "outputId": "bc8c5251-cf4d-4b92-9f18-1f7031eae5fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                  0\n",
       "workclass         1836\n",
       "fnlwgt               0\n",
       "education_num        0\n",
       "marital_status       0\n",
       "occupation        1843\n",
       "relationship         0\n",
       "race                 0\n",
       "sex                  0\n",
       "capital_gain         0\n",
       "capital_loss         0\n",
       "hours_per_week       0\n",
       "native_country     583\n",
       "income               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of null values in the dataset column wise\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "HuAUmKFphLAk",
    "outputId": "e0346320-4488-4070-ed23-d607c158defa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Barplot of Workclass Variable')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Grouping Workclass\n",
    "dataset.groupby(['workclass']).size().plot(kind=\"bar\",fontsize=14)\n",
    "plt.xlabel('Work Class Categories')\n",
    "plt.ylabel('Count of People')\n",
    "plt.title('Barplot of Workclass Variable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ut2qyixB0qbv"
   },
   "source": [
    "All the null values in the \"workclass\" can be replaced by \"private\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bSYgHh-nhadp",
    "outputId": "42eeb826-b84e-402f-a60c-3a5a92aa372d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Barplot of Occupation Variable')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Grouping Occupation\n",
    "dataset.groupby(['occupation']).size().plot(kind=\"bar\",fontsize=14)\n",
    "plt.xlabel('Occupation Categories')\n",
    "plt.ylabel('Count of People')\n",
    "plt.title('Barplot of Occupation Variable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_B_w5ZC01bk"
   },
   "source": [
    "\"occupation\" null values cant be replaced by mode , its more or like equally distributed. So drop the null values in this column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g-v2IT0xharM",
    "outputId": "65efba47-09b1-46df-9406-11da1bf6893c"
   },
   "outputs": [],
   "source": [
    "#Grouping Native Country\n",
    "dataset.groupby(['native_country']).size().plot(kind=\"bar\",fontsize=10)\n",
    "plt.xlabel('Native Country Categories')\n",
    "plt.ylabel('Count of People')\n",
    "plt.title('Barplot of Native Country Variable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bo5YcOol1J6D"
   },
   "source": [
    "Its clear the null values of \"native_country\" could be easily replaced by mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jec4ZRu1CU_y",
    "outputId": "52b43a18-bfcb-4aa9-903f-b0313f14547f"
   },
   "outputs": [],
   "source": [
    "#Droping null values in occupation column\n",
    "dataset.dropna(subset=['occupation'],inplace=True)\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-wOeq_i1X7i"
   },
   "source": [
    "By dropping the null values in \"occupation\" we lose the null values in \"workclass\" also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y8mzPvcit2kL"
   },
   "outputs": [],
   "source": [
    "#Imputing null values with Mode\n",
    "\n",
    "dataset['native_country'].fillna(dataset['native_country'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UKmNAHlqt2kb",
    "outputId": "e50e0240-d336-450d-aef7-a03f81ed20a6"
   },
   "outputs": [],
   "source": [
    "#Checking for null values\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5AE390fUt2kk",
    "outputId": "466dd151-865f-441c-cf2a-57a785ec3ad0"
   },
   "outputs": [],
   "source": [
    "#Confirming the Categorical Features\n",
    "categorical_feature_mask = dataset.dtypes==object\n",
    "categorical_feature_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOwv-NSs3PMu"
   },
   "source": [
    "## Label Encoding\n",
    "\n",
    "All the categorical columns and the columns with text data are being Label Encodeded in this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xbh_SJKIt2kr"
   },
   "outputs": [],
   "source": [
    "##Label encoding the all the categorical features\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "cat_list=['income','workclass','marital_status','occupation','relationship','race','sex','native_country']\n",
    "dataset[cat_list]=dataset[cat_list].apply(lambda x:le.fit_transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xX7bL3WxugnW",
    "outputId": "46450ad6-ec91-4d80-a1aa-a830c1b5680f"
   },
   "outputs": [],
   "source": [
    "#Number of categories in dataset\n",
    "dataset.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_Z4aEFN3UJP"
   },
   "source": [
    "## Correlation\n",
    "\n",
    "To find out whether there is any relation between variables, in other terms multicollineariaty.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nm2UEYdDucz7",
    "outputId": "7e6f8912-d0cb-44bd-819e-cb69b1c4cc04"
   },
   "outputs": [],
   "source": [
    "#Finding Correlation between variables\n",
    "corr = dataset.corr()\n",
    "mask = np.zeros(corr.shape, dtype=bool)\n",
    "mask[np.triu_indices(len(mask))] = True\n",
    "plt.subplots(figsize=(10,7))\n",
    "sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns,annot=True,cmap='RdYlGn',mask = mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-uqMDaJ6t6Z"
   },
   "outputs": [],
   "source": [
    "#Dropping \"sex\" variable since it is highly correlated with \"relationship\" variable \n",
    "dataset.drop(columns=['sex'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eDA395Np-7Hh"
   },
   "outputs": [],
   "source": [
    "#Slicing dataset into Independent(X) and Target(y) varibles\n",
    "y = dataset.pop('income')\n",
    "X = dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3eFvBUVw50SE"
   },
   "outputs": [],
   "source": [
    "#Scaling the dependent variables\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kUzutCCU_Y3J",
    "outputId": "146908f1-175c-4533-c938-2d09e082e51c"
   },
   "outputs": [],
   "source": [
    "#Dividing dataset into test and train\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3TMNoAaB4lZL"
   },
   "outputs": [],
   "source": [
    "#Performing Recursive Feauture Elimation with Cross Validation\n",
    "#Using Random forest for RFE-CV and logloss as scoring\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "clf_rf=RandomForestClassifier(random_state=0)\n",
    "rfecv=RFECV(estimator=clf_rf, step=1,cv=5,scoring='neg_log_loss')\n",
    "rfecv=rfecv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n5oPAzaW4oTI",
    "outputId": "aebb6d1d-e64d-404e-d35e-6d0b6f8a7cc7"
   },
   "outputs": [],
   "source": [
    "#Optimal number of features\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_test = pd.DataFrame(X_test)\n",
    "print('Optimal number of features :', rfecv.n_features_)\n",
    "print('Best features :', X_train.columns[rfecv.support_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ikq_JzbH5lg-"
   },
   "outputs": [],
   "source": [
    "#Feauture Ranking\n",
    "clf_rf = clf_rf.fit(X_train,y_train)\n",
    "importances = clf_rf.feature_importances_\n",
    "\n",
    "std = np.std([tree.feature_importances_ for tree in clf_rf.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pLasDzFZ5syQ"
   },
   "outputs": [],
   "source": [
    "#Selecting the Important Features\n",
    "X_train = X_train.iloc[:,X_train.columns[rfecv.support_]]\n",
    "X_test = X_test.iloc[:,X_test.columns[rfecv.support_]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KKrXYvss5vx-",
    "outputId": "ea47e266-c5ef-4a75-e7e0-07153006ed3a"
   },
   "outputs": [],
   "source": [
    "#Creating anew dataframe with column names and feature importance\n",
    "dset = pd.DataFrame()\n",
    "data1 = dataset\n",
    "\n",
    "dset['attr'] = data1.columns\n",
    "\n",
    "\n",
    "dset['importance'] = clf_rf.feature_importances_\n",
    "#Sorting with importance column\n",
    "dset = dset.sort_values(by='importance', ascending=True)\n",
    "\n",
    "#Barplot indicating Feature Importance\n",
    "plt.figure(figsize=(16, 14))\n",
    "plt.barh(y=dset['attr'], width=dset['importance'], color='#1976D2')\n",
    "plt.title('RFECV - Feature Importances', fontsize=20, fontweight='bold', pad=20)\n",
    "plt.xlabel('Importance', fontsize=14, labelpad=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hz7TTPdu3d6N"
   },
   "source": [
    "# 3 CLASSIFICATION MODEL BUILDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XEM_NOwp6CiY"
   },
   "outputs": [],
   "source": [
    "classifier_lg = LogisticRegression(random_state=0)\n",
    "classifier_dt = DecisionTreeClassifier(random_state=0)\n",
    "classifier_nb = GaussianNB()\n",
    "classifier_knn = KNeighborsClassifier()\n",
    "classifier_rf = RandomForestClassifier(random_state=0)\n",
    "classifier_xgb = XGBClassifier(random_state=0)\n",
    "classifier_cgb = CatBoostClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umKdtsku6Ir6"
   },
   "source": [
    "## Training with didfferent Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1PLTrGpF_osJ",
    "outputId": "6c2ea972-831a-42e4-9353-0538b136d7c1"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Instantiate the classfiers and make a list\n",
    "classifiers = [classifier_lg,\n",
    "               classifier_dt,\n",
    "               classifier_nb,\n",
    "               classifier_knn,\n",
    "               classifier_rf,\n",
    "               classifier_xgb,\n",
    "               classifier_cgb]\n",
    "# Define a result table as a DataFrame\n",
    "result_table = pd.DataFrame(columns=['classifiers', 'fpr','tpr','Roc Auc','Accuracy','f1 Score','logloss','Confusion Matrix','Precision','Recall'])\n",
    "\n",
    "# Train the models and record the results\n",
    "for cls in classifiers:\n",
    "    model = cls.fit(X_train, y_train)\n",
    "    y_proba = model.predict_proba(X_test)[::,1]\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(cls, '\\n','Confusion Matrix','\\n',confusion_matrix(y_test,  y_pred))\n",
    "    print('\\n','Classification Report','\\n',classification_report(y_test,  y_pred))\n",
    "    print('='*170)\n",
    "    fpr, tpr, _ = roc_curve(y_test,  y_proba)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    Accuracy = accuracy_score(y_test,y_pred)\n",
    "    f1score = f1_score(y_test,y_pred)\n",
    "    logloss = log_loss(y_test,y_proba)\n",
    "    cm = confusion_matrix(y_test,  y_pred)\n",
    "    precision = precision_score(y_test,  y_pred)\n",
    "    recall = recall_score(y_test,  y_pred)\n",
    "  \n",
    "    \n",
    "    result_table = result_table.append({'classifiers':cls.__class__.__name__,\n",
    "                                        'fpr':fpr, \n",
    "                                        'tpr':tpr, \n",
    "                                        'Roc Auc':auc,\n",
    "                                        'Accuracy':Accuracy,\n",
    "                                        'f1 Score':f1score,\n",
    "                                        'logloss':logloss,\n",
    "                                        'Confusion Matrix': cm,\n",
    "                                        'Precision':precision,\n",
    "                                        'Recall':recall}, ignore_index=True)\n",
    "\n",
    "# Set name of the classifiers as index labels\n",
    "result_table.set_index('classifiers', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSCu66396XDp"
   },
   "source": [
    "## Roc Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KqtUFsxL6ZBI",
    "outputId": "5d0b8eb6-aa09-4ede-f22b-cdfe9e166120"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "\n",
    "for i in result_table.index:\n",
    "    plt.plot(result_table.loc[i]['fpr'], \n",
    "             result_table.loc[i]['tpr'], \n",
    "             label=\"{}, AUC={:.3f}\".format(i, result_table.loc[i]['Roc Auc']))\n",
    "    \n",
    "plt.plot([0,1], [0,1], color='orange', linestyle='--')\n",
    "\n",
    "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
    "plt.xlabel(\"Flase Positive Rate\", fontsize=15)\n",
    "\n",
    "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
    "\n",
    "plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\n",
    "plt.legend(prop={'size':13}, loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GH8Tw0qI_tbB",
    "outputId": "35f990a5-6faf-4ac6-9811-418dc363c1a9"
   },
   "outputs": [],
   "source": [
    "result_table[['Roc Auc','Accuracy','f1 Score','logloss','Confusion Matrix','Precision','Recall']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTb4EcqM8S8c"
   },
   "source": [
    "CatBoost, XGBoost both performed well, both gave good accuracy, f1score, log_loss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
