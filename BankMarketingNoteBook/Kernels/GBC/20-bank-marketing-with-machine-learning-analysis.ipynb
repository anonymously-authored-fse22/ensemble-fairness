{"cells":[{"metadata":{"_uuid":"3892852bcb88b4d5b7b833b0caed65c84f4c51cd"},"cell_type":"markdown","source":"# Bank Marketing with Machine Learning"},{"metadata":{"_uuid":"bdda5f8833d0ea7ca5dbe84c1f1eed7d01a33dfc"},"cell_type":"markdown","source":"## Introduction"},{"metadata":{"_uuid":"834e927137c2a5025e9d8ca3fe4da376127ad59d"},"cell_type":"markdown","source":"\nMarketing to potential customers have traditionally been conducted through phone calls and emails but with machine learning we can use algorithms to calculate the people with best chance of buying a bank term deposit! A Portuguese banking institution conducted direct marketing campaigns which this data set is based off on. More than one contact to a client was required, in order to know if the product (bank term deposit) was subscribed by a client or not. Our goal is to predict if a client will subscribe to the bank term deposit (yes/no), which is done by classification!\n\nThe marketing campaigns dataset contains 21 columns including the output (y) – end result. I am going to discard the output column and use the remaining columns to find the most relatable independent variables (x) – features, that will predict if a customer will subscribe to a bank deposit or not. Let’s get started!"},{"metadata":{"_uuid":"a063040d41a1b672fc012df83b727b869f825a4d"},"cell_type":"markdown","source":"## Dataset"},{"metadata":{"_uuid":"86e3a28ae34cadcda9b80cc9995dae70e8e0e7af"},"cell_type":"markdown","source":"### Input variables:\n\n1 - age (numeric)\n\n2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n\n3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n\n4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n\n5 - default: has credit in default? (categorical: 'no','yes','unknown')\n\n6 - housing: has housing loan? (categorical: 'no','yes','unknown')\n\n7 - loan: has personal loan? (categorical: 'no','yes','unknown')\n\n### Related with the last contact of the current campaign:\n\n8 - contact: contact communication type (categorical: 'cellular','telephone') \n\n9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n\n10 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n\n11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n\n### Other attributes:\n\n12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n\n13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n\n14 - previous: number of contacts performed before this campaign and for this client (numeric)\n\n15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n\n### Social and economic context attributes:\n\n16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)\n\n17 - cons.price.idx: consumer price index - monthly indicator (numeric) \n\n18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric) \n\n19 - euribor3m: euribor 3 month rate - daily indicator (numeric)\n\n20 - nr.employed: number of employees - quarterly indicator (numeric)\n\n### Output variable (desired target):\n\n21 - y - has the client subscribed a term deposit? (binary: 'yes','no')\n\nSource:https://archive.ics.uci.edu/ml/machine-learning-databases/00222/ \n\nDataset has 40,000+ rows of data."},{"metadata":{"_uuid":"2a5999baaf4e33a99ae5ade4088ea1c8f1b4dfd9"},"cell_type":"markdown","source":"# Project Definition"},{"metadata":{"_uuid":"c7da635c519fb15621d078618636e406c363d875"},"cell_type":"markdown","source":"The classification goal is to predict if a client will subscribe to the bank term deposit (yes/no).\n"},{"metadata":{"_uuid":"9838a16b2287b43057d70679900a2503180577ba"},"cell_type":"markdown","source":"# Data Exploration"},{"metadata":{"_uuid":"79a79dd6cc679a14a7a0364591d3b8e773e14e05"},"cell_type":"markdown","source":"I started by importing the pandas package which is used for manupulation of data. Then, I loaded the dataset into the dataframe (df)."},{"metadata":{"trusted":true,"_uuid":"48134ff6f8e843b1380909f5528a8f66b73b61fb"},"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('../input/bank-additional-full.csv', sep=';')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3e84c2f4348d34c862e38483dda50c6f5ecba54b"},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a8c7d3dcf894018f86061791325db29cb1b69b5f"},"cell_type":"code","source":"df[list(df.columns)[6:]].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d757c86b7c077666bd0e67d9ab515b50415620ad"},"cell_type":"code","source":"df.groupby('y').size()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6e5d3ab6a892726375ca5dbd36f47d40df3f548"},"cell_type":"markdown","source":"I created a column called OUTPUT_LABEL which is going to represent 0 for the negative class and 1 for the positive class based on the bank marketing data set."},{"metadata":{"trusted":false,"_uuid":"bceef634008aa80f9144800c9b9621dab5ba6371"},"cell_type":"code","source":"df['OUTPUT_LABEL'] = (df.y == 'yes').astype('int')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7110879d7c7e59ebefb3ab0be3e4762d3c175c72"},"cell_type":"markdown","source":"The prevalence of the positive class is calculated here..."},{"metadata":{"trusted":false,"_uuid":"dadbd3ad1c4d2ae19a6d0972a81af022d80e87c0"},"cell_type":"code","source":"def calc_prevalence(y_actual):\n    # this function calculates the prevalence of the positive class (label = 1)\n    return (sum(y_actual)/len(y_actual))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"01eb8038bd81c154c51a327230e41dc8a1868060"},"cell_type":"code","source":"print('prevalence of the positive class: %.3f'%calc_prevalence(df['OUTPUT_LABEL'].values))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2708686f01ab100600e7065244c8e946105b2c6"},"cell_type":"markdown","source":"The prevlence of the positive class is 11.3% which means that the proportion of people who agreed to a term deposit (positive class) compared to the people who did not is 11.3%."},{"metadata":{"_uuid":"1fd20bf2f003a46e25686f12b612700abf2e099b"},"cell_type":"markdown","source":"## Exploring the data set and unique values"},{"metadata":{"_uuid":"8dfc0a522dc2d96ad404de9b5809a6acd9ed6dc6"},"cell_type":"markdown","source":"Pandas doesn't allow you to see all the columns at once, so we will look at them in groups of 10."},{"metadata":{"trusted":false,"_uuid":"12c098ed586f88e8576e19ed84198efa2245ed5d"},"cell_type":"code","source":"df[list(df.columns)[:10]].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d0fef0deff1e4ef09455c8a1bb6bdf73a1906fda"},"cell_type":"code","source":"df[list(df.columns)[10:]].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"70a0e1b69699734ad074ad0f82a1f299245ad741"},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"008f59a7918dddb0cf8f193d86fcb43e5c0faeeb"},"cell_type":"code","source":"print('Number of columns:',len(df.columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c1cfc367ef6bd1776fe1db2f19d325f4063e07c0"},"cell_type":"code","source":"# for each column\nfor a in list(df.columns):\n    \n    # get a list of unique values\n    n = df[a].unique()\n    \n    # if number of unique values is less than 30, print the values. Otherwise print the number of unique values\n    if len(n)<30:\n        print(a)\n        print(n)\n    else:\n        print(a + ': ' +str(len(n)) + ' unique values')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"357c8d1ab599a991b6c38585d68f94949b98be13"},"cell_type":"markdown","source":"### Key Observations:"},{"metadata":{"_uuid":"09e6a174aba7ef05016499c537c3862e95bcda15"},"cell_type":"markdown","source":"- From the output of the code, we can see that there are roughly the same amount of categorical and numeric values in the columns.\n\n- age, duration, campaign, pdays, previous, emp.var.rate, cons.price.idx, cons.conf.idx, euribor3m and nr.employed are numerical variables.\n\n- All the data inputted are non-null values, meaning that we have a value for every column.\n\n- Output (y) has two values: \"yes\" and \"no\".\n\n- default, housing and loan have 3 values each (yes, no and unknown).\n\n- We are discarding duration. This attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n\n- age has 78 unique variables, so we are going to break it down to less than or equal to 35 and greater than 35.\n\n- euribor3m (3 month rate - daily indicator) has 316 unique variables, which is a lot and does not bring major insights into our data set, therefore we are going to discard the 3 month rate daily indicator.\n\n"},{"metadata":{"_uuid":"8a32f9aed4784ebad2b0ff9ef5ab132c0cb0bd69"},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{"_uuid":"7e2f0912edb07e72737f9f8a23a52e412a54a88d"},"cell_type":"markdown","source":"Feature Engineering is classifying features such as numerial and categorical into groups in order to deeply section and analyze the data for results in machine learning algorithms."},{"metadata":{"_uuid":"2bc2e70dae3f2a2199755b752a7fe7cb26ae5719"},"cell_type":"markdown","source":"### Numerical Features"},{"metadata":{"_uuid":"8eb55df88c326236f46890b0943e1ce40b235bb1"},"cell_type":"markdown","source":"Breaking up age into two pasts - less than or equal to 35 and greater than 35. Because there are so many unique values for ages so I wanted to categorize them to make sense of the data. So that at the end of our analysis we can tell which customers that agree or don't agree to get a term deposit fall into which age category."},{"metadata":{"trusted":false,"_uuid":"ea647bd17af47cc1add6a40b98bdef238f0aaa91"},"cell_type":"code","source":"df['is_less_than_or_equal_to_35'] = (df['age'] <= 35).astype('int')\ndf['is_greater_than_35'] = (df['age'] > 35).astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c2e7317ddd9805089d005ae8bc380d0bc5070af0"},"cell_type":"code","source":"cols_num = ['campaign', 'pdays',\n       'previous', 'emp.var.rate', 'cons.price.idx','cons.conf.idx', 'nr.employed','is_less_than_or_equal_to_35','is_greater_than_35']","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"ae9edab34f93f4d29eacdb4c64262b2c2faddd01"},"cell_type":"code","source":"df[cols_num].head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce32e4294f212d3508253b80c4a3125f3a46a34a"},"cell_type":"markdown","source":"### Graphical Representation of Numerical Features"},{"metadata":{"trusted":false,"_uuid":"ebd6c29cd899f7747a4e850133bdcf2ba9c22f55"},"cell_type":"code","source":"df[cols_num].hist(column=cols_num, figsize = (16,16))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"af7c290e6ddadd3be9e3b3344c17a92308806ff1"},"cell_type":"markdown","source":"Let's check if there are any missing values in the numerical data. "},{"metadata":{"trusted":false,"_uuid":"67d51001f3b5d53b17ac0d8dd5fe2f3c92762a11"},"cell_type":"code","source":"df[cols_num].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e089f5b478a7115cca39d1756e543edd072b8b50"},"cell_type":"markdown","source":"## Categorical Features"},{"metadata":{"_uuid":"2c3f68d3120e6c5e2e14b86816d2dab8d50af411"},"cell_type":"markdown","source":"Categorical variables are non-numeric data such as race and gender. To turn these non-numerical data into variables, the simplest thing is to use a technique called one-hot encoding, which will be explained below.\n\nThe first set of categorical data we will deal with are these columns:"},{"metadata":{"trusted":false,"_uuid":"a3516371d67f35bd8b6c09ba180e9e089395569e"},"cell_type":"code","source":"cols_cat = ['job', 'marital', \n       'education', 'default',\n       'housing', 'loan', 'contact', 'month',\n       'day_of_week', 'poutcome']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83dbd313d74755a1b8cfcc009ba55be1e0c4eda3"},"cell_type":"markdown","source":"Let's check if there are any missing data"},{"metadata":{"trusted":false,"_uuid":"17e589f6cd14dc5fe4aab6027c480aa611ba1f37"},"cell_type":"code","source":"df[cols_cat].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd981fb2908f17a688689839ef70c845a1073115"},"cell_type":"markdown","source":"## One-Hot Encoding"},{"metadata":{"_uuid":"ecc9f57199f6c92f88fd13745110f4229f84d594"},"cell_type":"markdown","source":"Now we are going to use the one-hot enconding feature. This feature creates a unique column for each entry of every categorical variable so we can deeply anlyze them. "},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"b10bea20dccf2380cf7fa32851df4a660d97c0b1"},"cell_type":"code","source":"cols_cat = ['job', 'marital', \n       'education', 'default',\n       'housing', 'loan', 'contact', 'month',\n       'day_of_week', 'poutcome']\ndf[cols_cat]\ncols_new_cat=pd.get_dummies(df[cols_cat],drop_first = True)\ncols_new_cat","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d9069484425cc3e43047ef5f726e46f5ee55315a"},"cell_type":"markdown","source":"### Graphical Representation of Categorical Features"},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"29d93c18b3621db8b954f0f3ca59454775456e2f"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfig, ax = plt.subplots()\nfig.set_size_inches(25, 8)\nsns.countplot(x = 'education', data = df[cols_cat])\nax.set_xlabel('Levels of Education', fontsize=16)\nax.set_ylabel('Number', fontsize=16)\nax.set_title('Education', fontsize=16)\nax.tick_params(labelsize=16)\nsns.despine()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"aa1e6a7717b7cab95995f3ae237d29177a63060f"},"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.set_size_inches(25, 8)\nsns.countplot(x = 'marital', data = df[cols_cat])\nax.set_xlabel('Marital Status', fontsize=16)\nax.set_ylabel('Count', fontsize=16)\nax.set_title('Marital', fontsize=16)\nax.tick_params(labelsize=16)\nsns.despine()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"75301ba28c59b579adfb52b5852b243df4ed2320"},"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.set_size_inches(25, 8)\nsns.countplot(x = 'job', data = df[cols_cat])\nax.set_xlabel('Types of Jobs', fontsize=16)\nax.set_ylabel('Number', fontsize=16)\nax.set_title('Job', fontsize=16)\nax.tick_params(labelsize=16)\nsns.despine()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"5b359251938b922dcddcd08b90a7dbbcd133c300"},"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.set_size_inches(25, 8)\nsns.countplot(x = 'default', data = df[cols_cat])\nax.set_xlabel('Default Status', fontsize=16)\nax.set_ylabel('Number of Defaults', fontsize=16)\nax.set_title('Defaults', fontsize=16)\nax.tick_params(labelsize=16)\nsns.despine()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7f5166c9195c452f95c46c4731e7fe974cdd94aa"},"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.set_size_inches(25, 8)\nsns.countplot(x = 'housing', data = df[cols_cat])\nax.set_xlabel('Kind of Housing', fontsize=16)\nax.set_ylabel('Housing Count', fontsize=16)\nax.set_title('Housing', fontsize=16)\nax.tick_params(labelsize=16)\nsns.despine()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"51a16242142b41344d89c659f4834dd90258f029"},"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.set_size_inches(25, 8)\nsns.countplot(x = 'loan', data = df[cols_cat])\nax.set_xlabel('Loan Status', fontsize=16)\nax.set_ylabel('Number of Loans', fontsize=16)\nax.set_title('Loan', fontsize=16)\nax.tick_params(labelsize=16)\nsns.despine()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"332be7772125bf228355429027e06f3e860ee9b1"},"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.set_size_inches(25, 8)\nsns.countplot(x = 'contact', data = df[cols_cat])\nax.set_xlabel('Type of Contact', fontsize=16)\nax.set_ylabel('Number of Contacts', fontsize=16)\nax.set_title('Contacts', fontsize=16)\nax.tick_params(labelsize=16)\nsns.despine()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fe2478ea5e28ce0c9e2042a998469fc6db421d42"},"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.set_size_inches(25, 8)\nsns.countplot(x = 'month', data = df[cols_cat])\nax.set_xlabel('Months', fontsize=16)\nax.set_ylabel('Month Count', fontsize=16)\nax.set_title('Month', fontsize=16)\nax.tick_params(labelsize=16)\nsns.despine()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1e3352a6d6c07b64f450f3c7ef501a5e315ac5f4"},"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.set_size_inches(25, 8)\nsns.countplot(x = 'day_of_week', data = df[cols_cat])\nax.set_xlabel('Day', fontsize=16)\nax.set_ylabel('Day Count', fontsize=16)\nax.set_title('Day of the Week', fontsize=16)\nax.tick_params(labelsize=16)\nsns.despine()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"39774adee8f51aef4e7107da69f101549e3614cc"},"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.set_size_inches(25, 8)\nsns.countplot(x = 'poutcome', data = df[cols_cat])\nax.set_xlabel('Previous Marketing Campaign Outcome', fontsize=16)\nax.set_ylabel('Number of Previous Outcomes', fontsize=16)\nax.set_title('poutcome (Previous Marketing Campaign Outcome)', fontsize=16)\nax.tick_params(labelsize=16)\nsns.despine()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"246d0ffa3d10e47b6e708d29acbe3bbde50cb068"},"cell_type":"markdown","source":"In order to add the one-hot encoding columns to the dataframe, we use the concat function. axis = 1 is used to add the columns. "},{"metadata":{"trusted":false,"_uuid":"8eddb142405e176117a91a33e779a6334d50e49f"},"cell_type":"code","source":"df = pd.concat([df,cols_new_cat], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d446b6e9da5c868e5c41ddcd16d6811c93d37dd6"},"cell_type":"code","source":"cols_all_cat=list(cols_new_cat.columns)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"1c9bdd90ce171375feddc01d972713f8b998cda6"},"cell_type":"code","source":"df[cols_all_cat]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ebac4709747a0e2bfd73c062a56f4ee79c166da3"},"cell_type":"markdown","source":"I removed Campaign column from the dataset because there are so many unique values and it does not make any lasting effect in the dataset for affecting the outcome of a person opening a bank term deposit"},{"metadata":{"trusted":false,"_uuid":"16a15524fbef9970c8c9621f7c7184ff262fa12b"},"cell_type":"code","source":"cols_new_num = ['pdays',\n       'previous', 'emp.var.rate', 'cons.price.idx','cons.conf.idx','nr.employed','is_less_than_or_equal_to_35','is_greater_than_35']\ndf[cols_new_num].head(12)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ae2d1d59cef225d098cdab70c6429ab753823b9"},"cell_type":"markdown","source":"### Summary of Features Engineering "},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"0c8a7205c94e3c8098422e9f835a73a4f7d06436"},"cell_type":"code","source":"print('Total number of features:', len(cols_all_cat+cols_new_num))\nprint('Numerical Features:',len(cols_new_num))\nprint('Categorical Features:',len(cols_all_cat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"df00b6ab48daf9e86dd34d030002aa5fb7e9116d"},"cell_type":"code","source":"df[cols_new_num].head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b82fd4d644cebae372a60fac29d971e0f289191"},"cell_type":"markdown","source":"Data check for missing values"},{"metadata":{"trusted":false,"_uuid":"045439b792ccd53019a234a58aa8453825580e65"},"cell_type":"code","source":"df[cols_new_num+cols_all_cat].isnull().sum().sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10ce37250e35a88e6e72dc2fd34d74590131ed7d"},"cell_type":"markdown","source":"Good to go! No empty cells! Also, I created a new dataframe below, which includes the columns of interest. "},{"metadata":{"trusted":false,"_uuid":"ef40e6496911ef7c973c3e2aa390a9c088130bdd"},"cell_type":"code","source":"cols_input = cols_new_num + cols_all_cat\ndf_data = df[cols_input + ['OUTPUT_LABEL']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c67a4b7d9066fe722eec3238ce9dfadb08bd13d1"},"cell_type":"code","source":"cols_input","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"dff87fe24375dd17fdd66eac236ccfd6200c75ae"},"cell_type":"code","source":"len(cols_input)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"409aec2f8c08051cb633785bb39453f655baf41b"},"cell_type":"code","source":"df_data.head(6)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76957e640852f54772794ab194846714dd0f6fb8"},"cell_type":"markdown","source":"## Building Training, Validation & Test Samples"},{"metadata":{"_uuid":"18f1f1f814a4294bc9d4893dd653f46991c2b7b9"},"cell_type":"markdown","source":"Training samples: these are samples from the data set used to train the model. It can be 70% of the data.\nValidation samples: these are samples used to validate or make decisions from the model. It can be 15% of the data.\nTest samples: these are samples used to measure the accuracy or performace of the model. It can be 15% of the data."},{"metadata":{"_uuid":"f45da0df23bf9536f72b1c3e27a15d083f6afb86"},"cell_type":"markdown","source":"The training (df_train_all), validation (df_valid) and test (df_test) set were created below."},{"metadata":{"_uuid":"3789f65fb0e0cb5c4d14d437125999b835d1350c"},"cell_type":"markdown","source":"Shuffle the samples"},{"metadata":{"trusted":false,"_uuid":"90305b62b141ba02252c4b01ff292d504247d723"},"cell_type":"code","source":"df_data = df_data.sample(n = len(df_data), random_state = 42)\ndf_data = df_data.reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea08e02b9348167b769bcccea0bf68fdf26aa254"},"cell_type":"markdown","source":"30% of the validation and test samples:"},{"metadata":{"trusted":false,"_uuid":"65f68424526d48386a6051337e9089daced2fc99"},"cell_type":"code","source":"df_valid_test=df_data.sample(frac=0.30,random_state=42)\nprint('Split size: %.3f'%(len(df_valid_test)/len(df_data)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa77d79178ec73ea0c4c7c8273169e7700eebb3c"},"cell_type":"markdown","source":"Split into test and validation samples by 50% which makes 15% of test and 15% of validation samples."},{"metadata":{"trusted":false,"_uuid":"e33b5ef53f8c2bfa2a2e8b73f7f21a200416a4ce"},"cell_type":"code","source":"df_test = df_valid_test.sample(frac = 0.5, random_state = 42)\ndf_valid = df_valid_test.drop(df_test.index)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9adaa5b87e32ca155fd2589a150b89c09e9ffe50"},"cell_type":"markdown","source":"Use the rest of the data for the training samples"},{"metadata":{"trusted":false,"_uuid":"09eee9b59b758502faf927fd166de6fcd52a393f"},"cell_type":"code","source":"df_train_all=df_data.drop(df_valid_test.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c474f2671b875ee02d4762fc6a5983737b276471"},"cell_type":"code","source":"# check the prevalence of each \nprint('Test prevalence(n = %d):%.3f'%(len(df_test),calc_prevalence(df_test.OUTPUT_LABEL.values)))\nprint('Valid prevalence(n = %d):%.3f'%(len(df_valid),calc_prevalence(df_valid.OUTPUT_LABEL.values)))\nprint('Train all prevalence(n = %d):%.3f'%(len(df_train_all), calc_prevalence(df_train_all.OUTPUT_LABEL.values)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07c35358ac26964ea65190d05e937107ba4ca785"},"cell_type":"markdown","source":"We need to balance the data set because if we use the training data as the predictive model the accuracy is going to be very high because we haven't caught any of the y output which states whether a person will buy a term deposit or not. There are more negatives than positive so the predictive models assigns negatives to much of the samples. Creating a balance sheet will allow 50% of the samples to be both positive and negative."},{"metadata":{"trusted":false,"_uuid":"177545ef589babc9b5f1790b950a6f229a2667ef"},"cell_type":"code","source":"# split the training data into positive and negative\nrows_pos = df_train_all.OUTPUT_LABEL == 1\ndf_train_pos = df_train_all.loc[rows_pos]\ndf_train_neg = df_train_all.loc[~rows_pos]\n\n# merge the balanced data\ndf_train = pd.concat([df_train_pos, df_train_neg.sample(n = len(df_train_pos), random_state = 42)],axis = 0)\n\n# shuffle the order of training samples \ndf_train = df_train.sample(n = len(df_train), random_state = 42).reset_index(drop = True)\n\nprint('Train balanced prevalence(n = %d):%.3f'%(len(df_train), calc_prevalence(df_train.OUTPUT_LABEL.values)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"740f54f5c80f24d2d383e9927aad0db77d421c5d"},"cell_type":"markdown","source":"All 4 dataframes were saved intto csv and the cols_input"},{"metadata":{"trusted":false,"_uuid":"6274f6f4c35de7f3c728aaafbb8189d6007ae269"},"cell_type":"code","source":"df_train_all.to_csv('df_train_all.csv',index=False)\ndf_train.to_csv('df_train.csv',index=False)\ndf_valid.to_csv('df_valid.csv',index=False)\ndf_test.to_csv('df_test.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03259e8b92025d756bcbb3abbb60388198b37c27"},"cell_type":"markdown","source":"Saving cols_input too with a package called pickle"},{"metadata":{"trusted":false,"_uuid":"39ce99d99c3f691d5301d5ca79246de236b207eb"},"cell_type":"code","source":"import pickle\npickle.dump(cols_input, open('cols_input.sav', 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c5c632844e5406edf30d14bb745290460bb5648"},"cell_type":"markdown","source":"Any missing values were filled with the mean value"},{"metadata":{"trusted":false,"_uuid":"f0f3f7f92caf1aee83ebc4bef8130af2e8bef852"},"cell_type":"code","source":"def fill_my_missing(df, df_mean, col2use):\n    # This function fills the missing values\n\n    # check the columns are present\n    for c in col2use:\n        assert c in df.columns, c + ' not in df'\n        assert c in df_mean.col.values, c+ 'not in df_mean'\n    \n    # replace the mean \n    for c in col2use:\n        mean_value = df_mean.loc[df_mean.col == c,'mean_val'].values[0]\n        df[c] = df[c].fillna(mean_value)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f3f24f3e1ab400f45dc1fbccf8aa19acc9b7a411"},"cell_type":"markdown","source":"The mean value from the training data:"},{"metadata":{"trusted":false,"_uuid":"68580b7117ac5c99dde568beefdaf0bff8b796d4"},"cell_type":"code","source":"df_mean = df_train_all[cols_input].mean(axis = 0)\n# save the means\ndf_mean.to_csv('df_mean.csv',index=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f51f98996feb4577962fa9a776f9ae61bd1c2d88"},"cell_type":"markdown","source":"Loaded the means"},{"metadata":{"trusted":false,"_uuid":"239e4b5aa88fd7b26ea3d9eb86a8f43ac3f0bf25"},"cell_type":"code","source":"df_mean_in = pd.read_csv('df_mean.csv', names =['col','mean_val'])\ndf_mean_in.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d2deae75409a491f92675e76b8f40a9488a8a99d"},"cell_type":"code","source":"df_train_all = fill_my_missing(df_train_all, df_mean_in, cols_input)\ndf_train = fill_my_missing(df_train, df_mean_in, cols_input)\ndf_valid = fill_my_missing(df_valid, df_mean_in, cols_input)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"de4b73b2ed7e077eae7a952e474f41ec86cd6149"},"cell_type":"code","source":"# create the X and y matrices\nX_train = df_train[cols_input].values\nX_train_all = df_train_all[cols_input].values\nX_valid = df_valid[cols_input].values\n\ny_train = df_train['OUTPUT_LABEL'].values\ny_valid = df_valid['OUTPUT_LABEL'].values\n\nprint('Training All shapes:',X_train_all.shape)\nprint('Training shapes:',X_train.shape, y_train.shape)\nprint('Validation shapes:',X_valid.shape, y_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9eb9626a202340327e11b12922da5a9ae1c843c"},"cell_type":"markdown","source":"Created a scalar, saveed it, and scaled the X matrices"},{"metadata":{"trusted":false,"_uuid":"0a9ff956f8be8d63b9c9a79abb66b232ece415bc"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler  = StandardScaler()\nscaler.fit(X_train_all)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"16c40e94ca0fa447d6979d7f361828d23c4dbc5f"},"cell_type":"code","source":"scalerfile = 'scaler.sav'\npickle.dump(scaler, open(scalerfile, 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"17ec6d5a9fe893dd388710f8cdbf78f890ac95bd"},"cell_type":"code","source":"# load it back\nscaler = pickle.load(open(scalerfile, 'rb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f103b387542a710c82f4694f280bda299fd87b45"},"cell_type":"code","source":"# transform our data matrices\nX_train_tf = scaler.transform(X_train)\nX_valid_tf = scaler.transform(X_valid)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13646190e47c01bf18ed63b5c63569a347d81f0d"},"cell_type":"markdown","source":"## Model Selection "},{"metadata":{"_uuid":"14e067e0bc77134fa4031af0c103aec759322810"},"cell_type":"markdown","source":"This section allows us to test various  machine learning algorithm to see how our independent variables accurately predit our dependent y output variable."},{"metadata":{"trusted":false,"_uuid":"4ba7521b1f43cd847555687c0058e3ed1dc8cc15"},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\ndef calc_specificity(y_actual, y_pred, thresh):\n    # calculates specificity\n    return sum((y_pred < thresh) & (y_actual == 0)) /sum(y_actual ==0)\n\ndef print_report(y_actual, y_pred, thresh):\n    \n    auc = roc_auc_score(y_actual, y_pred)\n    accuracy = accuracy_score(y_actual, (y_pred > thresh))\n    recall = recall_score(y_actual, (y_pred > thresh))\n    precision = precision_score(y_actual, (y_pred > thresh))\n    specificity = calc_specificity(y_actual, y_pred, thresh)\n    print('AUC:%.3f'%auc)\n    print('accuracy:%.3f'%accuracy)\n    print('recall:%.3f'%recall)\n    print('precision:%.3f'%precision)\n    print('specificity:%.3f'%specificity)\n    print('prevalence:%.3f'%calc_prevalence(y_actual))\n    print(' ')\n    return auc, accuracy, recall, precision, specificity ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2313c8582ca39fc2acbaed739505dc66a24a07be"},"cell_type":"markdown","source":"Since we balanced our training data, let's set our threshold at 0.5 to label a predicted sample as positive. "},{"metadata":{"trusted":false,"_uuid":"a9381156a7ea1cd83f1d3fcd733112da24564b4e"},"cell_type":"code","source":"thresh = 0.5","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41b76ebe1338e179325f6d1e75afc30d060dfe47"},"cell_type":"markdown","source":"## Model Selection: baseline models"},{"metadata":{"_uuid":"ff75eafe06c08fb0a0a2f7ef88f2e8c6407c6869"},"cell_type":"markdown","source":"### K nearest neighbors (KNN)"},{"metadata":{"_uuid":"809bc1146b5a43cf0f7e10363bd1670683ebe73d"},"cell_type":"markdown","source":"K Nearest Neighbors looks at the k closest datapoints and probability sample that has positive labels. It is easy to implement, and you don't need an assumption for the data structure. KNN is also good for multivariate analysis."},{"metadata":{"_uuid":"fcffd4ea6f3ea4dc1608b3d70c119688b5132b9a"},"cell_type":"markdown","source":"Training and evaluating KNN performance:"},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"3840b72a170014e6bf79b0067a70c504daa16ec0"},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors = 100)\nknn.fit(X_train_tf, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"77519e689e71e5c3986d414b67396f7223ec115e"},"cell_type":"code","source":"y_train_preds = knn.predict_proba(X_train_tf)[:,1]\ny_valid_preds = knn.predict_proba(X_valid_tf)[:,1]\n\nprint('KNN')\nprint('Training:')\nknn_train_auc, knn_train_accuracy, knn_train_recall, \\\n    knn_train_precision, knn_train_specificity = print_report(y_train,y_train_preds, thresh)\nprint('Validation:')\nknn_valid_auc, knn_valid_accuracy, knn_valid_recall, \\\n    knn_valid_precision, knn_valid_specificity = print_report(y_valid,y_valid_preds, thresh)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3be60a93e0d0467fb4d7138c0e6d900f1cd0bf21"},"cell_type":"markdown","source":"Using K Nearest Neighbors and dividing the data into training and validation samples, I was able to get an AUC of 79.4% which catches 59.2% of potential customers using a threshold of 0.5 for the training set, which is good."},{"metadata":{"collapsed":true,"_uuid":"2cd218d0f23a958a2b7d8baa16b530027319a6e7"},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{"_uuid":"95d71723ed44c88f86c077e64c4b833b7747fc8f"},"cell_type":"markdown","source":"Logsitic regression uses a line (Sigmoid function) in the form of an \"S\" to predict if the dependent variable is true or false based on the independent variables. The \"S-shaped\" curve (on the line graph) will show the probability of the dependent variable occuring based on where the points of the independent variables lands on the curve. In this case, the output (y) is predicted by the numerical and categorical variables defined as \"x\" such as age, education and so on. Logistic regresssion is best used for classifying samples."},{"metadata":{"_uuid":"9f24bda1d43314f80fe708c29a18fc001c377cbe"},"cell_type":"markdown","source":"Training and evaluating the logistic regression performance:"},{"metadata":{"trusted":false,"_uuid":"409f1698b8e47793a6df0b60b8adf92b4f11a5f3"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr=LogisticRegression(random_state = 42)\nlr.fit(X_train_tf, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"94f0046b3809e2b237a4e682a93da2f1388cc6cf"},"cell_type":"code","source":"y_train_preds = lr.predict_proba(X_train_tf)[:,1]\ny_valid_preds = lr.predict_proba(X_valid_tf)[:,1]\n\nprint('Logistic Regression')\nprint('Training:')\nlr_train_auc, lr_train_accuracy, lr_train_recall, \\\n    lr_train_precision, lr_train_specificity = print_report(y_train,y_train_preds, thresh)\nprint('Validation:')\nlr_valid_auc, lr_valid_accuracy, lr_valid_recall, \\\n    lr_valid_precision, lr_valid_specificity = print_report(y_valid,y_valid_preds, thresh)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03d925c3d516ee53da836bca1c198c8a2385e070"},"cell_type":"markdown","source":"### Stochastic Gradient Descent"},{"metadata":{"_uuid":"d6e55d0c975c0c68808cf75d2d0694258c58aecb"},"cell_type":"markdown","source":"Stochastic Gradient Descent analyzes various sections of the data instead of the data as a whole and predicts the output using the independent variables. Stochastic Gradient Descent is faster than logistic regression in the sense that it doesn't run the whole dataset but instead looks at different parts of the dataset."},{"metadata":{"_uuid":"1ac90e00f485d765feee9a0366b445fc0b3eee3e"},"cell_type":"markdown","source":"Training and evaluating Stochastic Gradient Descent model performance:"},{"metadata":{"trusted":false,"_uuid":"45311ca11afffc991d0b9f25132d14480da6b628"},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nsgdc=SGDClassifier(loss = 'log',alpha = 0.1,random_state = 42)\nsgdc.fit(X_train_tf, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9532ba64becb790c7c6fb077bea13a1edcda61dc"},"cell_type":"code","source":"y_train_preds = sgdc.predict_proba(X_train_tf)[:,1]\ny_valid_preds = sgdc.predict_proba(X_valid_tf)[:,1]\n\nprint('Stochastic Gradient Descent')\nprint('Training:')\nsgdc_train_auc, sgdc_train_accuracy, sgdc_train_recall, sgdc_train_precision, sgdc_train_specificity =print_report(y_train,y_train_preds, thresh)\nprint('Validation:')\nsgdc_valid_auc, sgdc_valid_accuracy, sgdc_valid_recall, sgdc_valid_precision, sgdc_valid_specificity = print_report(y_valid,y_valid_preds, thresh)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa92508dcfd85d8dd0fafce8e6c58ba3cbf0f199"},"cell_type":"markdown","source":"### Naive Bayes"},{"metadata":{"_uuid":"0efcf538f55f6112faa967fdf836cc7c601e739d"},"cell_type":"markdown","source":"Naive Bayes assumes that all variables in the dataset are independent of each other. Meaning that there are no dependent variables or output. This algorithm uses Bayes rule which calculated the probability of an event related to previous knowledge of the variables converning the event. This won't really work in this case since we have an output of the bank customers who will get a bank deposit. This process is better for tasks such as image processing."},{"metadata":{"_uuid":"dc142367f3cc95932801bc38453f5eef6e9e9e91"},"cell_type":"markdown","source":"Training and evaluating Naive Bayes model performance:"},{"metadata":{"trusted":false,"_uuid":"5a8ef3b799bdb9f0862de7cb1362071bbb9c8744"},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\nnb = GaussianNB()\nnb.fit(X_train_tf, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"011ba5d683bee7ff5299b3fd1da5a03255a4fb44"},"cell_type":"code","source":"y_train_preds = nb.predict_proba(X_train_tf)[:,1]\ny_valid_preds = nb.predict_proba(X_valid_tf)[:,1]\n\nprint('Naive Bayes')\nprint('Training:')\nnb_train_auc, nb_train_accuracy, nb_train_recall, nb_train_precision, nb_train_specificity =print_report(y_train,y_train_preds, thresh)\nprint('Validation:')\nnb_valid_auc, nb_valid_accuracy, nb_valid_recall, nb_valid_precision, nb_valid_specificity = print_report(y_valid,y_valid_preds, thresh)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6eae6ce116ab4829492f79468426801e9bb9034f"},"cell_type":"markdown","source":"### Decision Tree Classifier"},{"metadata":{"_uuid":"abb102e895036165689e868c038346cdf6234ad2"},"cell_type":"markdown","source":"Decision trees works through the data to decide if one action occurs, what will then be the result of a \"yes\" and a \"no\". It works each data making the decision of which path to take based on the answer. Because of this decision making process, this algorithm has no assumptions about the structure of the data, but instead decides on the path to take through each decision the algorithm performs."},{"metadata":{"_uuid":"9d0d4173b44ac84456356ed82a47df68014a8bf3"},"cell_type":"markdown","source":"Training and evaluating Decision Tree model performance:"},{"metadata":{"trusted":false,"_uuid":"38bb50fc0275304850bbaa5fd96cf3fa1597d097"},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ntree = DecisionTreeClassifier(max_depth = 10, random_state = 42)\ntree.fit(X_train_tf, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"510bbb1732539be84320eb6c51e4261ba549bb70"},"cell_type":"code","source":"y_train_preds = tree.predict_proba(X_train_tf)[:,1]\ny_valid_preds = tree.predict_proba(X_valid_tf)[:,1]\n\nprint('Decision Tree')\nprint('Training:')\ntree_train_auc, tree_train_accuracy, tree_train_recall, tree_train_precision, tree_train_specificity =print_report(y_train,y_train_preds, thresh)\nprint('Validation:')\ntree_valid_auc, tree_valid_accuracy, tree_valid_recall, tree_valid_precision, tree_valid_specificity = print_report(y_valid,y_valid_preds, thresh)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d4c76fa7eda9c86a8d21a9e733983f79a6fd70c"},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"_uuid":"8db2cf95b6b17a010a3532802d331f8c225fa024"},"cell_type":"markdown","source":"Random forest works like a decision tree algorithm but it performs various decision tree analysis on the dataset as a whole. That is, it is the bigger version of the decision tree; a forest is bigger than a tree, you can think of it that way. Random forest takes random samples of trees and it works faster than the decision tree algorithm. "},{"metadata":{"_uuid":"cabe386c3bef88cef22817d3ab05472700f9fd4e"},"cell_type":"markdown","source":"Training and evaluating Random Forest model performance:"},{"metadata":{"trusted":false,"_uuid":"5822040dd610ff7c4d0b868b052171f8677b7b91"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf=RandomForestClassifier(max_depth = 6, random_state = 42)\nrf.fit(X_train_tf, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b596a626414c5e93b844529371d25f33bc7c379f"},"cell_type":"code","source":"y_train_preds = rf.predict_proba(X_train_tf)[:,1]\ny_valid_preds = rf.predict_proba(X_valid_tf)[:,1]\n\nprint('Random Forest')\nprint('Training:')\nrf_train_auc, rf_train_accuracy, rf_train_recall, rf_train_precision, rf_train_specificity =print_report(y_train,y_train_preds, thresh)\nprint('Validation:')\nrf_valid_auc, rf_valid_accuracy, rf_valid_recall, rf_valid_precision, rf_valid_specificity = print_report(y_valid,y_valid_preds, thresh)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ced63cec54fe45e6c18d13de404801a1eb90a34d"},"cell_type":"markdown","source":"### Gradient Boosting Classifier"},{"metadata":{"_uuid":"811b1fa7ee68aafbe33131d2ddb89a4aa1e23ef9"},"cell_type":"markdown","source":"Boosting is a technique that builds a new decision tree algorithm that focuses on the errors on the dataset to fix them. This learns the whole model in other to fix it and improve the prediction of the model. Aside from being related to decision trees, it also relates to gradient descent algorithm as the name signifies. Gradient boosting analyzes different parts of the dataset and builds trees that focuses and corrects those errors. The XGBoost library is also the determining factor in winning a lot of Kaggle data science competitions."},{"metadata":{"_uuid":"0334cb9c7bc8d41bf4f9779ebbac6db8b7f3ad88"},"cell_type":"markdown","source":"Training and evaluating Gradient Boosting model performance:"},{"metadata":{"trusted":false,"_uuid":"e44c2725e2bbab4eb9472f735461befd7f019680"},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngbc =GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n     max_depth=3, random_state=42)\ngbc.fit(X_train_tf, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c05346ef3c3849bfe27646987105988bf802d8a0"},"cell_type":"code","source":"y_train_preds = gbc.predict_proba(X_train_tf)[:,1]\ny_valid_preds = gbc.predict_proba(X_valid_tf)[:,1]\n\nprint('Gradient Boosting Classifier')\nprint('Training:')\ngbc_train_auc, gbc_train_accuracy, gbc_train_recall, gbc_train_precision, gbc_train_specificity = print_report(y_train,y_train_preds, thresh)\nprint('Validation:')\ngbc_valid_auc, gbc_valid_accuracy, gbc_valid_recall, gbc_valid_precision, gbc_valid_specificity = print_report(y_valid,y_valid_preds, thresh)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3fb1ae3d5a2f298f7fddfcefb3f39f0fe346d6c8"},"cell_type":"markdown","source":"## Analyze results baseline models"},{"metadata":{"_uuid":"e09ac2779db96d20bff00f275d387f46a7e6bcf1"},"cell_type":"markdown","source":"Let's make a dataframe with these results and plot the outcomes using a package called seaborn."},{"metadata":{"trusted":false,"_uuid":"e4a393e51cdb82425e5cf187d2fc22b57bfc227b"},"cell_type":"code","source":"df_results = pd.DataFrame({'classifier':['KNN','KNN','LR','LR','SGD','SGD','NB','NB','DT','DT','RF','RF','GB','GB'],\n                           'data_set':['train','valid']*7,\n                          'auc':[knn_train_auc, knn_valid_auc,lr_train_auc,lr_valid_auc,sgdc_train_auc,sgdc_valid_auc,nb_train_auc,nb_valid_auc,tree_train_auc,tree_valid_auc,rf_train_auc,rf_valid_auc,gbc_train_auc,gbc_valid_auc,],\n                          'accuracy':[knn_train_accuracy, knn_valid_accuracy,lr_train_accuracy,lr_valid_accuracy,sgdc_train_accuracy,sgdc_valid_accuracy,nb_train_accuracy,nb_valid_accuracy,tree_train_accuracy,tree_valid_accuracy,rf_train_accuracy,rf_valid_accuracy,gbc_train_accuracy,gbc_valid_accuracy,],\n                          'recall':[knn_train_recall, knn_valid_recall,lr_train_recall,lr_valid_recall,sgdc_train_recall,sgdc_valid_recall,nb_train_recall,nb_valid_recall,tree_train_recall,tree_valid_recall,rf_train_recall,rf_valid_recall,gbc_train_recall,gbc_valid_recall,],\n                          'precision':[knn_train_precision, knn_valid_precision,lr_train_precision,lr_valid_precision,sgdc_train_precision,sgdc_valid_precision,nb_train_precision,nb_valid_precision,tree_train_precision,tree_valid_precision,rf_train_precision,rf_valid_precision,gbc_train_precision,gbc_valid_precision,],\n                          'specificity':[knn_train_specificity, knn_valid_specificity,lr_train_specificity,lr_valid_specificity,sgdc_train_specificity,sgdc_valid_specificity,nb_train_specificity,nb_valid_specificity,tree_train_specificity,tree_valid_specificity,rf_train_specificity,rf_valid_specificity,gbc_train_specificity,gbc_valid_specificity,]})","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a3da8a382030d9cce939b21de5bd01b38c806c03"},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"darkgrid\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f78d161992efaa49739ee5f7dbe0b9a3e1efe1a5"},"cell_type":"markdown","source":"I picked AUC (area under the ROC curve) as a performance indicator. The reason I chose this over other indicators such as precision and accuracy is that it measures the relationshio between true positives and false positives in our data in order to derive a score that depicts that. Also, AUC is widely used and an easier metric to compare many models with."},{"metadata":{"_uuid":"2633f8911a2606a4abca550d3d9439d23f2c2b24"},"cell_type":"markdown","source":"All the algorithms have pretty much the same AUC, but the ones that stood out our decision tree (DT) and gradient boosting (GB). I would choose gradient boosting as the best metric to use because it has a higher auc (0.874) than the other algorithms. At a threshold of 0.5, an auc of 0.874 is good as it signifies that it is more than just a random guess towards a positive class and it is close to 1 which is perfect."},{"metadata":{"trusted":false,"_uuid":"c5b2a2ce8a377bfb3dc8a82c2851a02de871f9da"},"cell_type":"code","source":"ax = sns.barplot(x=\"classifier\", y=\"auc\", hue=\"data_set\", data=df_results)\nax.set_xlabel('Classifier',fontsize = 15)\nax.set_ylabel('AUC', fontsize = 15)\nax.tick_params(labelsize=15)\n\n# Put the legend out of the figure\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., fontsize = 15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c2f90238a54b71b0795eff2665a796ba9cf70df9"},"cell_type":"markdown","source":"## Learning Curves"},{"metadata":{"_uuid":"4e59d276f06fd59d9329a0810ddf64d85d225959"},"cell_type":"markdown","source":"Gradient Descent has the best AUC score (0.796) for the validation model and the learning curve for the model will be displayed below."},{"metadata":{"trusted":false,"_uuid":"e4aca3fde321be58df8fe14105390b615eb4ffe5"},"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\n\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"\n    Generate a simple plot of the test and training learning curve.\n\n    Parameters\n    ----------\n    estimator : object type that implements the \"fit\" and \"predict\" methods\n        An object of that type which is cloned for each validation.\n\n    title : string\n        Title for the chart.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    ylim : tuple, shape (ymin, ymax), optional\n        Defines minimum and maximum yvalues plotted.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n          - None, to use the default 3-fold cross-validation,\n          - integer, to specify the number of folds.\n          - An object to be used as a cross-validation generator.\n          - An iterable yielding train/test splits.\n\n        For integer/None inputs, if ``y`` is binary or multiclass,\n        :class:`StratifiedKFold` used. If the estimator is not a classifier\n        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validators that can be used here.\n\n    n_jobs : integer, optional\n        Number of jobs to run in parallel (default 1).\n    \"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"AUC\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring = 'roc_auc')\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"b\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"b\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"befb9e7b1ffece2f91e436587f4fb542bb23bee9"},"cell_type":"markdown","source":"The Stochastic Gradient Descent model with max_depth = 20, resulting in high variance"},{"metadata":{"trusted":false,"_uuid":"30499dc7295779b86347f071f6605e5cf34602ca"},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\ntitle = \"Learning Curves (Stochastic Gradient Descent)\"\n# Cross validation with 5 iterations to get smoother mean test and train\n# score curves, each time with 20% data randomly selected as a validation set.\ncv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nestimator = RandomForestClassifier(max_depth = 20, random_state = 42)\nplot_learning_curve(estimator, title, X_train_tf, y_train, ylim=(0.2, 1.01), cv=cv, n_jobs=4)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dfc8a6fa6081c18acfca6ddc4de3b3f335c55aa6"},"cell_type":"markdown","source":"### Variance and Bias"},{"metadata":{"_uuid":"cc757b44960a6405aea8bddf145477ccd7faff07"},"cell_type":"markdown","source":"My model has high bias because the training and cross-validation score are close to each other which shows there are near in numbers. The model has high variance because the training and cross-validation scores show a lot of samples which are close to each other. So the model has both high bias and variance."},{"metadata":{"_uuid":"69fd463b566759e861039632886e6299eb25c8ec"},"cell_type":"markdown","source":"## Feature Importance"},{"metadata":{"_uuid":"e4da463ab9eeaa6eef1744baccd54f83caf23f73"},"cell_type":"markdown","source":"This section focuses on the importance of the different features generated and in the dataframe. Depending on the importance score of some features, we can focus on higher importance scores to see if the AUC score (performance) of the model will improve."},{"metadata":{"_uuid":"c6b29c84da8ead4eba3cbe5a6241bf86fbf83297"},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{"trusted":false,"_uuid":"b6c7a0f15aa2178d2323f52273c6f4668c7a4935"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr=LogisticRegression(random_state = 42)\nlr.fit(X_train_tf, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f5b879b51343c38b660e1561b87875a5ec24a031"},"cell_type":"code","source":"feature_importances = pd.DataFrame(lr.coef_[0],\n                                   index = cols_input,\n                                    columns=['importance']).sort_values('importance',\n                                                                        ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"421191e302b57706e2db8cc4ffee45b57b402949"},"cell_type":"code","source":"feature_importances.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5ad7bff97f500a453e62a9d5a93cdd83acd16ba9"},"cell_type":"code","source":"num = np.min([50, len(cols_input)])\nylocs = np.arange(num)\n# get the feature importance for top num and sort in reverse order\nvalues_to_plot = feature_importances.iloc[:num].values.ravel()[::-1]\nfeature_labels = list(feature_importances.iloc[:num].index)[::-1]\n\nplt.figure(num=None, figsize=(8, 15), dpi=80, facecolor='w', edgecolor='k');\nplt.barh(ylocs, values_to_plot, align = 'center')\nplt.ylabel('Features')\nplt.xlabel('Importance Score')\nplt.title('Feature Importance Score - Logistic Regression')\nplt.yticks(ylocs, feature_labels)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"147d9c00574514965784d76b936393395909ca35"},"cell_type":"code","source":"values_to_plot = feature_importances.iloc[-num:].values.ravel()\nfeature_labels = list(feature_importances.iloc[-num:].index)\n\nplt.figure(num=None, figsize=(8, 15), dpi=80, facecolor='w', edgecolor='k');\nplt.barh(ylocs, values_to_plot, align = 'center')\nplt.ylabel('Features')\nplt.xlabel('Importance Score')\nplt.title('Feature Importance Score - Logistic Regression')\nplt.yticks(ylocs, feature_labels)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f52826272bf509a449325fed0c84747b49408e0f"},"cell_type":"markdown","source":"I realized the features that have more positive impact on the predictive outcomes of the model. nr.employed, mon_mar, and cons.price are very crucial as their importance score is higher than other nummerical variables. emp.var.rate has a high negative importance score. One way to look at it is I can remove the other columns from my dataset and maybe I can achieve a higher auc score."},{"metadata":{"_uuid":"37ceeaffa8646e1128df8c222d07e002a7a43d05"},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"trusted":false,"_uuid":"f28ea6d2a9f738fbd8d8696c72cceb6b0b9e0f64"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf=RandomForestClassifier(max_depth = 6, random_state = 42)\nrf.fit(X_train_tf, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ef63a9884a4ddd61628eabc438e272726fd0f903"},"cell_type":"code","source":"feature_importances = pd.DataFrame(rf.feature_importances_,\n                                   index = cols_input,\n                                    columns=['importance']).sort_values('importance',\n                                                                        ascending=False)\nfeature_importances.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"60438edf9c6f036c8ffdf593e68d4dce4a552be3"},"cell_type":"code","source":"num = np.min([50, len(cols_input)])\nylocs = np.arange(num)\n# get the feature importance for top num and sort in reverse order\nvalues_to_plot = feature_importances.iloc[:num].values.ravel()[::-1]\nfeature_labels = list(feature_importances.iloc[:num].index)[::-1]\n\nplt.figure(num=None, figsize=(8, 15), dpi=80, facecolor='w', edgecolor='k');\nplt.barh(ylocs, values_to_plot, align = 'center')\nplt.ylabel('Features')\nplt.xlabel('Importance Score')\nplt.title('Feature Importance Score - Random Forest')\nplt.yticks(ylocs, feature_labels)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d4ff4d84b2c81ff78ca9bbee3271e6724947e6e4"},"cell_type":"markdown","source":"### Other Algorithm Feature Importance Scores"},{"metadata":{"_uuid":"ba06759b010b62c591f0afa0726ede2e22e28a35"},"cell_type":"markdown","source":"### Gradient Boosting Classifier"},{"metadata":{"trusted":false,"_uuid":"123ca497ed110455b61d7bf03d398be2aa6a5d01"},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngbc=GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n     max_depth=3, random_state=42)\ngbc.fit(X_train_tf, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"43541f3c63a368c015be8e37cc2f97ae14570491"},"cell_type":"code","source":"feature_importances = pd.DataFrame(gbc.feature_importances_,\n                                   index = cols_input,\n                                    columns=['importance']).sort_values('importance',\n                                                                        ascending=False)\nfeature_importances.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f56aab4da77372194c52e51fdc47769f841a10c"},"cell_type":"markdown","source":"I realized the features that have more positive impact on the predictive outcomes of the model. nr.employed, emp.var.rate and cons.price.idx are very crucial as their importance score is higher than other nummerical variables. One way to look at it is I can remove the other columns from my dataset and maybe I can achieve a higher auc score."},{"metadata":{"trusted":false,"_uuid":"98cd6f6f9ed24ae955a5a316092d3c60942b2bc0"},"cell_type":"code","source":"num = np.min([50, len(cols_input)])\nylocs = np.arange(num)\n# get the feature importance for top num and sort in reverse order\nvalues_to_plot = feature_importances.iloc[:num].values.ravel()[::-1]\nfeature_labels = list(feature_importances.iloc[:num].index)[::-1]\n\nplt.figure(num=None, figsize=(8, 15), dpi=80, facecolor='w', edgecolor='k');\nplt.barh(ylocs, values_to_plot, align = 'center')\nplt.ylabel('Features')\nplt.xlabel('Importance Score')\nplt.title('Feature Importance Score - Gradient Boosting Classifier')\nplt.yticks(ylocs, feature_labels)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce429971a8fa744b61b7b65eb90a75a37957bfe9"},"cell_type":"markdown","source":"### Decision Trees"},{"metadata":{"trusted":false,"_uuid":"7a5921a3454f0274c2a79b6a82527e63c30fcd71"},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier(max_depth = 10, random_state = 42)\ntree.fit(X_train_tf, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7737ac3171857359770d7e7b163cef79dccc4d5e"},"cell_type":"code","source":"feature_importances = pd.DataFrame(tree.feature_importances_,\n                                   index = cols_input,\n                                    columns=['importance']).sort_values('importance',\n                                                                        ascending=False)\nfeature_importances.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"0eed71d6b53d7d81602dcedb82a86d4e77f61327"},"cell_type":"code","source":"num = np.min([50, len(cols_input)])\nylocs = np.arange(num)\n# get the feature importance for top num and sort in reverse order\nvalues_to_plot = feature_importances.iloc[:num].values.ravel()[::-1]\nfeature_labels = list(feature_importances.iloc[:num].index)[::-1]\n\nplt.figure(num=None, figsize=(8, 15), dpi=80, facecolor='w', edgecolor='k');\nplt.barh(ylocs, values_to_plot, align = 'center')\nplt.ylabel('Features')\nplt.xlabel('Importance Score')\nplt.title('Feature Importance Score - Decision Trees')\nplt.yticks(ylocs, feature_labels)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6733135623e14a562fb16f4d3169b3843ea90c60"},"cell_type":"markdown","source":"After looking at the importance of each features, I tried removing some columns with a lower importance from the model to see if my AUC will increase but it only decreased. Therefore, I'm leaving my features as it is."},{"metadata":{"_uuid":"5f5deb3e18844455cab5a77ae51a43a1bed2fd51"},"cell_type":"markdown","source":"## Hyperparameter tuning"},{"metadata":{"_uuid":"7a810f246f7ef4b265bb9000e23402f13109a99a"},"cell_type":"markdown","source":"A hyperparameter is a tool used in machine learning in order to estimate the model parameters (used in tuning a predictive modeling problem).  Hyperparameters are used in various machine learning algorithms. "},{"metadata":{"trusted":false,"_uuid":"eb2e3984f5b110287e1a6b52a132c29c4d8d9fa3"},"cell_type":"code","source":"# train a model for each max_depth in a list. Store the auc for the training and validation set\n\n# max depths\nmax_depths = np.arange(2,20,2)\n\ntrain_aucs = np.zeros(len(max_depths))\nvalid_aucs = np.zeros(len(max_depths))\n\nfor jj in range(len(max_depths)):\n    max_depth = max_depths[jj]\n\n    # fit model\n    rf=RandomForestClassifier(n_estimators = 100, max_depth = max_depth, random_state = 42)\n    rf.fit(X_train_tf, y_train)        \n    # get predictions\n    y_train_preds = rf.predict_proba(X_train_tf)[:,1]\n    y_valid_preds = rf.predict_proba(X_valid_tf)[:,1]\n\n    # calculate auc\n    auc_train = roc_auc_score(y_train, y_train_preds)\n    auc_valid = roc_auc_score(y_valid, y_valid_preds)\n\n    # save aucs\n    train_aucs[jj] = auc_train\n    valid_aucs[jj] = auc_valid","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"376c55e16fe6fbe726b377876de11673806c7dd5"},"cell_type":"markdown","source":"n_estimators is a hyperparameter in the RandomForestClassifier that depending on the numbers of estimators entered the model can be overfitted, good compromise or underfitted. n_estimators is used for fine tuning the models in order to fit the training data. max_depth is also another hyperparameter; it controls the depth of the machine learning algorithm model."},{"metadata":{"trusted":false,"_uuid":"435efb10d373a7cd52a2d0f678e0575e9f207df0"},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.plot(max_depths, train_aucs,'o-',label = 'train')\nplt.plot(max_depths, valid_aucs,'o-',label = 'valid')\n\nplt.xlabel('max_depth')\nplt.ylabel('AUC')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c5834a1a42b73de2c3695245b9b9913618602d2e"},"cell_type":"markdown","source":"Using RandomizedSearchCV to optimize a few of the baseline models. GradientBoosting Classifier may take a while so one might need to adjust the number of iterations or specific parameters. If this takes too long on the computer, one can take it out."},{"metadata":{"trusted":false,"_uuid":"66fdc56a7db7c0c59e4aba7d96626313aba12097"},"cell_type":"code","source":"rf.get_params()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d494f683bc70661fdffa6d3237abd78fee16bb46"},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n\n# number of trees\nn_estimators = range(200,1000,200)\n# maximum number of features to use at each split\nmax_features = ['auto','sqrt']\n# maximum depth of the tree\nmax_depth = range(2,20,2)\n# minimum number of samples to split a node\nmin_samples_split = range(2,10,2)\n# criterion for evaluating a split\ncriterion = ['gini','entropy']\n\n# random grid\n\nrandom_grid = {'n_estimators':n_estimators,\n              'max_features':max_features,\n              'max_depth':max_depth,\n              'min_samples_split':min_samples_split,\n              'criterion':criterion}\n\nprint(random_grid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3f32d7f4f79def8b09266e5f9c280da2d563b6d6"},"cell_type":"code","source":"from sklearn.metrics import make_scorer, roc_auc_score\nauc_scoring = make_scorer(roc_auc_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4a11a16f404619e40c5f56a26e3239e48ff61ec2"},"cell_type":"code","source":"# create a baseline model\nrf = RandomForestClassifier()\n\n# create the randomized search cross-validation\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, \n                               n_iter = 20, cv = 2, \n                               scoring=auc_scoring,verbose = 1, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"089ccf5fd4906b70fbe61b59e05a07f95a46e5a7"},"cell_type":"code","source":"import time\n# fit the random search model (this will take a few minutes)\nt1 = time.time()\nrf_random.fit(X_train_tf, y_train)\nt2 = time.time()\nprint(t2-t1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b377696061d02ababad46e04823097063a2d164"},"cell_type":"markdown","source":"See the best parameters"},{"metadata":{"trusted":false,"_uuid":"b28869e6499c628b5e4f5709827fa792b2dbffbc"},"cell_type":"code","source":"rf_random.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fa9e78d606b8139e01a282598e3af4e1fd9b688b"},"cell_type":"code","source":"rf=RandomForestClassifier(max_depth = 6, random_state = 42)\nrf.fit(X_train_tf, y_train)\n\ny_train_preds = rf.predict_proba(X_train_tf)[:,1]\ny_valid_preds = rf.predict_proba(X_valid_tf)[:,1]\n\nthresh = 0.5\n\nprint('Baseline Random Forest')\nrf_train_base_auc = roc_auc_score(y_train, y_train_preds)\nrf_valid_base_auc = roc_auc_score(y_valid, y_valid_preds)\n\nprint('Training AUC:%.3f'%(rf_train_base_auc))\nprint('Validation AUC:%.3f'%(rf_valid_base_auc))\n\nprint('Optimized Random Forest')\ny_train_preds_random = rf_random.best_estimator_.predict_proba(X_train_tf)[:,1]\ny_valid_preds_random = rf_random.best_estimator_.predict_proba(X_valid_tf)[:,1]\n\nrf_train_opt_auc = roc_auc_score(y_train, y_train_preds_random)\nrf_valid_opt_auc = roc_auc_score(y_valid, y_valid_preds_random)\n\nprint('Training AUC:%.3f'%(rf_train_opt_auc))\nprint('Validation AUC:%.3f'%(rf_valid_opt_auc))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d28087a3111771e5d6b73ffbaece86161d093700"},"cell_type":"markdown","source":"Optimized SGDClassifier"},{"metadata":{"trusted":false,"_uuid":"a7d8a9395b1939cfe66c5ad33795d79cd41859f1"},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nsgdc=SGDClassifier(loss = 'log',alpha = 0.1,random_state = 42)\nsgdc.fit(X_train_tf, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f1a59a6d25e6ec2f07998efa7fc283f8c5356542"},"cell_type":"code","source":"penalty = ['none','l2','l1']\nmax_iter = range(200,1000,200)\nalpha = [0.001,0.003,0.01,0.03,0.1,0.3]\nrandom_grid_sgdc = {'penalty':penalty,\n              'max_iter':max_iter,\n              'alpha':alpha}\n# create the randomized search cross-validation\nsgdc_random = RandomizedSearchCV(estimator = sgdc, param_distributions = random_grid_sgdc, n_iter = 20, cv = 2, scoring=auc_scoring,verbose = 0, random_state = 42)\n\nt1 = time.time()\nsgdc_random.fit(X_train_tf, y_train)\nt2 = time.time()\nprint(t2-t1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"98331231fa093e5fd3996c3465675ada18bcbed9"},"cell_type":"code","source":"sgdc_random.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"dbae98d4bfc34536a4c8edf6e39991b08adf57c7"},"cell_type":"code","source":"y_train_preds = sgdc.predict_proba(X_train_tf)[:,1]\ny_valid_preds = sgdc.predict_proba(X_valid_tf)[:,1]\n\nthresh = 0.5\n\nprint('Baseline sgdc')\nsgdc_train_base_auc = roc_auc_score(y_train, y_train_preds)\nsgdc_valid_base_auc = roc_auc_score(y_valid, y_valid_preds)\n\nprint('Training AUC:%.3f'%(sgdc_train_base_auc))\nprint('Validation AUC:%.3f'%(sgdc_valid_base_auc))\n\nprint('Optimized sgdc')\ny_train_preds_random = sgdc_random.best_estimator_.predict_proba(X_train_tf)[:,1]\ny_valid_preds_random = sgdc_random.best_estimator_.predict_proba(X_valid_tf)[:,1]\nsgdc_train_opt_auc = roc_auc_score(y_train, y_train_preds_random)\nsgdc_valid_opt_auc = roc_auc_score(y_valid, y_valid_preds_random)\n\nprint('Training AUC:%.3f'%(sgdc_train_opt_auc))\nprint('Validation AUC:%.3f'%(sgdc_valid_opt_auc))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7f6bac1a13e23a4f0947a0912045ec41d4a75c3"},"cell_type":"markdown","source":"Optimized Gradient Boosting Classifier"},{"metadata":{"trusted":false,"_uuid":"5529ff5c2386438773fdfaa98a766946bb6acbd6"},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngbc =GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n     max_depth=3, random_state=42)\ngbc.fit(X_train_tf, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9b1b7f62158ae1bcf8df16ecc2d4b0b0f1d2e628"},"cell_type":"code","source":"# number of trees\nn_estimators = range(50,200,50)\n\n# maximum depth of the tree\nmax_depth = range(1,5,1)\n\n# learning rate\nlearning_rate = [0.001,0.01,0.1]\n\n# random grid\n\nrandom_grid_gbc = {'n_estimators':n_estimators,\n              'max_depth':max_depth,\n              'learning_rate':learning_rate}\n\n# create the randomized search cross-validation\ngbc_random = RandomizedSearchCV(estimator = gbc, param_distributions = random_grid_gbc, n_iter = 20, cv = 2, scoring=auc_scoring,verbose = 0, random_state = 42)\n\nt1 = time.time()\ngbc_random.fit(X_train_tf, y_train)\nt2 = time.time()\nprint(t2-t1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"74c376c439492ec79dd626a1643ef9fc6da74f21"},"cell_type":"code","source":"gbc_random.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c4f3fecc1d5f21eb82ec656576481bc095d04659"},"cell_type":"code","source":"y_train_preds = gbc.predict_proba(X_train_tf)[:,1]\ny_valid_preds = gbc.predict_proba(X_valid_tf)[:,1]\n\nthresh = 0.5\n\nprint('Baseline gbc')\ngbc_train_base_auc = roc_auc_score(y_train, y_train_preds)\ngbc_valid_base_auc = roc_auc_score(y_valid, y_valid_preds)\n\nprint('Training AUC:%.3f'%(gbc_train_base_auc))\nprint('Validation AUC:%.3f'%(gbc_valid_base_auc))\nprint('Optimized gbc')\ny_train_preds_random = gbc_random.best_estimator_.predict_proba(X_train_tf)[:,1]\ny_valid_preds_random = gbc_random.best_estimator_.predict_proba(X_valid_tf)[:,1]\ngbc_train_opt_auc = roc_auc_score(y_train, y_train_preds_random)\ngbc_valid_opt_auc = roc_auc_score(y_valid, y_valid_preds_random)\n\nprint('Training AUC:%.3f'%(gbc_train_opt_auc))\nprint('Validation AUC:%.3f'%(gbc_valid_opt_auc))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9001a5a6c1993af53b6e9e5ada216a4eb9c5649a"},"cell_type":"markdown","source":"Analyzing the 3 results"},{"metadata":{"trusted":false,"_uuid":"cf29fb70b5f858f8728be4aa68e31bbf67ce1ab1"},"cell_type":"code","source":"df_results = pd.DataFrame({'classifier':['SGD','SGD','RF','RF','GB','GB'],\n                           'data_set':['baseline','optimized']*3,\n                          'auc':[sgdc_valid_base_auc,sgdc_valid_opt_auc,\n                                 rf_valid_base_auc,rf_valid_opt_auc,\n                                 gbc_valid_base_auc,gbc_valid_opt_auc],\n                          })","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6bbe11b269c3ff07023bdfbbfda2abc972d5522c"},"cell_type":"code","source":"df_results","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"44f7d3136cf2e6f3aabe7b7782a0521b20f1a4af"},"cell_type":"markdown","source":"Comparing the performance of the optimized models to the baseline models. "},{"metadata":{"trusted":false,"_uuid":"86d2608acf8be66388536302c86a1a9877849e38"},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"darkgrid\")\n\nax = sns.barplot(x=\"classifier\", y=\"auc\", hue=\"data_set\", data=df_results)\nax.set_xlabel('Classifier',fontsize = 15)\nax.set_ylabel('AUC', fontsize = 15)\nax.tick_params(labelsize=15)\n# Put the legend out of the figure\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., fontsize = 15)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"46d425b63a57d92dcf62809a160c48bb5ff61963"},"cell_type":"markdown","source":"## Picking the best model"},{"metadata":{"_uuid":"eeba2583f284ee39d6ff813ee242ef6acfac2cef"},"cell_type":"markdown","source":"I picked Gradient Boosting optimized version as my best model because the optimized version has a higher auc metric than the baseline models of Stochastic Gradient Descent and Random Forest. Gradient Boosting's AUC score also tells me that most of my data are predicted positives which has a good chance of occuring and can be used to make strategic decisions for management."},{"metadata":{"trusted":false,"_uuid":"714fd4d428e65a9149d5da35a3c4902f1fe990dd"},"cell_type":"code","source":"pickle.dump(gbc_random.best_estimator_, open('best_classifier.pkl', 'wb'),protocol = 4)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d176c7e97264668a34210a845758039570f297e"},"cell_type":"markdown","source":"# Model Evaluation"},{"metadata":{"_uuid":"3ca47a6be71b4b6327bfdb9b1d0fdbd26a58c112"},"cell_type":"markdown","source":"Below is the evaluation of the performance of the best model on the training, validation and test sets. I also created an ROC curve."},{"metadata":{"trusted":false,"_uuid":"0fb3d63290b89854655a6fb7726be406ad8bfa16"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pickle","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a40f9f0c69d183b1fc5bb672cc937755bbff4a4d"},"cell_type":"code","source":"# load the model, columns, mean values, and scaler\nbest_model = pickle.load(open('best_classifier.pkl','rb'))\ncols_input = pickle.load(open('cols_input.sav','rb'))\ndf_mean_in = pd.read_csv('df_mean.csv', names =['col','mean_val'])\nscaler = pickle.load(open('scaler.sav', 'rb'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3b4038305530bbcd37e48316a196cfd5f7d0b9b3"},"cell_type":"code","source":"# load the data\ndf_train = pd.read_csv('df_train.csv')\ndf_valid= pd.read_csv('df_valid.csv')\ndf_test= pd.read_csv('df_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"11287ce6bd17bd4d67a6d5dbea6be9d2e0ec320b"},"cell_type":"code","source":"# fill missing\ndf_train = fill_my_missing(df_train, df_mean_in, cols_input)\ndf_valid = fill_my_missing(df_valid, df_mean_in, cols_input)\ndf_test = fill_my_missing(df_test, df_mean_in, cols_input)\n\n# create X and y matrices\nX_train = df_train[cols_input].values\nX_valid = df_valid[cols_input].values\nX_test = df_test[cols_input].values\n\ny_train = df_train['OUTPUT_LABEL'].values\ny_valid = df_valid['OUTPUT_LABEL'].values\ny_test = df_test['OUTPUT_LABEL'].values\n\n# transform our data matrices \nX_train_tf = scaler.transform(X_train)\nX_valid_tf = scaler.transform(X_valid)\nX_test_tf = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f046eea9ca87ba80973acb76a1caa5431e8c4766"},"cell_type":"markdown","source":"Prediction possibilities"},{"metadata":{"trusted":false,"_uuid":"bba415a2e7c4563a754e2d22e499199d47ae6873"},"cell_type":"code","source":"y_train_preds = best_model.predict_proba(X_train_tf)[:,1]\ny_valid_preds = best_model.predict_proba(X_valid_tf)[:,1]\ny_test_preds = best_model.predict_proba(X_test_tf)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"768256a603c7a7651e9d8799c47057ccbb052ca4"},"cell_type":"markdown","source":"Evaluating performances"},{"metadata":{"trusted":false,"_uuid":"ba4c7c344c18df8be9043b9f3d5a5995262d7716"},"cell_type":"code","source":"thresh = .5\n\nprint('Training:')\ntrain_auc, train_accuracy, train_recall, train_precision, train_specificity = print_report(y_train,y_train_preds, thresh)\nprint('Validation:')\nvalid_auc, valid_accuracy, valid_recall, valid_precision, valid_specificity = print_report(y_valid,y_valid_preds, thresh)\nprint('Test:')\ntest_auc, test_accuracy, test_recall, test_precision, test_specificity = print_report(y_test,y_test_preds, thresh)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d164d3857f0e15b2e53ab86f2dcd5b4dba31b82"},"cell_type":"markdown","source":"### The ROC Curve"},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"32f8ae94a29a744b311a7c9a9244d76401ef9892"},"cell_type":"code","source":"from sklearn.metrics import roc_curve \n\nfpr_train, tpr_train, thresholds_train = roc_curve(y_train, y_train_preds)\nauc_train = roc_auc_score(y_train, y_train_preds)\n\nfpr_valid, tpr_valid, thresholds_valid = roc_curve(y_valid, y_valid_preds)\nauc_valid = roc_auc_score(y_valid, y_valid_preds)\n\nfpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_test_preds)\nauc_test = roc_auc_score(y_test, y_test_preds)\n\nplt.plot(fpr_train, tpr_train, 'r-',label ='Train AUC:%.3f'%auc_train)\nplt.plot(fpr_valid, tpr_valid, 'b-',label ='Valid AUC:%.3f'%auc_valid)\nplt.plot(fpr_test, tpr_test, 'g-',label ='Test AUC:%.3f'%auc_test)\nplt.plot([0,1],[0,1],'k--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bad41bc3b3bc26bbde31d39b06c74282cebe6892"},"cell_type":"markdown","source":"Our data shows that the results for the training, validation and testing data sets are skewed towards the true positive rate and above the treshold of 0.5 which is great because for example the test AUC of 0.791 means that the models predicts that 79.1% of the customers agree to open a bank term deposit and in actuallity the customers opens a bank term deposit."},{"metadata":{"trusted":false,"_uuid":"8c74a4c9aa435ddcdea1f4bda0a4a7d0fda56ec3"},"cell_type":"code","source":"df_data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df3bc8455e90c044d93491101eda8da2b89fbc5c"},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{"_uuid":"3d9f6db5f74519aefbaa8060d3fd0b1d5663f776"},"cell_type":"markdown","source":"- Our AUC for the test is 0.791 which means that we are 79.1% certain of the customers opening a bank deposit compared to human prediction of 11.4%\n\n- A precision of 0.37 divided by a prevalence of 0.11gives us 3.36, which means that the machine learning model helps us 3 times better than randomly guessing\n\n- We should focus on targeting customers with high consumer confidence index and consumer price index and other high importance features as  they are paramount to the performance of the model and in turn the business\n\n- Therefore, we save time and money in knowing the kinds of people we should call and that will lead to more customers and revenue"}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}