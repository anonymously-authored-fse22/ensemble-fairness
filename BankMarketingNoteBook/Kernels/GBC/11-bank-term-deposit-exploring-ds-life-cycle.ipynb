{"cells":[{"metadata":{},"cell_type":"markdown","source":"\n1. What is the Business Requirement.\n    * Predict the client will subscribe term deposit from Bank.\n    * Type of Analysis - Predictive Analysis \n    * Type of Learning - Supervised learning and it is Categorical problem with binary Categorization \n    * Target Variable - y -  (Has the client subscribed a term deposit?)\n    *\n    \n2. Data Understanding and Explore\n    * Data Set - Downloaded from https://www.kaggle.com/henriqueyamahata/bank-marketing#bank-additional-full.csv\n    * Data Science Tool - Python with Popular frameworks like - Panda, numpy ,sklearn,matplotlib ,flask and Pickle \n    * Data Science Environment - Local and Used Web API\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#2.1 Import python libraries to Load Data Set and downloaded to same directory in which this python file saved.\nimport pandas as pd\nimport numpy as np\nimport os\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns',500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Load Data from csv file\nbank_df = pd.read_csv('../input/bank-additional-full.csv', sep = ';')\n#bank_df = pd.read_csv('bank-additional-full_v1.csv')\nbank_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.Data Explore and Preparation \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#3.1 Identify Input feature Type - Categorical or Numerical \n#Input Numerical columns (10) - age duration campaign pdays previous emp.var.rate cons.price.idx cons.conf.idx euribor3m nr.employed\n#Input Categorical Columns (10) - Job,marital,education,default,housing,loan ,contact,month ,day_of_week,poutcome          \n\n#Check if any missing value in any of the feature\nbank_df.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#3.2 Convert Target feature value to 0 and 1\nbank_df['y'].unique()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df['y'] = np.where(bank_df['y']== 'yes',1,0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df['y'] = bank_df['y'].astype(np.int64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets Analyse Categorical input varaible\n# For Better Predictive model lets avoid unknown value for marital , job and education. - To DO\n# Remove records with value - unknown for education, job and marital and assume it is mandatory for predictive model - To DO\n#bank_df = bank_df[(bank_df['marital'] != 'unknown')]\n#bank_df = bank_df[(bank_df['education'] != 'unknown')]\n#bank_df = bank_df[(bank_df['job'] != 'unknown')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualize the Categorical feature distribution and relation with target variable. letâ€™s create a User Defined function\ndef Categorical_Grapgh(data,catfeature,distributionName):\n    fig, ax = plt.subplots()\n    fig.set_size_inches(20, 8)\n    sns.countplot(x = catfeature, data = data)\n    ax.set_xlabel(catfeature, fontsize=15)\n    ax.set_ylabel('Count', fontsize=15)\n    ax.set_title(distributionName, fontsize=15)\n    ax.tick_params(labelsize=15)\n    sns.despine()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Categorical_Grapgh(bank_df,'job','job Distribution')\nCategorical_Grapgh(bank_df,'marital','maritial Distribution')\nCategorical_Grapgh(bank_df,'education','education Distribution')\nCategorical_Grapgh(bank_df,'default','default Distribution')\nCategorical_Grapgh(bank_df,'housing','housing Distribution')\nCategorical_Grapgh(bank_df,'loan','loan Distribution')\nCategorical_Grapgh(bank_df,'contact','contact Distribution')\nCategorical_Grapgh(bank_df,'month','month Distribution')\nCategorical_Grapgh(bank_df,'day_of_week','day_of_week Distribution')\nCategorical_Grapgh(bank_df,'poutcome','poutcome Distribution')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Input Categorical feature Observation.\n\n* Job - More Job types are Admin , Technician and blue-collor and it means bank targeting high salaried people. \n* Marital - more people of type married - (#To Do - Check the target value distribution for high salaried married people) \n* Education - more count in university.degree people . of course High salaried people should have university degree :) expected. And illiterate count is very less. \n* default - most people have no credit , what it means? \n* housing - comparatively normal distribution loan - Not interested in personal loan :) , interesting. \n* contact - of course cellular communication is more convenient then telephone \n* month - Seems May is busy season in Portuguese \n* Day_of_week - Seems every day is busy but no weekends. \n* p_outcome - Success is small rate. (#To DO - Check how success correlates without put parameter ?)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check How Categorize variables correlated with Target Variables and How it impacted.\nfrom scipy import stats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check How Job Type , Education are correlated with Target Variable\nbank_df.groupby(['job','y']).y.count()\n#Admin are more interested in Term Deposit.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"F, p = stats.f_oneway(bank_df[bank_df.job=='admin.'].y,\n                      bank_df[bank_df.job=='blue-collar'].y,\n                      bank_df[bank_df.job=='entrepreneur'].y,\n                      bank_df[bank_df.job=='housemaid'].y,\n                      bank_df[bank_df.job=='management'].y,\n                      bank_df[bank_df.job=='retired'].y,\n                      bank_df[bank_df.job=='self-employed'].y,\n                      bank_df[bank_df.job=='services'].y,\n                      bank_df[bank_df.job=='student'].y,\n                      bank_df[bank_df.job=='technician'].y,\n                      bank_df[bank_df.job=='unemployed'].y\n                      )\nprint(F)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seems JOB has little impact on target variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df.groupby(['marital','y']).y.count()\n#married people are more interested in Term Deposit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df.groupby(['job','marital','y']).y.count()\n# And Admin - married people are more interested in Term Deposit.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df.groupby(['contact','y']).y.count()\n#Contact field has good correlation with Target variable. Since we have two observation for contact lets convert this to binary format. cellular -1 and telephone=0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"F, p = stats.f_oneway(bank_df[bank_df.contact=='telephone'].y,\n                      bank_df[bank_df.contact=='cellular'].y)\nprint(F)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"F, p = stats.f_oneway(bank_df[bank_df.day_of_week=='mon'].y,\n                      bank_df[bank_df.day_of_week=='tue'].y,\n                      bank_df[bank_df.day_of_week=='wed'].y,\n                      bank_df[bank_df.day_of_week=='thu'].y,\n                      bank_df[bank_df.day_of_week=='fri'].y)\nprint(F)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df.groupby(['day_of_week','y']).age.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# day_of_week - No significant correlation with Target variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"F, p = stats.f_oneway(bank_df[bank_df.loan=='no'].y,\n                      bank_df[bank_df.loan=='yes'].y,\n                      bank_df[bank_df.loan=='unknown'].y)\nprint(F)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loan - No correlation with Target variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df.groupby(['loan','y']).age.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df.groupby(['default','y']).age.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"F, p = stats.f_oneway(bank_df[bank_df.default=='no'].y,\n                      bank_df[bank_df.default=='yes'].y,\n                      bank_df[bank_df.default=='unknown'].y)\nprint(F)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df.groupby(['housing','y']).age.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"F, p = stats.f_oneway(bank_df[bank_df.housing=='no'].y,\n                      bank_df[bank_df.housing=='yes'].y,\n                      bank_df[bank_df.housing=='unknown'].y)\nprint(F)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# housing - No significant Relation with Target Variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df.groupby(['poutcome','y']).age.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"F, p = stats.f_oneway(bank_df[bank_df.poutcome=='success'].y,\n                      bank_df[bank_df.poutcome=='failure'].y,\n                      bank_df[bank_df.poutcome=='nonexistent'].y)\nprint(F)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# poutcome - Good Relation with Target Variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df.groupby(['month','y']).age.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Convert Categorical column to Continues Type. use Label conceding for ordinal category and one hot encoding for Nominal\nprint(bank_df.job.unique())\n#job  - Nominal\nprint(bank_df.marital.unique())\n#Maritial Nominal\nprint(bank_df.education.unique())\n#education - Ordinary\nprint(bank_df.default.unique())\n# seems Ordinary if we put -1 for unknown\nprint('housing', bank_df.housing.unique())\n# seems Ordinary if we put -1 for unknown\nprint(bank_df.loan.unique())\n# seems Ordinary if we put -1 for unknown\nprint(bank_df.contact.unique())\n# Nominal\nprint(bank_df.month.unique())\n#ordinal\nprint(bank_df.day_of_week.unique())\n#ordinal\nprint(bank_df.poutcome.unique())\n#ordinal if we put -1 for nonexistent","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlabelencoder_X = LabelEncoder()\n\nbank_df = pd.get_dummies(bank_df,columns=['job','marital','education','default','housing','loan'])\nbank_df['month'].replace(['mar','apr','may','jun','jul','aug','sep','oct','nov','dec'], [1,2,3,4,5,6,7,8,9,10], inplace  = True)\n#labelencoder_X.fit(bank_df['day_of_week'])\n#bank_df['day_of_week'] = labelencoder_X.transform(bank_df['day_of_week'])\nbank_df['day_of_week'].replace(['mon','tue','wed','thu','fri'],[1,2,3,4,5],inplace=True)\n#labelencoder_X.fit(bank_df['poutcome'])\n#bank_df['poutcome'] = labelencoder_X.transform(bank_df['poutcome'])\nbank_df['poutcome'].replace(['nonexistent', 'failure', 'success'], [1,2,3], inplace  = True)\nbank_df['isCellular'] = bank_df['contact']\nbank_df['isCellular'].replace(['telephone', 'cellular'], [0,1], inplace  = True)\n#bank_df['default'].replace(['unknown','no', 'yes'], [1,2,3], inplace  = True)\n#bank_df['housing'].replace(['no' ,'yes'], [0,1], inplace  = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#bank_df = bank_df.drop(['loan'],axis=1)\n#bank_df = bank_df.drop(['day_of_week'],axis=1)\n#bank_df = bank_df.drop(['housing'],axis=1)\nbank_df = bank_df.drop(['contact'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets Analyze Continuous features. - Use Describe and Correlation function.\nbank_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df.corr()\n# Input feature - nr.employed and  euribor3m (.94) and emp.var.rate and nr.employed (.90) \n#and euribor3m and emp.var.rate (.97) are more correlated and we can remove on column.\n# And lets Remove columns - euribor3m and emp.var.rate ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(40,40)) \nsns.heatmap(bank_df.corr())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Remove Low Correlated input variable \nbank_df = bank_df.drop(['euribor3m'],axis=1)\nbank_df = bank_df.drop(['emp.var.rate'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check outlier if any for Numberic column.\nbank_df.age.plot(kind='box')\n# There are outlier and check max age and age greated than 90","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(bank_df.age.max())\nbank_df[bank_df['age'] > 80].head(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df.age.plot(kind='hist')\n# it is bit positively skewed but it is ok and seems no high dependency with Output variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df.age.plot(kind='kde')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Binning for all numeric fields base on Box plot quantile\ndef binning(dataframe,featureName):\n    print (featureName)\n    q1 = dataframe[featureName].quantile(0.25)\n    q2 = dataframe[featureName].quantile(0.50)\n    q3 = dataframe[featureName].quantile(0.75)\n    dataframe.loc[(dataframe[featureName] <= q1), featureName] = 1\n    dataframe.loc[(dataframe[featureName] > q1) & (dataframe[featureName] <= q2), featureName] = 2\n    dataframe.loc[(dataframe[featureName] > q2) & (dataframe[featureName] <= q3), featureName] = 3\n    dataframe.loc[(dataframe[featureName] > q3), featureName] = 4 \n    print (q1, q2, q3)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"binning(bank_df,'age')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let check campaign field now and it is positively skewed..\nbank_df.campaign.plot(kind='hist')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df.campaign.plot(kind='box')\n# lot of exreme values.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(bank_df.campaign.max())\nprint(bank_df.campaign.mean())\nprint(bank_df.campaign.median())\nprint(bank_df.campaign.unique())\nprint('Y=1 for campaign > 10' , bank_df[(bank_df['campaign'] > 10) & (bank_df['y'] ==1)].age.count())\nprint('Y=1 for campaign < 10' , bank_df[(bank_df['campaign'] <= 10) & (bank_df['y'] ==1)].age.count())\nprint('Y=1 for campaign = 1' , bank_df[(bank_df['campaign'] == 1) & (bank_df['y'] ==1)].age.count())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df.groupby(['campaign','y']).y.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df['campaign'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"q1 = bank_df['campaign'].quantile(0.25)\nq2 = bank_df['campaign'].quantile(0.50)\nq3 = bank_df['campaign'].quantile(0.75)\n\nprint(q1)\nprint(q2)\nprint(q3)\n\niqr = q3-q1 #Interquartile range\n\nextreme_low_campaign = q1-1.5*iqr\nextreme_high_capmaign = q3+1.5*iqr\n\nprint (extreme_low_campaign)\nprint (extreme_high_capmaign)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"binning(bank_df,'campaign')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pdays - number of days that passed by after the client was last contacted from a previous \n#campaign (numeric; 999 means client was not previously contacted)\n# exclude Pdays = 999\nbank_df[bank_df['pdays'] != 999].pdays.plot(kind='hist')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df[bank_df['pdays'] != 999].pdays.plot(kind='box')\n#sems Box plot is not applicable here ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets replace 999 with 0 to avoid extrem upper bound impact in machine learning.\n\nbank_df.loc[(bank_df['pdays'] >= 0) & (bank_df['pdays'] <= 5), 'pdays'] = 2\nbank_df.loc[(bank_df['pdays'] > 5) & (bank_df['pdays'] <= 10), 'pdays'] = 3\nbank_df.loc[(bank_df['pdays'] > 10) & (bank_df['pdays'] <= 20), 'pdays'] = 4\nbank_df.loc[(bank_df['pdays'] > 20) & (bank_df['pdays'] != 999) , 'pdays'] = 5 \n\nbank_df.loc[(bank_df['pdays'] == 999), 'pdays'] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df.pdays.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df.groupby(['previous','y']).age.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df.duration.plot(kind='box')\n\n#sems Box plot is not applicable here ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df.duration.plot(kind='hist')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df.duration.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df[bank_df['duration'] > 3000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#binning(bank_df,'duration')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df[['cons.price.idx','cons.conf.idx','nr.employed','y']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    bank_df[['cons.price.idx','cons.conf.idx','nr.employed','y']].corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#UDF for create model\nm_bank_df=bank_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m_bank_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Split_Data(processeddata):\n    #processeddata['y'] = np.where(processeddata['y'] == 'yes',1,0)\n    columns = [column for column in processeddata.columns if column != 'y']\n    columns  = ['y']+columns\n    processeddata= processeddata[columns]\n\n    y=processeddata['y'].ravel()\n    del processeddata['y']\n    X= processeddata.as_matrix().astype('float')\n\n    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=.2,random_state=0)\n    return X_train,X_test,y_train,y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Convert_Model(X_train,y_train,X_test,y_test,classifier):\n     from sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix\n     classifier.fit(X_train,y_train) \n     print(classifier.score(X_test,y_test)) \n     print(confusion_matrix(y_test,classifier.predict(X_test)))\n     print(accuracy_score(y_test,classifier.predict(X_test)))\n     print(precision_score(y_test,classifier.predict(X_test)))\n     print(recall_score(y_test,classifier.predict(X_test)))\n     f1 = 2 * precision_score(y_test,classifier.predict(X_test)) * recall_score(y_test,classifier.predict(X_test)) / (precision_score(y_test,classifier.predict(X_test)) + recall_score(y_test,classifier.predict(X_test)))\n     print(\"f1 score\", f1)\n     return classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = Split_Data(m_bank_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# inport Dummy Classifier for creating Base Model\nfrom sklearn.dummy import DummyClassifier\nclassifier = DummyClassifier(strategy='most_frequent',random_state=0)\nfinalModel = Convert_Model(X_train,y_train,X_test,y_test,classifier)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# inport Dummy Classifier for creating Base Model\nfrom sklearn.linear_model import LogisticRegression\nclassifier_lr = LogisticRegression(random_state=0)\nfinalModel_lr = Convert_Model(X_train,y_train,X_test,y_test,classifier_lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# roc curve and auc on imbalanced dataset\nfrom sklearn.datasets import make_classification\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom matplotlib import pyplot\nprobs = finalModel_lr.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n# calculate AUC\nauc = roc_auc_score(y_test, probs)\nprint('AUC: %.3f' % auc)\n# calculate roc curve\nfpr, tpr, thresholds = roc_curve(y_test, probs)\n# plot no skill\npyplot.plot([0, 1], [0, 1], linestyle='--')\n# plot the precision-recall curve for the model\npyplot.plot(fpr, tpr, marker='.')\n# show the plot\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier()\nfinalModel_gb = Convert_Model(X_train,y_train,X_test,y_test,gb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# roc curve and auc on imbalanced dataset\nfrom sklearn.datasets import make_classification\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom matplotlib import pyplot\nprobs = finalModel_gb.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n# calculate AUC\nauc = roc_auc_score(y_test, probs)\nprint('AUC: %.3f' % auc)\n# calculate roc curve\nfpr, tpr, thresholds = roc_curve(y_test, probs)\n# plot no skill\npyplot.plot([0, 1], [0, 1], linestyle='--')\n# plot the precision-recall curve for the model\npyplot.plot(fpr, tpr, marker='.')\n# show the plot\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion \nGradientBoosting Classifier is the best classifier to target this proplem.\n"},{"metadata":{},"cell_type":"markdown","source":"# Part 2\nReal Time Integration using API. Here we have to remove duration column since this is not known when Bank agent initiate the call to customer."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Ignore Duration field as it is - Duration: last contact duration, in seconds (numeric). \n#Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\nm_bank_df = m_bank_df.drop(['duration'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = Split_Data(m_bank_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier()\nfinalModel_gb = Convert_Model(X_train,y_train,X_test,y_test,gb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Save model to file"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import pickle library\nimport pickle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the file paths\nmodel_file_path = os.path.join('gb_model.pkl')\n#scaler_file_path = os.path.join(os.path.pardir,'models','lr_scaler.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# open the files to write \nmodel_file_pickle = open(model_file_path, 'wb')\n#scaler_file_pickle = open(scaler_file_path, 'wb')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# open the files to write \nmodel_file_pickle = open(model_file_path, 'wb')\n#scaler_file_pickle = open(scaler_file_path, 'wb')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# persist the model and scaler\npickle.dump(finalModel_lr, model_file_pickle)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# close the file\nmodel_file_pickle.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Tru Negative   |  False Positive\n#False Negative |  True Positive\n\n#Accuarcay - Correct Prediction / Total Count\n\n#Precision - TP / (TP + FP)\n\n#ReCall - TP / (TP + FN)"},{"metadata":{},"cell_type":"markdown","source":"# Creating API using flask "},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nhello_world_script_file = os.path.join('bank_api.py')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile $hello_world_script_file\nimport pandas as pd\nimport json\nfrom flask import Flask, request\nimport numpy as np\nimport os\nimport pickle\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\napp = Flask(__name__)\n\n# Load Model and Scaler Files\nmodel_path = os.path.join('models')\nmodel_filepath = os.path.join(model_path, 'lr_model.pkl')\n\nmodel = pickle.load(open(model_filepath))\n\n\n@app.route('/api', methods=['POST'])\ndef say_hello():\n    print('Reached here  1')\n    data = json.dumps(request.get_json(force=True))\n    print(data)\n    print('Reached here  2 ')\n      \n    # create pandas dataframe using json string\n    my_df = pd.read_json(data)\n   \n    print(my_df['age'])\n   \n    my_df.info()\n    #my_df=json.loads(request.get_json(force=True))\n    print(my_df['campaign'])\n    my_df['age'] = my_df['age'].astype(np.int64)\n    my_df['campaign'] = my_df['campaign'].astype(np.int64)\n    my_df['pdays'] = my_df['pdays'].astype(np.int64)\n    my_df['previous'] = my_df['previous'].astype(np.int64)\n    \n    inputData=my_df\n    print(\"Data Preparation and input structure\")\n    inputData = inputData.drop(['euribor3m'],axis=1)\n    inputData = inputData.drop(['loan'],axis=1)\n    inputData = inputData.drop(['day_of_week'],axis=1)\n    inputData = inputData.drop(['emp.var.rate'],axis=1)\n    inputData = inputData.drop(['duration'],axis=1)\n    inputData = inputData.drop(['housing'],axis=1)\n    #inputData = inputData.drop(['cons.conf.idx'],axis=1)\n   \n    inputData = pd.get_dummies(inputData,columns=['job','marital','education','default'])\n    inputData['month'].replace(['mar','apr','may','jun','jul','aug','sep','oct','nov','dec'], [1,2,3,4,5,6,7,8,9,10], inplace  = True)\n    inputData['poutcome'].replace(['nonexistent', 'failure', 'success'], [1,2,3], inplace  = True)\n    inputData['isCellular'] =inputData['contact']\n    inputData['isCellular'].replace(['telephone', 'cellular'], [0,1], inplace  = True)\n    inputData = inputData.drop(['contact'],axis=1)\n  \n    print(\"Data Preparation and input structure\")\n    \n    inputData.loc[(inputData['age'] <=32),'age'] = 1\n    inputData.loc[(inputData['age'] > 32) &  (inputData['age'] <= 38),  'age'] = 2\n    inputData.loc[(inputData['age'] > 38) & (inputData['age'] <= 47),  'age'] = 3\n    inputData.loc[(inputData['age'] > 47), 'age'] = 4 \n\n    inputData.loc[(inputData['campaign'] <= 1) , 'campaign'] = 1\n    inputData.loc[(inputData['campaign'] > 1) & (inputData['campaign'] <= 2), 'campaign'] = 2\n    inputData.loc[(inputData['campaign'] > 2) & (inputData['campaign'] <= 3), 'campaign'] = 3\n    inputData.loc[(inputData['campaign'] > 3) , 'campaign'] = 4 \n \n    inputData.loc[(inputData['pdays'] >= 0) & (inputData['pdays'] <= 5), 'pdays'] = 2\n    inputData.loc[(inputData['pdays'] > 5) & (inputData['pdays'] <= 10), 'pdays'] = 3\n    inputData.loc[(inputData['pdays'] > 10) & (inputData['pdays'] <= 20), 'pdays'] = 4\n    inputData.loc[(inputData['pdays'] > 20) & (inputData['pdays'] != 999) , 'pdays'] = 5 \n    inputData.loc[(inputData['pdays'] == 999), 'pdays'] = 1\n    \n    column = [u'age', u'contact', u'month', u'campaign', u'pdays', u'previous',\n       u'poutcome', u'cons.price.idx', u'nr.employed', u'y', u'job_admin.',\n       u'job_blue-collar', u'job_entrepreneur', u'job_housemaid',\n       u'job_management', u'job_retired', u'job_self-employed',\n       u'job_services', u'job_student', u'job_technician', u'job_unemployed',\n       u'job_unknown', u'marital_divorced', u'marital_married',\n       u'marital_single', u'marital_unknown', u'education_basic.4y',\n       u'education_basic.6y', u'education_basic.9y', u'education_high.school',\n       u'education_illiterate', u'education_professional.course',\n       u'education_university.degree', u'education_unknown', u'housing_no',\n       u'housing_unknown', u'housing_yes', u'default_no', u'default_unknown',\n       u'default_yes']\n\n    input_columns = inputData.columns\n    dif = list(set(column) - set(input_columns))\n    print (dif)\n\n    for x in dif:\n        if x != 'y':\n            inputData.insert(1, x, 0)\n            inputData[x] = inputData[x].astype(np.uint8)\n        \n    print(list(set(column) - set(input_columns)))\n   \n    inputData.info()\n    prediction = model.predict(inputData)\n    result = pd.DataFrame({'result': prediction})\n    print (result['result'])\n    return result.to_json(orient='records')\n    #return \"{0}\".format(result['result'])\n\nif __name__ == '__main__':\n    app.run(port=10001, debug=True)\n   ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}