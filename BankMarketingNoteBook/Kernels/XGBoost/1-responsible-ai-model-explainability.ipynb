{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://www.logikk.com/wp-content/uploads/2019/01/AI_Ethics_Graphic.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Model Interpretability\n",
    "\n",
    "This kernel was used for a presentation at ODSC New Delhi Meetup, which can be accessed [here](https://app.aiplus.training/courses/explainable-ai-and-interpret-ability-of-ai-solutions-strategic-overview-challenges-and-caveats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Have you been rejected from a job by an ATS? Chances are, it was a poorly trained one.\n",
    "Do you now someone who was denied a loan without any reason?\n",
    "How do you ensure you algorithm doesn't discriminate? You hold it accountable.\n",
    "This notebook lists a few Pythonian ways to do so, using libraries that explain the way predictions are made. \n",
    "    \n",
    "<b> Please upvote this if you find it useful </b>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:27:51.808547Z",
     "iopub.status.busy": "2021-07-04T12:27:51.808189Z",
     "iopub.status.idle": "2021-07-04T12:27:51.818922Z",
     "shell.execute_reply": "2021-07-04T12:27:51.817163Z",
     "shell.execute_reply.started": "2021-07-04T12:27:51.80849Z"
    }
   },
   "source": [
    "<h1> Importing the libraries </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:28:45.411133Z",
     "iopub.status.busy": "2021-07-04T12:28:45.410655Z",
     "iopub.status.idle": "2021-07-04T12:28:45.419701Z",
     "shell.execute_reply": "2021-07-04T12:28:45.418013Z",
     "shell.execute_reply.started": "2021-07-04T12:28:45.411054Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Some sklearn tools for preprocessing and building a pipeline. \n",
    "# ColumnTransformer was introduced in 0.20 so make sure you have this version\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Our algorithms, by from the easiest to the hardest to intepret.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.datasets import StandardDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "import matplotlib.patches as patches\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "#from packages import *\n",
    "#from ml_fairness import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This notebook uses features introduced in Python 3.6 and sklearn 0.20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset can be downloaded [here](https://archive.ics.uci.edu/ml/datasets/bank+marketing). It consists of data from marketing campaigns of a Portuguese bank. We will try to build classifiers that can predict whether or not the client targeted by the campaign ended up subscribing to a term deposit (column `y`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:29:03.172841Z",
     "iopub.status.busy": "2021-07-04T12:29:03.172533Z",
     "iopub.status.idle": "2021-07-04T12:29:03.535992Z",
     "shell.execute_reply": "2021-07-04T12:29:03.534688Z",
     "shell.execute_reply.started": "2021-07-04T12:29:03.172791Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no     36548\n",
       "yes     4640\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../Data/bank-additional-full.csv', sep = ';')\n",
    "\n",
    "df.y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is imbalanced, we will need to keep that in mind when building our models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:29:08.04346Z",
     "iopub.status.busy": "2021-07-04T12:29:08.043028Z",
     "iopub.status.idle": "2021-07-04T12:29:08.073201Z",
     "shell.execute_reply": "2021-07-04T12:29:08.072314Z",
     "shell.execute_reply.started": "2021-07-04T12:29:08.043373Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get X, y\n",
    "y = df[\"y\"].map({\"no\":0, \"yes\":1})\n",
    "X = df.drop(\"y\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the features in the X matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data dictionary</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. age (numeric)\n",
    "2. job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n",
    "3. marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n",
    "4. education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n",
    "5. default: has credit in default? (categorical: 'no','yes','unknown')\n",
    "6. housing: has housing loan? (categorical: 'no','yes','unknown')\n",
    "7. loan: has personal loan? (categorical: 'no','yes','unknown')\n",
    "8. contact: contact communication type (categorical: 'cellular','telephone') \n",
    "9. month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n",
    "10. day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n",
    "11. duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n",
    "12. campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "13. pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n",
    "14. previous: number of contacts performed before this campaign and for this client (numeric)\n",
    "15. poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n",
    "16. emp.var.rate: employment variation rate - quarterly indicator (numeric)\n",
    "17. cons.price.idx: consumer price index - monthly indicator (numeric) \n",
    "18. cons.conf.idx: consumer confidence index - monthly indicator (numeric) \n",
    "19. euribor3m: euribor 3 month rate - daily indicator (numeric)\n",
    "20. nr.employed: number of employees - quarterly indicator (numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the comment about `duration` feature. We will exclude it from our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:29:43.024988Z",
     "iopub.status.busy": "2021-07-04T12:29:43.024695Z",
     "iopub.status.idle": "2021-07-04T12:29:43.039815Z",
     "shell.execute_reply": "2021-07-04T12:29:43.038283Z",
     "shell.execute_reply.started": "2021-07-04T12:29:43.02495Z"
    }
   },
   "outputs": [],
   "source": [
    "X.drop(\"duration\", inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:29:44.327942Z",
     "iopub.status.busy": "2021-07-04T12:29:44.327581Z",
     "iopub.status.idle": "2021-07-04T12:29:44.340554Z",
     "shell.execute_reply": "2021-07-04T12:29:44.339365Z",
     "shell.execute_reply.started": "2021-07-04T12:29:44.32789Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                 int64\n",
       "job                object\n",
       "marital            object\n",
       "education          object\n",
       "default            object\n",
       "housing            object\n",
       "loan               object\n",
       "contact            object\n",
       "month              object\n",
       "day_of_week        object\n",
       "campaign            int64\n",
       "pdays               int64\n",
       "previous            int64\n",
       "poutcome           object\n",
       "emp.var.rate      float64\n",
       "cons.price.idx    float64\n",
       "cons.conf.idx     float64\n",
       "euribor3m         float64\n",
       "nr.employed       float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:29:45.739917Z",
     "iopub.status.busy": "2021-07-04T12:29:45.739555Z",
     "iopub.status.idle": "2021-07-04T12:29:45.745203Z",
     "shell.execute_reply": "2021-07-04T12:29:45.744371Z",
     "shell.execute_reply.started": "2021-07-04T12:29:45.739863Z"
    }
   },
   "outputs": [],
   "source": [
    "# Some such as default would be binary features, but since\n",
    "# they have a third class \"unknown\" we'll process them as non binary categorical\n",
    "num_features = [\"age\", \"campaign\", \"pdays\", \"previous\", \"emp.var.rate\", \n",
    "                \"cons.price.idx\", \"cons.conf.idx\",\"euribor3m\", \"nr.employed\"]\n",
    "\n",
    "cat_features = [\"job\", \"marital\", \"education\",\"default\", \"housing\", \"loan\",\n",
    "                \"contact\", \"month\", \"day_of_week\", \"poutcome\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We'll define a new `ColumnTransformer` object (new in sklearn 0.20) that keeps our numerical features and apply one hot encoding on our categorical features. That will allow us to create a clean pipeline that includes both features engineering (one hot encoding here) and training the model (a nice way to avoid data leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:29:49.615191Z",
     "iopub.status.busy": "2021-07-04T12:29:49.614548Z",
     "iopub.status.idle": "2021-07-04T12:29:49.621509Z",
     "shell.execute_reply": "2021-07-04T12:29:49.619983Z",
     "shell.execute_reply.started": "2021-07-04T12:29:49.615103Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([(\"numerical\", \"passthrough\", num_features), \n",
    "                                  (\"categorical\", OneHotEncoder(sparse=False, handle_unknown=\"ignore\"),\n",
    "                                   cat_features)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define our 4 models as sklearn `Pipeline` object, containing our preprocessing step and training of one given algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Creating model training pipelines </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:30:28.053528Z",
     "iopub.status.busy": "2021-07-04T12:30:28.053151Z",
     "iopub.status.idle": "2021-07-04T12:30:28.062015Z",
     "shell.execute_reply": "2021-07-04T12:30:28.060323Z",
     "shell.execute_reply.started": "2021-07-04T12:30:28.053475Z"
    }
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "lr_model = Pipeline([(\"preprocessor\", preprocessor), \n",
    "                     (\"model\", LogisticRegression(class_weight=\"balanced\", solver=\"liblinear\", random_state=42))])\n",
    "\n",
    "# Decision Tree\n",
    "dt_model = Pipeline([(\"preprocessor\", preprocessor), \n",
    "                     (\"model\", DecisionTreeClassifier(class_weight=\"balanced\"))])\n",
    "\n",
    "# Random Forest\n",
    "rf_model = Pipeline([(\"preprocessor\", preprocessor), \n",
    "                     (\"model\", RandomForestClassifier(class_weight=\"balanced\", n_estimators=100, n_jobs=-1))])\n",
    "\n",
    "# XGBoost\n",
    "xgb_model = Pipeline([(\"preprocessor\", preprocessor), \n",
    "                      # Add a scale_pos_weight to make it balanced\n",
    "                      (\"model\", XGBClassifier(scale_pos_weight=(1 - y.mean()), n_jobs=-1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data into training and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Splitting the data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:31:04.404416Z",
     "iopub.status.busy": "2021-07-04T12:31:04.404075Z",
     "iopub.status.idle": "2021-07-04T12:31:04.455685Z",
     "shell.execute_reply": "2021-07-04T12:31:04.454616Z",
     "shell.execute_reply.started": "2021-07-04T12:31:04.404337Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're good to go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Libraries for Explainable AI </h1>\n",
    "<h2> The Eli5 Library: Intepreting \"white box\" models </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's fine tune our logistic regression and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:31:57.500838Z",
     "iopub.status.busy": "2021-07-04T12:31:57.500481Z",
     "iopub.status.idle": "2021-07-04T12:32:09.910079Z",
     "shell.execute_reply": "2021-07-04T12:32:09.908755Z",
     "shell.execute_reply.started": "2021-07-04T12:31:57.500785Z"
    }
   },
   "outputs": [],
   "source": [
    "gs = GridSearchCV(lr_model, {\"model__C\": [1, 1.3, 1.5]}, n_jobs=-1, cv=5, scoring=\"accuracy\")\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see our best parameters and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:32:09.912763Z",
     "iopub.status.busy": "2021-07-04T12:32:09.912419Z",
     "iopub.status.idle": "2021-07-04T12:32:09.918153Z",
     "shell.execute_reply": "2021-07-04T12:32:09.917006Z",
     "shell.execute_reply.started": "2021-07-04T12:32:09.912707Z"
    }
   },
   "outputs": [],
   "source": [
    "print(gs.best_params_)\n",
    "print(gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:32:09.919601Z",
     "iopub.status.busy": "2021-07-04T12:32:09.91934Z",
     "iopub.status.idle": "2021-07-04T12:32:09.936127Z",
     "shell.execute_reply": "2021-07-04T12:32:09.935038Z",
     "shell.execute_reply.started": "2021-07-04T12:32:09.919556Z"
    }
   },
   "outputs": [],
   "source": [
    "lr_model.set_params(**gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:32:09.938065Z",
     "iopub.status.busy": "2021-07-04T12:32:09.937784Z",
     "iopub.status.idle": "2021-07-04T12:32:09.952741Z",
     "shell.execute_reply": "2021-07-04T12:32:09.951369Z",
     "shell.execute_reply.started": "2021-07-04T12:32:09.938014Z"
    }
   },
   "outputs": [],
   "source": [
    "lr_model.get_params(\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit the model on the whole training set and calculate accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:32:09.954663Z",
     "iopub.status.busy": "2021-07-04T12:32:09.95427Z",
     "iopub.status.idle": "2021-07-04T12:32:10.691167Z",
     "shell.execute_reply": "2021-07-04T12:32:10.689845Z",
     "shell.execute_reply.started": "2021-07-04T12:32:09.954581Z"
    }
   },
   "outputs": [],
   "source": [
    "lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:32:10.69324Z",
     "iopub.status.busy": "2021-07-04T12:32:10.692859Z",
     "iopub.status.idle": "2021-07-04T12:32:10.767623Z",
     "shell.execute_reply": "2021-07-04T12:32:10.766784Z",
     "shell.execute_reply.started": "2021-07-04T12:32:10.693167Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = lr_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:32:10.769467Z",
     "iopub.status.busy": "2021-07-04T12:32:10.76888Z",
     "iopub.status.idle": "2021-07-04T12:32:10.777877Z",
     "shell.execute_reply": "2021-07-04T12:32:10.77671Z",
     "shell.execute_reply.started": "2021-07-04T12:32:10.769403Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:32:10.780482Z",
     "iopub.status.busy": "2021-07-04T12:32:10.779483Z",
     "iopub.status.idle": "2021-07-04T12:32:10.815832Z",
     "shell.execute_reply": "2021-07-04T12:32:10.814905Z",
     "shell.execute_reply.started": "2021-07-04T12:32:10.779763Z"
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `eli5` to visualise the weights associated to each feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:32:10.817954Z",
     "iopub.status.busy": "2021-07-04T12:32:10.817348Z",
     "iopub.status.idle": "2021-07-04T12:32:11.091071Z",
     "shell.execute_reply": "2021-07-04T12:32:11.090134Z",
     "shell.execute_reply.started": "2021-07-04T12:32:10.817899Z"
    }
   },
   "outputs": [],
   "source": [
    "import eli5\n",
    "eli5.show_weights(lr_model.named_steps[\"model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**That gives us the weights associated to each feature, that can be seen as the contribution of each feature into predicting that the class will be y=1 (the client will subscribe after the campaign).**\n",
    "\n",
    "The names for each features aren't really helping though, we can pass a list of column names to `eli5` but we'll need to do a little gymnastics first to extract names from our preprocessor in the pipeline (since we've generated new features on the fly with the one hot encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:32:24.284576Z",
     "iopub.status.busy": "2021-07-04T12:32:24.28407Z",
     "iopub.status.idle": "2021-07-04T12:32:24.290495Z",
     "shell.execute_reply": "2021-07-04T12:32:24.289297Z",
     "shell.execute_reply.started": "2021-07-04T12:32:24.284339Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocessor = lr_model.named_steps[\"preprocessor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:32:24.716473Z",
     "iopub.status.busy": "2021-07-04T12:32:24.715838Z",
     "iopub.status.idle": "2021-07-04T12:32:24.720443Z",
     "shell.execute_reply": "2021-07-04T12:32:24.719802Z",
     "shell.execute_reply.started": "2021-07-04T12:32:24.716104Z"
    }
   },
   "outputs": [],
   "source": [
    "ohe_categories = preprocessor.named_transformers_[\"categorical\"].categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:32:25.201682Z",
     "iopub.status.busy": "2021-07-04T12:32:25.201023Z",
     "iopub.status.idle": "2021-07-04T12:32:25.20696Z",
     "shell.execute_reply": "2021-07-04T12:32:25.205908Z",
     "shell.execute_reply.started": "2021-07-04T12:32:25.201626Z"
    }
   },
   "outputs": [],
   "source": [
    "new_ohe_features = [f\"{col}__{val}\" for col, vals in zip(cat_features, ohe_categories) for val in vals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:32:26.040699Z",
     "iopub.status.busy": "2021-07-04T12:32:26.040163Z",
     "iopub.status.idle": "2021-07-04T12:32:26.045564Z",
     "shell.execute_reply": "2021-07-04T12:32:26.044774Z",
     "shell.execute_reply.started": "2021-07-04T12:32:26.040653Z"
    }
   },
   "outputs": [],
   "source": [
    "all_features = num_features + new_ohe_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so now we have a nice list of columns after processing. Let's visualise the data in a dataframe just for sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:32:27.453394Z",
     "iopub.status.busy": "2021-07-04T12:32:27.452831Z",
     "iopub.status.idle": "2021-07-04T12:32:27.728681Z",
     "shell.execute_reply": "2021-07-04T12:32:27.727632Z",
     "shell.execute_reply.started": "2021-07-04T12:32:27.453332Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(lr_model.named_steps[\"preprocessor\"].transform(X_train), columns=all_features).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:32:29.604946Z",
     "iopub.status.busy": "2021-07-04T12:32:29.604602Z",
     "iopub.status.idle": "2021-07-04T12:32:29.615634Z",
     "shell.execute_reply": "2021-07-04T12:32:29.614938Z",
     "shell.execute_reply.started": "2021-07-04T12:32:29.604893Z"
    }
   },
   "outputs": [],
   "source": [
    "eli5.show_weights(lr_model.named_steps[\"model\"], feature_names=all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it's picking principally on whether the month is march or not, the marketting campaign seem to have been more efficient in march?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `eli5` to explain a specific prediction, let's pick a row in the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:32:32.719222Z",
     "iopub.status.busy": "2021-07-04T12:32:32.718713Z",
     "iopub.status.idle": "2021-07-04T12:32:32.759912Z",
     "shell.execute_reply": "2021-07-04T12:32:32.759212Z",
     "shell.execute_reply.started": "2021-07-04T12:32:32.71917Z"
    }
   },
   "outputs": [],
   "source": [
    "i = 4\n",
    "X_test.iloc[[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:32:33.215658Z",
     "iopub.status.busy": "2021-07-04T12:32:33.215133Z",
     "iopub.status.idle": "2021-07-04T12:32:33.221369Z",
     "shell.execute_reply": "2021-07-04T12:32:33.220168Z",
     "shell.execute_reply.started": "2021-07-04T12:32:33.215609Z"
    }
   },
   "outputs": [],
   "source": [
    "y_test.iloc[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our client subsribed to the term deposit after the campaign! Let's see what our model would have predicted and how it would explain it.\n",
    "\n",
    "We'll need to first transform our row into the format expected by our model as `eli5` cannot work directly with our pipeline.\n",
    "\n",
    "Note: `eli5` actually does support pipeline, but with a limited number of transformations only. In our pipeline it does not support the `passthrough` transformation (which, funny enough, doesn't do anything...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:32:36.515987Z",
     "iopub.status.busy": "2021-07-04T12:32:36.515643Z",
     "iopub.status.idle": "2021-07-04T12:32:36.586293Z",
     "shell.execute_reply": "2021-07-04T12:32:36.585199Z",
     "shell.execute_reply.started": "2021-07-04T12:32:36.515933Z"
    }
   },
   "outputs": [],
   "source": [
    "eli5.show_prediction(lr_model.named_steps[\"model\"], \n",
    "                     lr_model.named_steps[\"preprocessor\"].transform(X_test)[i],\n",
    "                     feature_names=all_features, show_feature_values=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With a Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`eli5` can also be used to intepret decision trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:32:39.653539Z",
     "iopub.status.busy": "2021-07-04T12:32:39.65319Z",
     "iopub.status.idle": "2021-07-04T12:32:45.00383Z",
     "shell.execute_reply": "2021-07-04T12:32:45.002729Z",
     "shell.execute_reply.started": "2021-07-04T12:32:39.653489Z"
    }
   },
   "outputs": [],
   "source": [
    "gs = GridSearchCV(dt_model, {\"model__max_depth\": [3, 5, 7], \n",
    "                             \"model__min_samples_split\": [2, 5]}, \n",
    "                  n_jobs=-1, cv=5, scoring=\"accuracy\")\n",
    "\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see our best parameters and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:32:45.005769Z",
     "iopub.status.busy": "2021-07-04T12:32:45.005469Z",
     "iopub.status.idle": "2021-07-04T12:32:45.01211Z",
     "shell.execute_reply": "2021-07-04T12:32:45.009766Z",
     "shell.execute_reply.started": "2021-07-04T12:32:45.005701Z"
    }
   },
   "outputs": [],
   "source": [
    "print(gs.best_params_)\n",
    "print(gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:32:45.013881Z",
     "iopub.status.busy": "2021-07-04T12:32:45.013589Z",
     "iopub.status.idle": "2021-07-04T12:32:45.031981Z",
     "shell.execute_reply": "2021-07-04T12:32:45.030552Z",
     "shell.execute_reply.started": "2021-07-04T12:32:45.013825Z"
    }
   },
   "outputs": [],
   "source": [
    "dt_model.set_params(**gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:32:45.033451Z",
     "iopub.status.busy": "2021-07-04T12:32:45.033173Z",
     "iopub.status.idle": "2021-07-04T12:32:45.388861Z",
     "shell.execute_reply": "2021-07-04T12:32:45.387762Z",
     "shell.execute_reply.started": "2021-07-04T12:32:45.0334Z"
    }
   },
   "outputs": [],
   "source": [
    "dt_model.fit(X_train, y_train)\n",
    "y_pred = dt_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:32:45.390765Z",
     "iopub.status.busy": "2021-07-04T12:32:45.390514Z",
     "iopub.status.idle": "2021-07-04T12:32:45.39686Z",
     "shell.execute_reply": "2021-07-04T12:32:45.396194Z",
     "shell.execute_reply.started": "2021-07-04T12:32:45.39072Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:32:45.397919Z",
     "iopub.status.busy": "2021-07-04T12:32:45.3977Z",
     "iopub.status.idle": "2021-07-04T12:32:45.422631Z",
     "shell.execute_reply": "2021-07-04T12:32:45.421314Z",
     "shell.execute_reply.started": "2021-07-04T12:32:45.397878Z"
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Decision Trees, `eli5` only gives feature importance, which does not say in what direction a feature impact the predicted outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:32:45.424505Z",
     "iopub.status.busy": "2021-07-04T12:32:45.424216Z",
     "iopub.status.idle": "2021-07-04T12:32:45.636558Z",
     "shell.execute_reply": "2021-07-04T12:32:45.635594Z",
     "shell.execute_reply.started": "2021-07-04T12:32:45.424442Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eli5.show_weights(dt_model.named_steps[\"model\"], feature_names=all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the most important feature seems to be `nr.employed`. We can also get an explanation for a given prediction, this will calculate the contribution of each feature in the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:32:45.638634Z",
     "iopub.status.busy": "2021-07-04T12:32:45.638185Z",
     "iopub.status.idle": "2021-07-04T12:32:45.714586Z",
     "shell.execute_reply": "2021-07-04T12:32:45.713322Z",
     "shell.execute_reply.started": "2021-07-04T12:32:45.638583Z"
    }
   },
   "outputs": [],
   "source": [
    "eli5.show_prediction(dt_model.named_steps[\"model\"], \n",
    "                     dt_model.named_steps[\"preprocessor\"].transform(X_test)[i],\n",
    "                     feature_names=all_features, show_feature_values=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the explanation for a single prediction is calculated by following the decision path in the tree, and adding up contribution of each feature from each node crossed into the overall probability predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`eli5` can also be used to explain black box models, but we will use `Lime` and `SHAP` for our two last models instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> LIME(Local Interpretable Model-agnostic Explanation) to generate local intepretations of black box models</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIME stands for `Local Interpretable Model-Agnostic Explanations`. We can use it with any model we've built in order to explain why it took a specific decision for a given observation. To do so, LIME creates a dataset in the locality of our observation by perturbating the different features. Then it fits a local linear model on this data and uses the weights on each feature to provide an explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with a Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:33:55.822491Z",
     "iopub.status.busy": "2021-07-04T12:33:55.82211Z",
     "iopub.status.idle": "2021-07-04T12:34:18.934626Z",
     "shell.execute_reply": "2021-07-04T12:34:18.933273Z",
     "shell.execute_reply.started": "2021-07-04T12:33:55.822427Z"
    }
   },
   "outputs": [],
   "source": [
    "gs = GridSearchCV(rf_model, {\"model__max_depth\": [10, 15], \n",
    "                             \"model__min_samples_split\": [5, 10]}, \n",
    "                  n_jobs=-1, cv=5, scoring=\"accuracy\")\n",
    "\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see our best parameters and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:34:18.936323Z",
     "iopub.status.busy": "2021-07-04T12:34:18.936087Z",
     "iopub.status.idle": "2021-07-04T12:34:18.941062Z",
     "shell.execute_reply": "2021-07-04T12:34:18.940052Z",
     "shell.execute_reply.started": "2021-07-04T12:34:18.936278Z"
    }
   },
   "outputs": [],
   "source": [
    "print(gs.best_params_)\n",
    "print(gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:34:18.942587Z",
     "iopub.status.busy": "2021-07-04T12:34:18.94213Z",
     "iopub.status.idle": "2021-07-04T12:34:18.959465Z",
     "shell.execute_reply": "2021-07-04T12:34:18.958405Z",
     "shell.execute_reply.started": "2021-07-04T12:34:18.942329Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_model.set_params(**gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:34:18.961794Z",
     "iopub.status.busy": "2021-07-04T12:34:18.961483Z",
     "iopub.status.idle": "2021-07-04T12:34:20.457809Z",
     "shell.execute_reply": "2021-07-04T12:34:20.456594Z",
     "shell.execute_reply.started": "2021-07-04T12:34:18.961713Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:34:20.459267Z",
     "iopub.status.busy": "2021-07-04T12:34:20.459009Z",
     "iopub.status.idle": "2021-07-04T12:34:20.467647Z",
     "shell.execute_reply": "2021-07-04T12:34:20.466412Z",
     "shell.execute_reply.started": "2021-07-04T12:34:20.45923Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:34:20.469673Z",
     "iopub.status.busy": "2021-07-04T12:34:20.469122Z",
     "iopub.status.idle": "2021-07-04T12:34:20.492614Z",
     "shell.execute_reply": "2021-07-04T12:34:20.491709Z",
     "shell.execute_reply.started": "2021-07-04T12:34:20.46961Z"
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the features importance with Eli5 first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:34:20.494417Z",
     "iopub.status.busy": "2021-07-04T12:34:20.493904Z",
     "iopub.status.idle": "2021-07-04T12:34:20.81899Z",
     "shell.execute_reply": "2021-07-04T12:34:20.817756Z",
     "shell.execute_reply.started": "2021-07-04T12:34:20.49433Z"
    }
   },
   "outputs": [],
   "source": [
    "eli5.show_weights(rf_model.named_steps[\"model\"], \n",
    "                  feature_names=all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can explain roughly what our model seems to focus on mostly. We also get the standard deviation of feature importance accross the multiple trees in our ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train our XGB model as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:34:20.820857Z",
     "iopub.status.busy": "2021-07-04T12:34:20.820582Z",
     "iopub.status.idle": "2021-07-04T12:34:42.519008Z",
     "shell.execute_reply": "2021-07-04T12:34:42.518292Z",
     "shell.execute_reply.started": "2021-07-04T12:34:20.820808Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('preprocessor',\n",
       "                                        ColumnTransformer(transformers=[('numerical',\n",
       "                                                                         'passthrough',\n",
       "                                                                         ['age',\n",
       "                                                                          'campaign',\n",
       "                                                                          'pdays',\n",
       "                                                                          'previous',\n",
       "                                                                          'emp.var.rate',\n",
       "                                                                          'cons.price.idx',\n",
       "                                                                          'cons.conf.idx',\n",
       "                                                                          'euribor3m',\n",
       "                                                                          'nr.employed']),\n",
       "                                                                        ('categorical',\n",
       "                                                                         OneHotEncoder(handle_unknown='ignore',\n",
       "                                                                                       sparse=False),\n",
       "                                                                         ['job',\n",
       "                                                                          'marital',\n",
       "                                                                          'education',\n",
       "                                                                          'default',\n",
       "                                                                          'housing',\n",
       "                                                                          'l...\n",
       "                                                      monotone_constraints=None,\n",
       "                                                      n_estimators=100,\n",
       "                                                      n_jobs=-1,\n",
       "                                                      num_parallel_tree=None,\n",
       "                                                      random_state=None,\n",
       "                                                      reg_alpha=None,\n",
       "                                                      reg_lambda=None,\n",
       "                                                      scale_pos_weight=0.8873458288821987,\n",
       "                                                      subsample=None,\n",
       "                                                      tree_method=None,\n",
       "                                                      validate_parameters=None,\n",
       "                                                      verbosity=None))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'model__max_depth': [5, 10],\n",
       "                         'model__min_child_weight': [5, 10],\n",
       "                         'model__n_estimators': [25]},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = GridSearchCV(xgb_model, {\"model__max_depth\": [5, 10],\n",
    "                              \"model__min_child_weight\": [5, 10],\n",
    "                              \"model__n_estimators\": [25]},\n",
    "                  n_jobs=-1, cv=5, scoring=\"accuracy\")\n",
    "\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see our best parameters and score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:34:42.524533Z",
     "iopub.status.busy": "2021-07-04T12:34:42.522625Z",
     "iopub.status.idle": "2021-07-04T12:34:43.574885Z",
     "shell.execute_reply": "2021-07-04T12:34:43.574206Z",
     "shell.execute_reply.started": "2021-07-04T12:34:42.52448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model__max_depth': 5, 'model__min_child_weight': 5, 'model__n_estimators': 25}\n",
      "0.9003156091438719\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('numerical', 'passthrough',\n",
       "                                                  ['age', 'campaign', 'pdays',\n",
       "                                                   'previous', 'emp.var.rate',\n",
       "                                                   'cons.price.idx',\n",
       "                                                   'cons.conf.idx', 'euribor3m',\n",
       "                                                   'nr.employed']),\n",
       "                                                 ('categorical',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore',\n",
       "                                                                sparse=False),\n",
       "                                                  ['job', 'marital',\n",
       "                                                   'education', 'default',\n",
       "                                                   'housing', 'loan', 'contact',\n",
       "                                                   'month', 'day_...\n",
       "                               importance_type='gain',\n",
       "                               interaction_constraints='',\n",
       "                               learning_rate=0.300000012, max_delta_step=0,\n",
       "                               max_depth=5, min_child_weight=5, missing=nan,\n",
       "                               monotone_constraints='()', n_estimators=25,\n",
       "                               n_jobs=-1, num_parallel_tree=1, random_state=0,\n",
       "                               reg_alpha=0, reg_lambda=1,\n",
       "                               scale_pos_weight=0.8873458288821987, subsample=1,\n",
       "                               tree_method='exact', validate_parameters=1,\n",
       "                               verbosity=None))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n",
    "xgb_model.set_params(**gs.best_params_)\n",
    "xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:34:43.580278Z",
     "iopub.status.busy": "2021-07-04T12:34:43.578401Z",
     "iopub.status.idle": "2021-07-04T12:34:43.657288Z",
     "shell.execute_reply": "2021-07-04T12:34:43.656571Z",
     "shell.execute_reply.started": "2021-07-04T12:34:43.580227Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = xgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:34:43.663657Z",
     "iopub.status.busy": "2021-07-04T12:34:43.661614Z",
     "iopub.status.idle": "2021-07-04T12:34:43.673664Z",
     "shell.execute_reply": "2021-07-04T12:34:43.672967Z",
     "shell.execute_reply.started": "2021-07-04T12:34:43.6636Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:34:43.674991Z",
     "iopub.status.busy": "2021-07-04T12:34:43.674634Z",
     "iopub.status.idle": "2021-07-04T12:34:43.695508Z",
     "shell.execute_reply": "2021-07-04T12:34:43.694825Z",
     "shell.execute_reply.started": "2021-07-04T12:34:43.674954Z"
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an explainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to explain why the model classifies invidividual observations as class 0 or 1, we are going to use the `LimeTabularExplainer` from the library `lime`, this is the main explainer to use for tabular data. Lime also provides an explainer for text data, for images and for time-series.\n",
    "\n",
    "When using the tabular explainer, we need to provide our training set as parameter so that `lime` can compute statistics on each feature, either `mean` and `std` for numerical features, or frequency of values for categorical features. Those statistics are used to scale the data and generate new perturbated data to train our local linear models on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:34:43.69686Z",
     "iopub.status.busy": "2021-07-04T12:34:43.696505Z",
     "iopub.status.idle": "2021-07-04T12:34:43.711303Z",
     "shell.execute_reply": "2021-07-04T12:34:43.71022Z",
     "shell.execute_reply.started": "2021-07-04T12:34:43.696821Z"
    }
   },
   "outputs": [],
   "source": [
    "from lime.lime_tabular import LimeTabularExplainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters passed to the explainer are:\n",
    "- our training set, we need to make sure we use the training set *without* one hot encoding\n",
    "- `mode`: the explainer can be used for classification or regression\n",
    "- `feature_names`: list of labels for our features\n",
    "- `categorical_features`: list of indexes of categorical features\n",
    "- `categorical_names`: dict mapping each index of categorical feature to a list of corresponding labels\n",
    "- `dicretize_continuous`: will discretize numerical values into buckets that can be used for explanation. For instance it can tell us that the decision was made because distance is in bucket [5km, 10km] instead of telling us distance is an importante feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, in order to get the `categorical_names` parameter we need to build a dictionary with indexes of categorical values in original dataset as keys and lists of possible categories as values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:34:43.712809Z",
     "iopub.status.busy": "2021-07-04T12:34:43.712582Z",
     "iopub.status.idle": "2021-07-04T12:34:43.719143Z",
     "shell.execute_reply": "2021-07-04T12:34:43.717616Z",
     "shell.execute_reply.started": "2021-07-04T12:34:43.712762Z"
    }
   },
   "outputs": [],
   "source": [
    "categorical_names = {}\n",
    "for col in cat_features:\n",
    "    categorical_names[X_train.columns.get_loc(col)] = [new_col.split(\"__\")[1] \n",
    "                                                       for new_col in new_ohe_features \n",
    "                                                       if new_col.split(\"__\")[0] == col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:34:43.720849Z",
     "iopub.status.busy": "2021-07-04T12:34:43.72058Z",
     "iopub.status.idle": "2021-07-04T12:34:43.733447Z",
     "shell.execute_reply": "2021-07-04T12:34:43.732669Z",
     "shell.execute_reply.started": "2021-07-04T12:34:43.720802Z"
    }
   },
   "outputs": [],
   "source": [
    "categorical_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Lime` needs the dataset that is passed to have categorical values converted to integer labels that maps to the values in `categorical_names`. For instance, label `0` for the column `2` will map to `divorced`. We will use a custom helper function to do so, that converts data from original to LIME and from LIME to original format.\n",
    "\n",
    "That function is going over all categorical features and replacing strings by the correct integer labels, feel free to check `helpers.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:34:43.735295Z",
     "iopub.status.busy": "2021-07-04T12:34:43.735049Z",
     "iopub.status.idle": "2021-07-04T12:34:43.744795Z",
     "shell.execute_reply": "2021-07-04T12:34:43.743402Z",
     "shell.execute_reply.started": "2021-07-04T12:34:43.735251Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_lime_format(X, categorical_names, col_names=None, invert=False):\n",
    "    \"\"\"Converts data with categorical values as string into the right format \n",
    "    for LIME, with categorical values as integers labels.\n",
    "\n",
    "    It takes categorical_names, the same dictionary that has to be passed\n",
    "    to LIME to ensure consistency. \n",
    "\n",
    "    col_names and invert allow to rebuild the original dataFrame from\n",
    "    a numpy array in LIME format to be passed to a Pipeline or sklearn\n",
    "    OneHotEncoder\n",
    "    \"\"\"\n",
    "\n",
    "    # If the data isn't a dataframe, we need to be able to build it\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X_lime = pd.DataFrame(X, columns=col_names)\n",
    "    else:\n",
    "        X_lime = X.copy()\n",
    "\n",
    "    for k, v in categorical_names.items():\n",
    "        if not invert:\n",
    "            label_map = {\n",
    "                str_label: int_label for int_label, str_label in enumerate(v)\n",
    "            }\n",
    "        else:\n",
    "            label_map = {\n",
    "                int_label: str_label for int_label, str_label in enumerate(v)\n",
    "            }\n",
    "\n",
    "        X_lime.iloc[:, k] = X_lime.iloc[:, k].map(label_map)\n",
    "\n",
    "    return X_lime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that it worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:34:43.747031Z",
     "iopub.status.busy": "2021-07-04T12:34:43.746658Z",
     "iopub.status.idle": "2021-07-04T12:34:43.870638Z",
     "shell.execute_reply": "2021-07-04T12:34:43.86934Z",
     "shell.execute_reply.started": "2021-07-04T12:34:43.746956Z"
    }
   },
   "outputs": [],
   "source": [
    "convert_to_lime_format(X_train, categorical_names).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:34:43.872428Z",
     "iopub.status.busy": "2021-07-04T12:34:43.872053Z",
     "iopub.status.idle": "2021-07-04T12:34:44.239537Z",
     "shell.execute_reply": "2021-07-04T12:34:44.238575Z",
     "shell.execute_reply.started": "2021-07-04T12:34:43.872336Z"
    }
   },
   "outputs": [],
   "source": [
    "explainer = LimeTabularExplainer(convert_to_lime_format(X_train, categorical_names).values,\n",
    "                                 mode=\"classification\",\n",
    "                                 feature_names=X_train.columns.tolist(),\n",
    "                                 categorical_names=categorical_names,\n",
    "                                 categorical_features=categorical_names.keys(),\n",
    "                                 discretize_continuous=True,\n",
    "                                 random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, our explainer is ready. Now let's pick an observation we want to explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain new observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a variable called `observation` that contains our ith observation in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:34:44.241784Z",
     "iopub.status.busy": "2021-07-04T12:34:44.241416Z",
     "iopub.status.idle": "2021-07-04T12:34:44.284545Z",
     "shell.execute_reply": "2021-07-04T12:34:44.283333Z",
     "shell.execute_reply.started": "2021-07-04T12:34:44.241715Z"
    }
   },
   "outputs": [],
   "source": [
    "i = 2\n",
    "X_observation = X_test.iloc[[i], :]\n",
    "X_observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:34:44.286402Z",
     "iopub.status.busy": "2021-07-04T12:34:44.286044Z",
     "iopub.status.idle": "2021-07-04T12:34:44.419104Z",
     "shell.execute_reply": "2021-07-04T12:34:44.418262Z",
     "shell.execute_reply.started": "2021-07-04T12:34:44.286305Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"\"\"\\\n",
    "* True label: {y_test.iloc[i]}\n",
    "* LR: {lr_model.predict_proba(X_observation)[0]}\n",
    "* DT: {dt_model.predict_proba(X_observation)[0]}\n",
    "* RF: {rf_model.predict_proba(X_observation)[0]}\n",
    "* XGB: {xgb_model.predict_proba(X_observation)[0]}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert our observation to lime format and convert it to a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:34:44.426908Z",
     "iopub.status.busy": "2021-07-04T12:34:44.42479Z",
     "iopub.status.idle": "2021-07-04T12:34:44.461016Z",
     "shell.execute_reply": "2021-07-04T12:34:44.459937Z",
     "shell.execute_reply.started": "2021-07-04T12:34:44.426849Z"
    }
   },
   "outputs": [],
   "source": [
    "observation = convert_to_lime_format(X_test.iloc[[i], :],categorical_names).values[0]\n",
    "observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to explain a prediction, we use the `explain_instance` method on our explainer. This will generate new data with perturbated features around the observation and learn a local linear model. It needs to take:\n",
    "- our observation as a numpy array\n",
    "- a function that uses our model to predict probabilities given the data (in same format we've passed in our explainer). That means we cannot pass directly our `rf_model.predict_proba` because our pipeline expects string labels for categorical values. We will need to create a custom function `rf_predict_proba` that first converts back integer labels to strings and then calls `rf_model.predict_proba`.\n",
    "- `num_features`: number of features to consider in explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:34:44.462577Z",
     "iopub.status.busy": "2021-07-04T12:34:44.462313Z",
     "iopub.status.idle": "2021-07-04T12:34:44.467677Z",
     "shell.execute_reply": "2021-07-04T12:34:44.466999Z",
     "shell.execute_reply.started": "2021-07-04T12:34:44.46253Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let write a custom predict_proba functions for our models:\n",
    "from functools import partial\n",
    "\n",
    "def custom_predict_proba(X, model):\n",
    "    X_str = convert_to_lime_format(X, categorical_names, col_names=X_train.columns, invert=True)\n",
    "    return model.predict_proba(X_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:34:44.46905Z",
     "iopub.status.busy": "2021-07-04T12:34:44.4687Z",
     "iopub.status.idle": "2021-07-04T12:34:44.481484Z",
     "shell.execute_reply": "2021-07-04T12:34:44.480249Z",
     "shell.execute_reply.started": "2021-07-04T12:34:44.469014Z"
    }
   },
   "outputs": [],
   "source": [
    "lr_predict_proba = partial(custom_predict_proba, model=lr_model)\n",
    "dt_predict_proba = partial(custom_predict_proba, model=dt_model)\n",
    "rf_predict_proba = partial(custom_predict_proba, model=rf_model)\n",
    "xgb_predict_proba = partial(custom_predict_proba, model=xgb_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our custom function to make sure it generates propabilities properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:34:44.483743Z",
     "iopub.status.busy": "2021-07-04T12:34:44.483375Z",
     "iopub.status.idle": "2021-07-04T12:34:45.42815Z",
     "shell.execute_reply": "2021-07-04T12:34:45.426845Z",
     "shell.execute_reply.started": "2021-07-04T12:34:44.483669Z"
    }
   },
   "outputs": [],
   "source": [
    "explanation = explainer.explain_instance(observation, lr_predict_proba, num_features=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have generated our explanation, we have access to several representations. The most useful one when working in a notebook is `show_in_notebook`.\n",
    "\n",
    "\n",
    "On the left it shows the list of probabilities for each class, here the model classified our observation as 0 (non subsribed) with a high probability.\n",
    "* If you set `show_table=True`, you will see the table with the most important features for this observation on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:34:45.430414Z",
     "iopub.status.busy": "2021-07-04T12:34:45.430038Z",
     "iopub.status.idle": "2021-07-04T12:34:45.489195Z",
     "shell.execute_reply": "2021-07-04T12:34:45.487604Z",
     "shell.execute_reply.started": "2021-07-04T12:34:45.430342Z"
    }
   },
   "outputs": [],
   "source": [
    "explanation.show_in_notebook(show_table=True, show_all=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also save the explanation to an html file with `save_to_file` to share it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:34:45.51276Z",
     "iopub.status.busy": "2021-07-04T12:34:45.501819Z",
     "iopub.status.idle": "2021-07-04T12:34:45.547452Z",
     "shell.execute_reply": "2021-07-04T12:34:45.545929Z",
     "shell.execute_reply.started": "2021-07-04T12:34:45.512605Z"
    }
   },
   "outputs": [],
   "source": [
    "explanation.save_to_file(\"explanation.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIME is fitting a linear model on a local perturbated dataset. You can access the coefficients, the intercept and the R squared of the linear model by calling respectively `.local_exp`, `.intercept` and `.score` on your explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:34:45.549798Z",
     "iopub.status.busy": "2021-07-04T12:34:45.549243Z",
     "iopub.status.idle": "2021-07-04T12:34:45.555856Z",
     "shell.execute_reply": "2021-07-04T12:34:45.554856Z",
     "shell.execute_reply.started": "2021-07-04T12:34:45.549548Z"
    }
   },
   "outputs": [],
   "source": [
    "print(explanation.local_exp)\n",
    "print(explanation.intercept)\n",
    "print(explanation.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:34:45.557475Z",
     "iopub.status.busy": "2021-07-04T12:34:45.557182Z",
     "iopub.status.idle": "2021-07-04T12:34:45.568704Z",
     "shell.execute_reply": "2021-07-04T12:34:45.567717Z",
     "shell.execute_reply.started": "2021-07-04T12:34:45.55742Z"
    }
   },
   "outputs": [],
   "source": [
    "# dt_predict_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your R-squared is low, the linear model that LIME fitted isn't a great approximation to your model, which means you should not rely too much on the explanation it provides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:34:45.570886Z",
     "iopub.status.busy": "2021-07-04T12:34:45.570275Z",
     "iopub.status.idle": "2021-07-04T12:34:46.42085Z",
     "shell.execute_reply": "2021-07-04T12:34:46.419548Z",
     "shell.execute_reply.started": "2021-07-04T12:34:45.57083Z"
    }
   },
   "outputs": [],
   "source": [
    "explanation = explainer.explain_instance(observation, dt_predict_proba, num_features=5)\n",
    "explanation.show_in_notebook(show_table=True, show_all=False)\n",
    "print(explanation.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:34:46.423041Z",
     "iopub.status.busy": "2021-07-04T12:34:46.422569Z",
     "iopub.status.idle": "2021-07-04T12:34:47.409107Z",
     "shell.execute_reply": "2021-07-04T12:34:47.407883Z",
     "shell.execute_reply.started": "2021-07-04T12:34:46.422827Z"
    }
   },
   "outputs": [],
   "source": [
    "explanation = explainer.explain_instance(observation, rf_predict_proba, num_features=5)\n",
    "explanation.show_in_notebook(show_table=True, show_all=False)\n",
    "print(explanation.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:34:47.411146Z",
     "iopub.status.busy": "2021-07-04T12:34:47.410674Z",
     "iopub.status.idle": "2021-07-04T12:34:48.312055Z",
     "shell.execute_reply": "2021-07-04T12:34:48.311142Z",
     "shell.execute_reply.started": "2021-07-04T12:34:47.410931Z"
    }
   },
   "outputs": [],
   "source": [
    "explanation = explainer.explain_instance(observation, xgb_predict_proba, num_features=5)\n",
    "explanation.show_in_notebook(show_table=True, show_all=False)\n",
    "print(explanation.score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More local interpretation with SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:34:48.314351Z",
     "iopub.status.busy": "2021-07-04T12:34:48.313727Z",
     "iopub.status.idle": "2021-07-04T12:34:49.349187Z",
     "shell.execute_reply": "2021-07-04T12:34:49.347984Z",
     "shell.execute_reply.started": "2021-07-04T12:34:48.314289Z"
    }
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "# Need to load JS vis in the notebook\n",
    "shap.initjs() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHAP has a generic explainer that works for any model and a TreeExplainer optimised for tree based models. Here we will focus on the `TreeExplainer` with our XGB model (the hardest to intepret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(xgb_model.named_steps[\"model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compute the shapley values with the tree explainer, we need to call the `shap_values` methods passing a dataset. That can be quite computationally expensive, so we will only pass 1000 samples picked at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = xgb_model.named_steps[\"preprocessor\"].transform(X_train.sample(1000, random_state=42))\n",
    "shap_values = explainer.shap_values(observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start visualising our explanations using the `force_plot` function from the shap package passing our first shap_value (we also need to pass `explainer.expected_value` which is the base value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "shap.force_plot(explainer.expected_value, shap_values[i], \n",
    "                features=observations[i], feature_names=all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This explanation shows how each feature contributes to shifting the prediction from the base value to the output value of the model either by decreasing or increasing the probability of our class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualise all points in our dataset at once with a given class by passing all explanations for that class to `force_plot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value, shap_values,\n",
    "                features=observations, feature_names=all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see our 1000 samples on the x axis. The y-axis corresponds to the same scale we were looking at before, where blue values corresponds to the probability decreasing, red increasing. Hover with your mouse on a point to see the main features impacting a given observation. You can also use the drop down on the left to visualise the impact of specific features, for example duration only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting plot that we can generate with SHAP is the `summary_plot`, it can be seen as a feature importance plot with more meaningful insights. Below we're plotting the summary plot for class 1 on the whole subset.\n",
    "The colour corresponds to the value of the feature and the x axis corresponds to the SHAP value, meaning the impact on the probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, features=observations, feature_names=all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better than the built-in feature importance on RandomForest because not only we can see what features are important but also how they affect our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"nr.employed\", shap_values, \n",
    "                     pd.DataFrame(observations, columns=all_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intepreting models with non tabular data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tools we have seen above also work with text data and images. There are plenty of examples available online for text-data. Here we will just demonstrate how to use `Lime` to explain an image classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting image classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lime can also be used to explain decisions made for image classification. \n",
    "\n",
    "In this example we will use the pretrained `InceptionV3` model available with Keras. Lime is quite slow with images, so it's wiser to stick to a \"shallow\" deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input, decode_predictions\n",
    "from keras.preprocessing.image import load_img, img_to_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new instance of InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InceptionV3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll load a picture of a toucan, we need to make sure we load it at the good size for inception, here 229*229"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_raw = load_img(\"../input/toucan-for-xai/toucan.jpeg\", target_size=(299,299))\n",
    "image_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to process the image to get a numpy array compatible with our model. Here we simply loads it to an array, reshape it and use the preprocess_input method provided by Keras that ensures all the preprocessing steps are made for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy array, reshape and preprocess\n",
    "image = img_to_array(image_raw)\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "image = preprocess_input(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our image is ready, generate predictions by using `.predict` as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check what labels your predictions correspond to by calling the function `decode_predictions` on your predictions. By default it returns the 5 more likely predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_predictions(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we predicted a toucan with a probability of 99%, that's promising!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that LIME needs the indices of the class we are interested in. Execute the cell bellow to get the indices corresponding to the 5 most probably classes we predicted above. Those indices correspond to the classes used in the ImageNet dataset that was used to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(image).argsort()[0, -5:][::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the toucan corresponds to index 96, the school bus to index 779, etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started. First import the `LimeImageExplainer` and instantiate a new explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_image import LimeImageExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = LimeImageExplainer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The explainer is the same as before, we call `explain_instance` to generate a new explanation. We need to provide:\n",
    "- our observation: here the first row of our numpy matrix (that has only one row since we only have one image)\n",
    "- our predict function, we can simply use the one from our model here\n",
    "- `top_labels` the number of classes to explain. Here our model generate probabilities for more than a 1000 classes (and we looked at the five first). We do not want LIME to generate local models to explain each of those classes. As lime is pretty slow with images, let's only ask for the explanation to our two main classes, toucan and school bus\n",
    "- `num_samples`: the number of new datapoints to create to fit a linear model, let's set it to 1000\n",
    "\n",
    "*WARNING*: that will be slow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation = explainer.explain_instance(image[0], model.predict, \n",
    "                                         top_labels=2, num_samples=100,\n",
    "                                         random_seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.segmentation import mark_boundaries\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's check the explanation for the predicted class `toucan`. That corresponds to label 96 in the ImageNet classes. We need to use the method `get_image_and_mask` on our explanation object with the following parameters:\n",
    "- index of the class to explain. We'll start with the index of the main class predicted, that was 96\n",
    "- positive_only: in order to show the part of the image that contribute positively to this class being selected\n",
    "- num_features: number of superpixels to use. LIME breaks down our image into a set of superpixels, each containing several pixels. Those superpixels are equivalent to `features` in tabular data.\n",
    "- hide_rest: to hide the rest of the image\n",
    "\n",
    "That returns a new image and a mask as numpy arrays. You can then use `mark_boundaries` to show the image together with the mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp, mask = explanation.get_image_and_mask(96, positive_only=True, num_features=5, hide_rest=True)\n",
    "# plot image and mask together\n",
    "plt.imshow(mark_boundaries(temp / 2 + 0.5, mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What feature do you expect to be the most important in that decision? Plot the image with only the main feature (`num_features=1`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp, mask = explanation.get_image_and_mask(96, positive_only=True, num_features=1, hide_rest=True)\n",
    "plt.imshow(mark_boundaries(temp / 2 + 0.5, mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second class predicted by our model was a bus (label 779), set `positive_only=False` in order to see what features contributed positively and negatively to that decision. What do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp, mask = explanation.get_image_and_mask(779, positive_only=False, num_features=8, hide_rest=True)\n",
    "plt.imshow(mark_boundaries(temp / 2 + 0.5, mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now you can try to change the number of features you're looking at and deactivate `positive_only` in order to see features that contribute negatively to the class. You can also look at other classes or try other pictures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>References</h1>\n",
    "\n",
    "**Open the Black Box: an Introduction to Model Interpretability with LIME and SHAP - Kevin Lemagnen\n",
    "\n",
    "Source : https://github.com/klemag/pydata_nyc2018-intro-to-model-interpretability\n",
    "Thanks Kevin!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This DataFrame is created to stock differents models and fair metrics that we produce in this notebook\n",
    "algo_metrics = pd.DataFrame(columns=['model', 'fair_metrics', 'prediction', 'probs'])\n",
    "\n",
    "def add_to_df_algo_metrics(algo_metrics, model, fair_metrics, preds, probs, name):\n",
    "    return algo_metrics.append(pd.DataFrame(data=[[model, fair_metrics, preds, probs]], columns=['model', 'fair_metrics', 'prediction', 'probs'], index=[name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fair_metrics(dataset, pred, pred_is_dataset=False):\n",
    "    if pred_is_dataset:\n",
    "        dataset_pred = pred\n",
    "    else:\n",
    "        dataset_pred = dataset.copy()\n",
    "        dataset_pred.labels = pred\n",
    "    \n",
    "    cols = ['statistical_parity_difference', 'equal_opportunity_difference', 'average_abs_odds_difference',  'disparate_impact', 'theil_index']\n",
    "    obj_fairness = [[0,0,0,1,0]]\n",
    "    \n",
    "    fair_metrics = pd.DataFrame(data=obj_fairness, index=['objective'], columns=cols)\n",
    "    \n",
    "    for attr in dataset_pred.protected_attribute_names:\n",
    "        idx = dataset_pred.protected_attribute_names.index(attr)\n",
    "        privileged_groups =  [{attr:dataset_pred.privileged_protected_attributes[idx][0]}] \n",
    "        unprivileged_groups = [{attr:dataset_pred.unprivileged_protected_attributes[idx][0]}] \n",
    "        \n",
    "        classified_metric = ClassificationMetric(dataset, \n",
    "                                                     dataset_pred,\n",
    "                                                     unprivileged_groups=unprivileged_groups,\n",
    "                                                     privileged_groups=privileged_groups)\n",
    "\n",
    "        metric_pred = BinaryLabelDatasetMetric(dataset_pred,\n",
    "                                                     unprivileged_groups=unprivileged_groups,\n",
    "                                                     privileged_groups=privileged_groups)\n",
    "\n",
    "        acc = classified_metric.accuracy()\n",
    "\n",
    "        row = pd.DataFrame([[metric_pred.mean_difference(),\n",
    "                                classified_metric.equal_opportunity_difference(),\n",
    "                                classified_metric.average_abs_odds_difference(),\n",
    "                                metric_pred.disparate_impact(),\n",
    "                                classified_metric.theil_index()]],\n",
    "                           columns  = cols,\n",
    "                           index = [attr]\n",
    "                          )\n",
    "        fair_metrics = fair_metrics.append(row)    \n",
    "    \n",
    "    fair_metrics = fair_metrics.replace([-np.inf, np.inf], 2)\n",
    "        \n",
    "    return fair_metrics\n",
    "\n",
    "def plot_fair_metrics(fair_metrics):\n",
    "    fig, ax = plt.subplots(figsize=(20,4), ncols=5, nrows=1)\n",
    "\n",
    "    plt.subplots_adjust(\n",
    "        left    =  0.125, \n",
    "        bottom  =  0.1, \n",
    "        right   =  0.9, \n",
    "        top     =  0.9, \n",
    "        wspace  =  .5, \n",
    "        hspace  =  1.1\n",
    "    )\n",
    "\n",
    "    y_title_margin = 1.2\n",
    "\n",
    "    plt.suptitle(\"Fairness metrics\", y = 1.09, fontsize=20)\n",
    "    sns.set(style=\"dark\")\n",
    "\n",
    "    cols = fair_metrics.columns.values\n",
    "    obj = fair_metrics.loc['objective']\n",
    "    size_rect = [0.2,0.2,0.2,0.4,0.25]\n",
    "    rect = [-0.1,-0.1,-0.1,0.8,0]\n",
    "    bottom = [-1,-1,-1,0,0]\n",
    "    top = [1,1,1,2,1]\n",
    "    bound = [[-0.1,0.1],[-0.1,0.1],[-0.1,0.1],[0.8,1.2],[0,0.25]]\n",
    "\n",
    "    display(Markdown(\"### Check bias metrics :\"))\n",
    "    display(Markdown(\"A model can be considered bias if just one of these five metrics show that this model is biased.\"))\n",
    "    for attr in fair_metrics.index[1:len(fair_metrics)].values:\n",
    "        display(Markdown(\"#### For the %s attribute :\"%attr))\n",
    "        check = [bound[i][0] < fair_metrics.loc[attr][i] < bound[i][1] for i in range(0,5)]\n",
    "        display(Markdown(\"With default thresholds, bias against unprivileged group detected in **%d** out of 5 metrics\"%(5 - sum(check))))\n",
    "\n",
    "    for i in range(0,5):\n",
    "        plt.subplot(1, 5, i+1)\n",
    "        ax = sns.barplot(x=fair_metrics.index[1:len(fair_metrics)], y=fair_metrics.iloc[1:len(fair_metrics)][cols[i]])\n",
    "        \n",
    "        for j in range(0,len(fair_metrics)-1):\n",
    "            a, val = ax.patches[j], fair_metrics.iloc[j+1][cols[i]]\n",
    "            marg = -0.2 if val < 0 else 0.1\n",
    "            ax.text(a.get_x()+a.get_width()/5, a.get_y()+a.get_height()+marg, round(val, 3), fontsize=15,color='black')\n",
    "\n",
    "        plt.ylim(bottom[i], top[i])\n",
    "        plt.setp(ax.patches, linewidth=0)\n",
    "        ax.add_patch(patches.Rectangle((-5,rect[i]), 10, size_rect[i], alpha=0.3, facecolor=\"green\", linewidth=1, linestyle='solid'))\n",
    "        plt.axhline(obj[i], color='black', alpha=0.3)\n",
    "        plt.title(cols[i])\n",
    "        ax.set_ylabel('')    \n",
    "        ax.set_xlabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fair_metrics_and_plot(data, model, plot=False, model_aif=False):\n",
    "    pred = model.predict(data).labels if model_aif else model.predict(data.features)\n",
    "    # fair_metrics function available in the metrics.py file\n",
    "    fair = fair_metrics(data, pred)\n",
    "\n",
    "    if plot:\n",
    "        # plot_fair_metrics function available in the visualisations.py file\n",
    "        # The visualisation of this function is inspired by the dashboard on the demo of IBM aif360 \n",
    "        plot_fair_metrics(fair)\n",
    "        display(fair)\n",
    "    \n",
    "    return fair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>housemaid</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.4y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.6y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41183</th>\n",
       "      <td>73</td>\n",
       "      <td>retired</td>\n",
       "      <td>married</td>\n",
       "      <td>professional.course</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>nov</td>\n",
       "      <td>fri</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>94.767</td>\n",
       "      <td>-50.8</td>\n",
       "      <td>1.028</td>\n",
       "      <td>4963.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41184</th>\n",
       "      <td>46</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>professional.course</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>nov</td>\n",
       "      <td>fri</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>94.767</td>\n",
       "      <td>-50.8</td>\n",
       "      <td>1.028</td>\n",
       "      <td>4963.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41185</th>\n",
       "      <td>56</td>\n",
       "      <td>retired</td>\n",
       "      <td>married</td>\n",
       "      <td>university.degree</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>nov</td>\n",
       "      <td>fri</td>\n",
       "      <td>2</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>94.767</td>\n",
       "      <td>-50.8</td>\n",
       "      <td>1.028</td>\n",
       "      <td>4963.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41186</th>\n",
       "      <td>44</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>professional.course</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>nov</td>\n",
       "      <td>fri</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>94.767</td>\n",
       "      <td>-50.8</td>\n",
       "      <td>1.028</td>\n",
       "      <td>4963.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41187</th>\n",
       "      <td>74</td>\n",
       "      <td>retired</td>\n",
       "      <td>married</td>\n",
       "      <td>professional.course</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>nov</td>\n",
       "      <td>fri</td>\n",
       "      <td>3</td>\n",
       "      <td>999</td>\n",
       "      <td>1</td>\n",
       "      <td>failure</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>94.767</td>\n",
       "      <td>-50.8</td>\n",
       "      <td>1.028</td>\n",
       "      <td>4963.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41188 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age          job  marital            education  default housing loan  \\\n",
       "0       56    housemaid  married             basic.4y       no      no   no   \n",
       "1       57     services  married          high.school  unknown      no   no   \n",
       "2       37     services  married          high.school       no     yes   no   \n",
       "3       40       admin.  married             basic.6y       no      no   no   \n",
       "4       56     services  married          high.school       no      no  yes   \n",
       "...    ...          ...      ...                  ...      ...     ...  ...   \n",
       "41183   73      retired  married  professional.course       no     yes   no   \n",
       "41184   46  blue-collar  married  professional.course       no      no   no   \n",
       "41185   56      retired  married    university.degree       no     yes   no   \n",
       "41186   44   technician  married  professional.course       no      no   no   \n",
       "41187   74      retired  married  professional.course       no     yes   no   \n",
       "\n",
       "         contact month day_of_week  campaign  pdays  previous     poutcome  \\\n",
       "0      telephone   may         mon         1    999         0  nonexistent   \n",
       "1      telephone   may         mon         1    999         0  nonexistent   \n",
       "2      telephone   may         mon         1    999         0  nonexistent   \n",
       "3      telephone   may         mon         1    999         0  nonexistent   \n",
       "4      telephone   may         mon         1    999         0  nonexistent   \n",
       "...          ...   ...         ...       ...    ...       ...          ...   \n",
       "41183   cellular   nov         fri         1    999         0  nonexistent   \n",
       "41184   cellular   nov         fri         1    999         0  nonexistent   \n",
       "41185   cellular   nov         fri         2    999         0  nonexistent   \n",
       "41186   cellular   nov         fri         1    999         0  nonexistent   \n",
       "41187   cellular   nov         fri         3    999         1      failure   \n",
       "\n",
       "       emp.var.rate  cons.price.idx  cons.conf.idx  euribor3m  nr.employed  y  \n",
       "0               1.1          93.994          -36.4      4.857       5191.0  0  \n",
       "1               1.1          93.994          -36.4      4.857       5191.0  0  \n",
       "2               1.1          93.994          -36.4      4.857       5191.0  0  \n",
       "3               1.1          93.994          -36.4      4.857       5191.0  0  \n",
       "4               1.1          93.994          -36.4      4.857       5191.0  0  \n",
       "...             ...             ...            ...        ...          ... ..  \n",
       "41183          -1.1          94.767          -50.8      1.028       4963.6  1  \n",
       "41184          -1.1          94.767          -50.8      1.028       4963.6  0  \n",
       "41185          -1.1          94.767          -50.8      1.028       4963.6  0  \n",
       "41186          -1.1          94.767          -50.8      1.028       4963.6  1  \n",
       "41187          -1.1          94.767          -50.8      1.028       4963.6  0  \n",
       "\n",
       "[41188 rows x 20 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['y'] = y\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X)\n",
    "\n",
    "\n",
    "#combine_final = [train_df, test_df]\n",
    "#result = pd.concat(combine_final)\n",
    "#print(result.ifany())\n",
    "#print(result)\n",
    "privileged_groups = [{'age': 1}]\n",
    "unprivileged_groups = [{'age': 0}]\n",
    "dataset_orig = StandardDataset(bank_final,\n",
    "                                  label_name='y',\n",
    "                                  protected_attribute_names=['age'],\n",
    "                                  favorable_classes=[1],\n",
    "                                  privileged_classes=[[1]])\n",
    "\n",
    "#metric_orig_train = BinaryLabelDatasetMetric(dataset_orig, \n",
    "#                                             unprivileged_groups=unprivileged_groups,\n",
    "#                                             privileged_groups=privileged_groups)\n",
    "#display(Markdown(\"#### Original training dataset\"))\n",
    "#print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
