{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 id=\"tocheading\">Table of Contents</h1>\n<div id=\"toc\"></div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%javascript\n$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport pandas as pd\n\nfrom sklearn.preprocessing import OneHotEncoder,OrdinalEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# algorithms, ranging from easiest to the hardest to intepret.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost.sklearn import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This notebook is to learn about the intepretable ML models like SHAP and LIME. The data set used is the Bank marketing UCI dataset. \nThe reference to this project is the PyData NY conference 2018([Open the Black Box: an Introduction to Model Interpretability with LIME and SHAP - Kevin Lemagnen](https://www.youtube.com/watch?v=C80SQe16Rao&ab_channel=PyData))"},{"metadata":{},"cell_type":"markdown","source":"- Models are opinions embedded in mathematics, if there is a bias in your dataset then it will flow into the model\n- Classify images - Wolf and Husky dog- The model worked well, the problem is that the model looked at the snow and then identified Husky. So basically a snow detector!! "},{"metadata":{},"cell_type":"markdown","source":"Why to intepret the models:\n* how the decisions are made?\n* convert to white box models\n* Reduce the bias in the model data\n* harder models like ensembles, boosting, Deep NNs are difficult to interpret\n* if the feature is effecting positively or negatively?"},{"metadata":{},"cell_type":"markdown","source":"We are going to look at the following methods:\n- ELI5\n- SHAP\n- LIME"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/bank-marketing/bank-additional-full.csv', sep=';')\ndata.head()\n# Output variable (desired target):if the client subscribed a term deposit? (binary: \"yes\",\"no\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets check the target variable to see if it is balanced or not\ndata['y'].value_counts()\n# the dataset is imbalanced","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets built the attribute set and the target data set\ndata_y= data['y'].map({'yes':1,'no':0})\ndata_X= data.drop('y',axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Attribute information:**\n*Input variables:*\n\n*bank client data:*\n\n1 - age (numeric)\n\n2 - job : type of job (categorical: \"admin.\",\"unknown\",\"unemployed\",\"management\",\"housemaid\",\"entrepreneur\",\"student\", \"blue-collar\",\"self-employed\",\"retired\",\"technician\",\"services\")\n\n3 - marital : marital status (categorical: \"married\",\"divorced\",\"single\"; note: \"divorced\" means divorced or widowed)\n\n4 - education (categorical: \"unknown\",\"secondary\",\"primary\",\"tertiary\")\n\n5 - default: has credit in default? (binary: \"yes\",\"no\")\n\n6 - balance: average yearly balance, in euros (numeric)\n\n7 - housing: has housing loan? (binary: \"yes\",\"no\")\n\n8 - loan: has personal loan? (binary: \"yes\",\"no\")\n\n*related with the last contact of the current campaign:*\n9 - contact: contact communication type (categorical: \"unknown\",\"telephone\",\"cellular\")\n\n10 - day: last contact day of the month (numeric)\n\n11 - month: last contact month of year (categorical: \"jan\", \"feb\", \"mar\", ..., \"nov\", \"dec\")\n\n12 - duration: last contact duration, in seconds (numeric)\n\nother attributes:\n13 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n\n14 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric, -1 means client was not previously contacted)\n\n15 - previous: number of contacts performed before this campaign and for this client (numeric)\n\n16 - poutcome: outcome of the previous marketing campaign (categorical: \"unknown\",\"other\",\"failure\",\"success\")"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets check if there is a case of missing data in the columns\n#check for the missing values in the train data\ndata_X.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_X.drop('duration',inplace=True,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_X.dtypes # to look at the data types of the features\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_features_cat =  data_X.dtypes[data_X.dtypes == 'object'].index.to_list()\ndata_features_num =  (data_X.dtypes[data_X.dtypes == 'int64'].index |  data_X.dtypes[data_X.dtypes == 'float64'].index).to_list()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('categorical features are: \\n', data_features_cat)\nprint('----'*25)\nprint ('numerical features are: \\n', data_features_num)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* To process these two lists seperately, we can use  a column transformer for that we have to pass in the respective variables as Tuples\n* Each transformer is a three-element tuple that defines the name of the transformer, the transform to apply, and the column indices to \n  apply it to. For example: (Name, Object, Columns)\n*  https://machinelearningmastery.com/columntransformer-for-numerical-and-categorical-data/"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\npreprocessor = ColumnTransformer(transformers=[('numerical','passthrough',data_features_num),('categorical',OneHotEncoder(sparse=False,handle_unknown='ignore'),\n                                   data_features_cat)])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The classification models that we are going to try out are:\n# The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to \n# class frequencies in the input data as n_samples / (n_classes * np.bincount(y))\n\n# 1. Logistic regression\nLGmodel =  LogisticRegression(class_weight='balanced',solver='liblinear',random_state=42)\n\n# 2.Decision tree\nDTmodel = DecisionTreeClassifier(class_weight='balanced')\n\n# 3.Random forest\nRFmodel = RandomForestClassifier(class_weight='balanced',n_estimators=100, n_jobs=-1)\n\n# 4.XG Boost\nXGBmodel =  XGBClassifier(scale_pos_weight=(1 - data_y.mean()), n_jobs=-1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Lets built a pipeline so that we can chain the steps sequentially\n* The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters\n* Gridsearch, CVs etc are all possible on a pipeline\n* Paras → List of (name, transform) tuples (implementing fit/transform) that are chained, in the order in which they are chained, with the last    object an estimator.\n* https://machinelearningmastery.com/machine-learning-modeling-pipelines/"},{"metadata":{"trusted":true},"cell_type":"code","source":"# defining the model pipelines\n\nlog_reg_model = Pipeline([('preprocessor',preprocessor),('model',LGmodel)])\ndt_model = Pipeline([('preprocessor',preprocessor),('model',DTmodel)])\nrf_model = Pipeline([('preprocessor',preprocessor),('model',RFmodel)])\nxgb_model = Pipeline([('preprocessor',preprocessor),('model',XGBmodel)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train Test Data split"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test= train_test_split(data_X,data_y,stratify=data_y,test_size=0.3,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ELI5\n- useful to debug scikit learn models\n- provides global interpretation of white box models\n- show feature importances and explain predictions\n-  Two functions - show weights and show observations"},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit the log reg model first \n# Grid search is an approach to parameter tuning that will methodically build and evaluate a model for each combination of \n# algorithm parameters specified in a grid.\ngs_logreg = GridSearchCV(estimator=log_reg_model, param_grid={\"model__C\": [1,1.2,1.2, 1.3, 1.4, 1.5]}, n_jobs=-1, cv=5, scoring=\"accuracy\")\ngs_logreg.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the best parameters and best score\nprint(gs_logreg.best_params_)\nprint(gs_logreg.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_reg_model.set_params(**gs_logreg.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_reg_model.get_params('model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit the best values on the training set again\nlog_reg_model.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now generate the predictions\ny_pred = log_reg_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,y_pred))\n# bad recall and precision for the minority class","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's use eli5 to visualise the weights associated to each feature:\n\n\nimport eli5\neli5.show_weights(log_reg_model.named_steps[\"model\"]) # going into the pipeline and getting the model\n# can't get much enough from this as the features are encoded","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we need to get back the name of the features from the post one hot encoding\n# so the steps are go to the model → preprocessor → categorical feats\n\npreprocessor = log_reg_model.named_steps['preprocessor'] # to get the preprocessor from the model\nohe_feats = preprocessor.named_transformers_['categorical'].categories_ # to get the categories from the transformation inside preprocessor\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data_features_cat)\nprint('****'*25)\nprint(ohe_feats)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#build a list of featurs by combining the cat_feats with the sub categories\nnew_ohe_feats =  [f'{col}_{val}' for col,vals in zip(data_features_cat,ohe_feats) for val in vals]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(new_ohe_feats)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_all_features = data_features_num + new_ohe_feats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(log_reg_model.named_steps[\"preprocessor\"].transform(X_train), columns=data_all_features).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eli5.show_weights(log_reg_model.named_steps['model'],feature_names = data_all_features)\n# Notes\n# whether campaign happened in march ? ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# show observations\n# Take row 4 from the data\ntest_row = 4\nX_test.iloc[[test_row]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ny_test.iloc[[test_row]] # =1 was able to get it","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eli5.show_prediction(log_reg_model.named_steps['model'],\n                     log_reg_model.named_steps['preprocessor'].transform(X_test)[test_row],\n                     feature_names=data_all_features,show_feature_values = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can see that the modt imp feature that eli5 looked at consumer price index, but that feature is not linked to client but to the company\n# it means it is not a good model\n# again the feature that negatively effecting the model is the no: employees, again dependent on the company not the client!","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision trees"},{"metadata":{"trusted":true},"cell_type":"code","source":"gs_dt =GridSearchCV(dt_model, {\"model__max_depth\": [3, 5, 7], \n                             \"model__min_samples_split\": [2, 5]},n_jobs=1,cv=5,scoring='accuracy')\ngs_dt.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import set_config\n\nset_config(display='diagram')\ngs_dt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To check the parameters in the pipeline\n#sorted(dt_model.get_params().keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(gs_dt.best_params_)\nprint(gs_dt.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_model.set_params(**gs_dt.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_model.fit(X_train,y_train)\ny_pred = dt_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,y_pred)) # still not a good model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eli5.show_weights(dt_model.named_steps['model'],feature_names= data_all_features)\n#its picking on the no:employees again!! Not a good model!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eli5.show_prediction(dt_model.named_steps['model'],\n                     dt_model.named_steps['preprocessor'].transform(X_test)[test_row],\n                     feature_names = data_all_features, show_feature_values = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SHAP"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"## Random forests\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"gs_rf =GridSearchCV(rf_model, {\"model__max_depth\": [10,12,15], \n                             \"model__min_samples_split\": [2,7,10]},n_jobs=1,cv=5,scoring='accuracy')\ngs_rf.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(gs_rf.best_params_)\nprint(gs_rf.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_model.set_params(**gs_rf.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_model.fit(X_train,y_train)\ny_pred= rf_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# look the features with eli5 first of all\neli5.show_weights(rf_model.named_steps[\"model\"], \n                  feature_names=data_all_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gradient Boosting "},{"metadata":{"trusted":true},"cell_type":"code","source":"gs_xb = GridSearchCV(xgb_model, {\"model__max_depth\": [5, 10],\n                              \"model__min_child_weight\": [5, 10],\n                              \"model__n_estimators\": [25]},\n                  n_jobs=-1, cv=5, scoring=\"accuracy\")\n\ngs_xb.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit model and create Predictions\n\nprint(gs_xb.best_params_)\nprint(gs_xb.best_score_)\nxgb_model.set_params(**gs_xb.best_params_)\nxgb_model.fit(X_train, y_train)\n\n#predictions\ny_pred = xgb_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check accuracy and classification report\nprint('Accuracy is :',accuracy_score(y_test,y_pred))\nprint('--'*25)\nprint(classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SHAP"},{"metadata":{},"cell_type":"markdown","source":"- Explanation model is a simpler model that is agood approximation of a complex model\n- Local explanation is a linear combination of the features - using shapely values from game theory\n- How a feature is impacting a decision when it is added to the subset\n- Tree explainer (for tree based models) and kernelexplainer for other models\n"},{"metadata":{},"cell_type":"markdown","source":"**Steps**\n- Create a new explainer\n- Calculate shap values\n- Use visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"import shap\nshap.initjs()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"explainer =shap.TreeExplainer(xgb_model.named_steps['model'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"observations = xgb_model.named_steps['preprocessor'].transform(X_train.sample(1000,random_state=42))\nshap_values= explainer.shap_values(observations)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualizations\nshap_i = 0\nshap.force_plot(explainer.expected_value,shap_values[shap_i],features=observations[shap_i],feature_names=data_all_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Inferences**\n- how diff features influence my decision\n- red features pushes the features to 1 and blue push the decions to class 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.force_plot(explainer.expected_value, shap_values,\n                features=observations, feature_names=data_all_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Here we look at all observations at once\n- can look at individual features in particular"},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.summary_plot(shap_values, features=observations, feature_names=data_all_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- gloabal explanation of the model\n- sorted on the order of importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.dependence_plot(\"nr.employed\", shap_values, \n                     pd.DataFrame(observations, columns=data_all_features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}