{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"**OPEN THE BLACK BOX - INTRODUCTION TO MODEL INTERPRETABILITY**\n> Why do we need it??\n> Lets say you have done your job as building the pipeline\n1. Cleaned and processed messy data\n2. Engineered Fancy new features\n3. Selected the best model and tuned parametres\n4. Trained Your Final Model\n5. Got Great Performance On the Test Set\n> And Suddenly Your Boss or Any Non-Technical Guy asks -`Can you explain How Your Model Works??`\n> In this situation we should understand the importance of model interpretability->\n`Algorithms are everywhere,sometime automating important decisions that have an impact on people`\n   *   *Insurance* - model to predict the best price to charge the client\n   *    *Bank* - model to predict who should get loan or not\n   *    *Police* - model to predict who is most likely to buy a product\n   `In these types of situations we need to understand the underlying principle decisions that are taken by the algorithm to predict an outcome`\n   \n   \n    It is  also helpful to capture bias in the data, for example:\n    ->Predict employees' performance at a big company\n   Data Available\n   *past performance reviews of indiviual employees for the last 10 years*\n    \n    'What if the company tends to promote men more than women?'\n    The model will learn the bias, and predict that men are more likely to be performant'\n    \n   In these situations we need to understand the model behaviour and decisions that it is taking to predict and \n   \n   So to interpret models we have several packages made available to us-\n  >[ELI5](https://github.com/TeamHG-Memex/eli5) -`Useful to debug skelearn-models and communicate with domain experts(usually used for white-box  models)`\n \n   > [LIME](https://github.com/marcotcr/lime)-`Explains why a single datapoint was classified as a specific class(used for black box algorithms)`\n  \n  > [SHAP](https://github.com/slundberg/shap)-`Tree explainer(only for tree based models-used with scikit-learn,xgboost,lightgbm,catboost) and kernel explainer-(Model Agnostic explainer)`\n  \n  \n  \n \n `SO LETS GET STARTED` \n\n \n      \n\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"<a id=\"contents\"></a>\n # Contents\n \n1.[Import the necessary Libraries](#imports)<br>\n \n2.[Load dataset and process it](#dataloading)<br>\n\n3.[ELI5 to interpret white box models](#eli)<br>\n  > 3.a)[With a Logistic Regression](#e)<br>\n  3.b)[With a Decision Classifier](#d)<br>\n     \n4.[Black Box Model Interpretation->>Random Forests](#rand)<br>\n  > 4.a)[Confidence based on Tree Variance](#int)<br>\n   4.b)[Permutation Importance(Why not to trust scikit learns' feature importance)](#it)<br>\n   4.c)[Removing Redundant Features](#itc )<br>\n   4.d)[Partial Dependence Plots](#ts)<br>\n   4.e) [WaterFall Models(Useful in buisness case situations)](#interpret results)<br>\n   \n5.[Black Box Model Local Interpretation with Shap ->>Tree based models(Xgboost,Catboost,LightGbm)](#interpret results)<br>\n\n6.[Interpreting models with Non -Tabular Data](#interpret results)<br>\n    \n\n\n\n\n\n\n\n\n\n\n"},{"metadata":{"_uuid":"dfeb9f52ed9a248c4376fa2aadc7855a35b1f869"},"cell_type":"markdown","source":"<a id='imports'></a>\n## 1. Import the required libraries"},{"metadata":{"trusted":true,"_uuid":"f1b07ca9419034860f15839504db997a667e5a45"},"cell_type":"code","source":"# Obviously\n%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy \n# Some sklearn tools for preprocessing and building a pipeline. \n# ColumnTransformer was introduced in 0.20 so make sure you have this version\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Our algorithms, by from the easiest to the hardest to intepret.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost.sklearn import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7582c2c81108f5ce02a5d26ea78b3198e0249306"},"cell_type":"markdown","source":"<a id='dataloading'></a>\n## 2. Load dataset and process it\n  >The dataset is from the UCI machine learning repo containing information about the  marketing campaigns of a Portuguese bank. We will try to build classifiers that can predict whether or not the client targeted   by the campaign ended up subscribing to a term deposit (column y)"},{"metadata":{"trusted":true,"_uuid":"8605a33466215bb6bd1ac6583e76a2c236237f60"},"cell_type":"code","source":"df = pd.read_csv('../input/bank-additional-full.csv',sep=\";\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"46494848fb565a6662f1e7aaa0d3bdeb24869e97"},"cell_type":"code","source":"df.y.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79ce3ba8533d64ea880d07d954de7b8903bb22e8"},"cell_type":"markdown","source":"> The dataset is imbalanced, we will need to keep that in mind when building our models!"},{"metadata":{"trusted":true,"_uuid":"d1f8773bdd4781abedc2fb39055ddf86060d25ef"},"cell_type":"code","source":"# Get X, y\ny = df[\"y\"].map({\"no\":0, \"yes\":1})\nX = df.drop(\"y\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"081bbc9dc46b83b8fcbbaffef0eadc6a398c239f"},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad0b778f748b31d61171bcbb12ad6b844a2d5bd4"},"cell_type":"markdown","source":"Let's look at the features in the X matrix:\n\n>age (numeric)\n\n>job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n\n>marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n\n>education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n\n>default: has credit in default? (categorical: 'no','yes','unknown')\n\n>housing: has housing loan? (categorical: 'no','yes','unknown')\n\n>loan: has personal loan? (categorical: 'no','yes','unknown')\n\n>contact: contact communication type (categorical: 'cellular','telephone') \n\n>month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n\n>day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n\n>duration: last contact duration, in seconds (numeric).\n\n`Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.`\n>campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n\n>pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n>previous: number of contacts performed before this campaign and for this client (numeric)\n\n>poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n\n>emp.var.rate: employment variation rate - quarterly indicator (numeric)\n\n>cons.price.idx: consumer price index - monthly indicator (numeric) \n\n>cons.conf.idx: consumer confidence index - monthly indicator (numeric) \n\n>euribor3m: euribor 3 month rate - daily indicator (numeric)\n\n>nr.employed: number of employees - quarterly indicator (numeric)\n\nNote the comment about duration feature. We will exclude it from our analysis."},{"metadata":{"trusted":true,"_uuid":"7811a56fa9252440d21af85d79c1395d3f76237f"},"cell_type":"code","source":"X.drop(\"duration\", inplace=True, axis=1)\nX.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35208b135e6dfe4ef0c895c6670e2cd1e92f3915"},"cell_type":"code","source":"# Some such as default would be binary features, but since\n# they have a third class \"unknown\" we'll process them as non binary categorical\nnum_features = [\"age\", \"campaign\", \"pdays\", \"previous\", \"emp.var.rate\", \n                \"cons.price.idx\", \"cons.conf.idx\",\"euribor3m\", \"nr.employed\"]\n\ncat_features = [\"job\", \"marital\", \"education\",\"default\", \"housing\", \"loan\",\n                \"contact\", \"month\", \"day_of_week\", \"poutcome\"]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3be427580d0572bec61f0627826457b55e08606a"},"cell_type":"markdown","source":"`We'll define a new ColumnTransformer object that keeps our numerical features and apply one hot encoding on our categorical features. That will allow us to create a clean pipeline that includes both features engineering (one hot encoding here) and training the model (a nice way to avoid data leakage)`"},{"metadata":{"trusted":true,"_uuid":"890aa1a8e43d29d8c49ba49ceefea1718522e559"},"cell_type":"code","source":"preprocessor = ColumnTransformer([(\"numerical\", \"passthrough\", num_features), \n                                  (\"categorical\", OneHotEncoder(sparse=False, handle_unknown=\"ignore\"),\n                                   cat_features)])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66e245c2c19cf99a972f9cb8adcf57c91d5ac716"},"cell_type":"markdown","source":"Now we can define our 2 models as sklearn Pipeline object, containing our preprocessing step and training of one given algorithm.[](http://)"},{"metadata":{"trusted":true,"_uuid":"5dbc358f4be88624c2749ff953c5fea9d5942e0c"},"cell_type":"code","source":"# Logistic Regression\nlr_model = Pipeline([(\"preprocessor\", preprocessor), \n                     (\"model\", LogisticRegression(class_weight=\"balanced\", solver=\"liblinear\", random_state=42))])\n\n# Decision Tree\ndt_model = Pipeline([(\"preprocessor\", preprocessor), \n                     (\"model\", DecisionTreeClassifier(class_weight=\"balanced\"))])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec4d5365a0dfba9d0176c817c73c47d7e48aa914"},"cell_type":"markdown","source":"Let's split the data into training and test sets."},{"metadata":{"trusted":true,"_uuid":"2ba7293aba3e022f931941883a1e319c7d3e40d4"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=.3, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"32eaeabbf925dd683b0cf86b770ea23bb1f06d1f"},"cell_type":"markdown","source":"<a id='eli'></a>\n## 3. ELI5 to interpret white box models\n<a id='e'></a>\n>   **3.a) With Logistic Regression**"},{"metadata":{"_uuid":"074f3916b00fa908ac84db864bd533d099b50fbc"},"cell_type":"markdown","source":"First let's fine tune our logistic regression and evaluate its performance."},{"metadata":{"trusted":true,"_uuid":"b39707708f9982caae84356bd0d2a728a20a3d6f"},"cell_type":"code","source":"gs = GridSearchCV(lr_model, {\"model__C\": [1, 1.3, 1.5]}, n_jobs=-1, cv=5, scoring=\"accuracy\")\ngs.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59e25e40e898733f5013858c0e0c9b6b859953a0"},"cell_type":"markdown","source":"Let's see our best parameters and score"},{"metadata":{"trusted":true,"_uuid":"345986a1ec01c3950e08c00282a4b7128fab72d9"},"cell_type":"code","source":"print(gs.best_params_)\nprint(gs.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2aae0c6c4ef1d9ee2ac68904e23db9c4077d76f"},"cell_type":"code","source":"lr_model.set_params(**gs.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4990c3e9cafbdf4c08784a8a99b0868c38eebe2"},"cell_type":"code","source":"lr_model.get_params(\"model\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bdf1f5a1a9fa919e5c8c89cd0a84484a17e446f3"},"cell_type":"markdown","source":"Now we can fit the model on the whole training set and calculate accuracy on the test set."},{"metadata":{"trusted":true,"_uuid":"842f04071db828101dea29c5ab6de082bdd98be6"},"cell_type":"code","source":"lr_model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db25a963765be373f97628cc389e77aa1da648c9"},"cell_type":"markdown","source":"Generate Predictions"},{"metadata":{"trusted":true,"_uuid":"577fd7fbbeb76cf15067e4fe996ce95d18206e7f"},"cell_type":"code","source":"y_pred = lr_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ebfd9d1ec004a8c034c5d510d68d60d07f7166c5"},"cell_type":"code","source":"accuracy_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f6d3378deadf77d5341e3ec3959bd4f5db58a3d4"},"cell_type":"code","source":"print(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c1bf3bf708ee4b9c2bb9841541574a33726712c"},"cell_type":"markdown","source":"y = 1 being the minority class has lower precision and recall so the accuracy doesnt help us\n>Let's use eli5 to visualise the weights associated to each feature:"},{"metadata":{"trusted":true,"_uuid":"03188eec889da68053bec99ccfa52c4f3757ac17"},"cell_type":"code","source":"import eli5\neli5.show_weights(lr_model.named_steps[\"model\"])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"093d45f090cd8bac60b662ba13c045d2dc909ae9"},"cell_type":"markdown","source":"That gives us the weights associated to each feature, that can be seen as the contribution of each feature into predicting that the class will be y=1 (the client will subscribe after the campaign).\n\nThe names for each features aren't really helping though, we can pass a list of column names to eli5 but we'll need to do a little hard work first to extract names from our preprocessor in the pipeline (since we've generated new features on the fly with the one hot encoder)"},{"metadata":{"trusted":true,"_uuid":"96ce8934a375ae8324432e8c0598c12ced609c1a"},"cell_type":"code","source":"preprocessor = lr_model.named_steps[\"preprocessor\"]\nohe_categories = preprocessor.named_transformers_[\"categorical\"].categories_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5960a2d21ac284d55555a5a4a5de9137c5df543e"},"cell_type":"code","source":"new_ohe_features = [f\"{col}__{val}\" for col, vals in zip(cat_features, ohe_categories) for val in vals]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"efc3ecdf87ff3e34f9cacb8d666f98f6e9715d2c"},"cell_type":"code","source":"all_features = num_features + new_ohe_features\npd.DataFrame(lr_model.named_steps[\"preprocessor\"].transform(X_train), columns=all_features).head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d8ef160bef3d145aed91a6b800cf9f3d04f543b"},"cell_type":"markdown","source":"we have a nice list of columns after processing"},{"metadata":{"trusted":true,"_uuid":"d5d42497820f1b061fdcfb16727765ca4cc15268"},"cell_type":"code","source":"eli5.show_weights(lr_model.named_steps[\"model\"], feature_names=all_features)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"84b4e16ad5aea546c73698a08d78fe9b010f1a03"},"cell_type":"markdown","source":"`Looks like it's picking principally on whether the month is march or not, the marketing campaign seem to have been more efficient in march?`\n`So once we know the important feature the model is spitting out, we can communicate with the domain expert or the marketing head of the campaign and ask questions specifically whether there was something different they had done in the month of march and so on, commmunication is also the key`"},{"metadata":{"_uuid":"c28a1facf4801624074c75d6dbff065156576782"},"cell_type":"markdown","source":"We can also use eli5 to explain a specific prediction, let's pick a row in the test data:"},{"metadata":{"trusted":true,"_uuid":"b40d1cf040e865aa608c892a69e7c1800473c2bb"},"cell_type":"code","source":"i = 4\nX_test.iloc[[i]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b55d859728881ea53cafa0669a0881cbd544f89"},"cell_type":"code","source":"y_test.iloc[i]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33adc0af8253561bdb0f749c2560860fa862fea1"},"cell_type":"markdown","source":">Our client subsribed to the term deposit after the campaign! Let's see what our model would have predicted and how it would explain it."},{"metadata":{"_uuid":"291acaa604a3c217567fd79114773081ebc564e9"},"cell_type":"markdown","source":"We'll need to first transform our row into the format expected by our model as eli5 cannot work directly with our pipeline."},{"metadata":{"trusted":true,"_uuid":"92b12765a532ffb7d5aaceddca3c588b2aba1039"},"cell_type":"code","source":"eli5.show_prediction(lr_model.named_steps[\"model\"], \n                     lr_model.named_steps[\"preprocessor\"].transform(X_test)[i],\n                     feature_names=all_features, show_feature_values=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba8fd87b5746bbaea879a75ef425848a3fca8206"},"cell_type":"markdown","source":"For this particular client ,it has predicted with 0.963 probability that he/she will subscribe, and  most importantly the model is principally\npicking conumer_price_index as one of the important predictors but according to our data dictionary this particular feature is not actually linked \nwith the client but is linked to the company so it  is trying to use as a proxy whatever the company was upto at that time so the model is not looking at\nthe client characteristics to make its decision rather it is depending upon the companys' consumer_index around that time of marketing campaign, \nso you could easily tell that the model is not making decision based on clients characteristics but rather something happening in the company\nSo this model is probably not that great`"},{"metadata":{"_uuid":"cf529e5af2ae2febba61b8f322f584e05217aa29"},"cell_type":"markdown","source":"<a id='d'></a>\n## 3.b) With a Decision Classifier"},{"metadata":{"trusted":true,"_uuid":"a369fd6c70f25c98034ad8bfc9ba91071cdf85d4"},"cell_type":"code","source":"gs = GridSearchCV(dt_model, {\"model__max_depth\": [3, 5, 7], \n                             \"model__min_samples_split\": [2, 5]}, \n                  n_jobs=-1, cv=5, scoring=\"accuracy\")\n\ngs.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6443259c80f36d22eead9b9f46309dc5989b80a2"},"cell_type":"code","source":"print(gs.best_params_)\nprint(gs.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1969647f3f90e47279b120078520b304a787c708"},"cell_type":"code","source":"dt_model.set_params(**gs.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72d37b10e655ad9633571cee99d6708e51c54cee"},"cell_type":"code","source":"dt_model.fit(X_train, y_train)\ny_pred = dt_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe31a334f8554d64e6d642e9cc4341b5ab2941d3"},"cell_type":"code","source":"accuracy_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79c8b89abe6efbca0ae0caa43c188f819345beac"},"cell_type":"markdown","source":"Accuracy improved a bit  with decision classifier"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"bf6566da8dad90826c4b01cffeb6deb74d93f8b0"},"cell_type":"code","source":"print(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65674d810632c8d3bc7532bdd07dffbc1fb0e15d"},"cell_type":"markdown","source":"For Decision Trees, eli5 only gives feature importance, which does not say in what direction a feature impact the predicted outcome."},{"metadata":{"trusted":true,"_uuid":"9b05f980eb28653fe85bc6131122720d2f35fbc9"},"cell_type":"code","source":"eli5.show_weights(dt_model.named_steps[\"model\"], feature_names=all_features)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"507e8da06dc038f40fbbd6084f052e1699a96166"},"cell_type":"markdown","source":"Here the most important feature seems to be nr.employed.(which again is not linked with clients characteristics)-> Not a very great model . We can also get an explanation for a given prediction, this will calculate the contribution of each feature in the prediction:"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"11a75e3d53022614612d7be8ebb298bb8f9a7ef2"},"cell_type":"code","source":"#This is for the same client that we had calculated for logistic regression\n#Local interpretation of a particular row\neli5.show_prediction(dt_model.named_steps[\"model\"], \n                     dt_model.named_steps[\"preprocessor\"].transform(X_test)[i],\n                     feature_names=all_features, show_feature_values=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3ea0edbe636511ad7ed49a0b7cc2648efa86393"},"cell_type":"markdown","source":"Here the explanation for a single prediction is calculated by following the decision path in the tree, and adding up contribution of each feature from each node crossed into the overall probability predicted.\nAgain the model is not so great as it is taking the decision on the basis of companys' characteristics from that time .i.e here it is number of employees , which is pretty vague in predicted whether the customer subscribed or not"},{"metadata":{"_uuid":"95c02cc851849d6f9d61f1efde030dd00a1aa3ef"},"cell_type":"markdown","source":"bbb<a id='rand'></a>\n## 4. Black Box Model Interpretation ->Random Forest"},{"metadata":{"trusted":true,"_uuid":"f1f2bb2744ef30596570cba3fdde8678d511006f"},"cell_type":"code","source":"# Random Forest\nrf_model = Pipeline([(\"preprocessor\", preprocessor), \n                     (\"model\", RandomForestClassifier(class_weight=\"balanced\", n_estimators=100, n_jobs=-1))])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86226bb8e5a3b6a7c250bd223ec0f56293df7e31"},"cell_type":"code","source":"gs = GridSearchCV(rf_model, {\"model__max_depth\": [10, 15], \n                             \"model__min_samples_split\": [5, 10]}, \n                  n_jobs=-1, cv=5, scoring=\"accuracy\")\n\ngs.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9cd8a6ec0005f8071208d53b3f4e7967f044f0b"},"cell_type":"code","source":"print(gs.best_params_)\nprint(gs.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc81540ec699ca92831330bcee1a609f2f4e33c7"},"cell_type":"code","source":"rf_model.set_params(**gs.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a70dbd5ab2140eb94558af3cf3eaa636a8b7514"},"cell_type":"code","source":"rf_model.fit(X_train, y_train)\ny_pred = rf_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67e1d6952cc04cc12898766d1717b32779d15551"},"cell_type":"code","source":"accuracy_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10ebd2a39dd3168ee864f8925f0f4af38df83b50"},"cell_type":"markdown","source":"Accuracy has improved a bit using Random Forests"},{"metadata":{"trusted":true,"_uuid":"25b8718c661a4866ac270a7de8b57244b6612b5c"},"cell_type":"code","source":"print(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8bcecbdad555b1aefeeda5e0b01ce70a94c9480a"},"cell_type":"markdown","source":"<a id='int'></a>\n## 4.1 Confidence Based on Tree Variance "},{"metadata":{"_uuid":"7fa92373e3a8c3867bdaf559d40f318bfdfc96b7"},"cell_type":"markdown","source":"In order to explain why the model classifies invidividual observations as class 0 or 1, we are going to use the `LimeTabularExplainer` from the library lime, this is the main explainer to use for tabular data. Lime also provides an explainer for text data, for images and for time-series.\nWhen using the tabular explainer, we need to provide our training set as parameter so that lime can compute statistics on each feature, either mean and std for numerical features, or frequency of values for `categorical features`. Those statistics are used to scale the data and generate new perturbated data to train our local linear models on."},{"metadata":{"trusted":true,"_uuid":"ff45c729ca7691311ef64fcf2160b9a12825087a"},"cell_type":"code","source":"from lime.lime_tabular import LimeTabularExplainer","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1aa215da4f12fe9708d0b8745d453d1dbb95883f"},"cell_type":"markdown","source":"The parameters passed to the explainer are:\n>1.`our training set`, we need to make sure we use the training set without one hot encoding\n\n>2`mode`: the explainer can be used for classification or regression\n\n>3.`feature_names`: list of labels for our features\n\n>4.`categorical_features`: list of indexes of categorical features\n\n>5.`categorical_names`: dict mapping each index of categorical feature to a list of corresponding labels\n\n>6.`dicretize_continuous`: will discretize numerical values into buckets that can be used for explanation. For instance it can tell us that the decision was made because distance is in bucket [5km, 10km] instead of telling us distance is an importante feature.\n\nFirst, in order to get the categorical_names parameter we need to build a dictionary with indexes of categorical values in original dataset as keys and lists of possible categories as values:"},{"metadata":{"trusted":true,"_uuid":"3704a8e21bd1bbd2f2ad5a84e4410b5b53921d89"},"cell_type":"code","source":"categorical_names = {}\nfor col in cat_features:\n    categorical_names[X_train.columns.get_loc(col)] = [new_col.split(\"__\")[1] \n                                                       for new_col in new_ohe_features \n                                                       if new_col.split(\"__\")[0] == col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"57b352c82ea8236f9f5e69b3d1a1a946c44dad7a"},"cell_type":"code","source":"categorical_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"31fb692249af7822b50bf9767363524d8e8d642b"},"cell_type":"code","source":"import pandas as pd\ndef convert_to_lime_format(X, categorical_names, col_names=None, invert=False):\n\n    \"\"\"Converts data with categorical values as string into the right format \n     for LIME, with categorical values as integers labels.\n    It takes categorical_names, the same dictionary that has to be passed\n    to LIME to ensure consistency. \n    col_names and invert allow to rebuild the original dataFrame from\n    a numpy array in LIME format to be passed to a Pipeline or sklearn\n    OneHotEncoder\n    \"\"\"\n    # If the data isn't a dataframe, we need to be able to build it\n\n    if not isinstance(X, pd.DataFrame):\n\n        X_lime = pd.DataFrame(X, columns=col_names)\n\n    else:\n\n        X_lime = X.copy()\n\n    for k, v in categorical_names.items():\n\n        if not invert:\n\n            label_map = {\n\n                str_label: int_label for int_label, str_label in enumerate(v)\n            }\n\n        else:\n\n            label_map = {\n\n                int_label: str_label for int_label, str_label in enumerate(v)\n\n            }\n\n        X_lime.iloc[:, k] = X_lime.iloc[:, k].map(label_map)\n\n    return X_lime","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"3b7ced44d394ba88644b6d38f76e79525696e627"},"cell_type":"code","source":"convert_to_lime_format(X_train, categorical_names).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4115afc413b83ce5200258b73764e386e3818474"},"cell_type":"code","source":"explainer = LimeTabularExplainer(convert_to_lime_format(X_train, categorical_names).values,\n                                 mode=\"classification\",\n                                 feature_names=X_train.columns.tolist(),\n                                 categorical_names=categorical_names,\n                                 categorical_features=categorical_names.keys(),\n                                 discretize_continuous=True,\n                                 random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66d8c03c1344f3889c6ba61b1576db1ea6db1673"},"cell_type":"markdown","source":"Great, our explainer is ready. Now let's pick an observation we want to explain."},{"metadata":{"_uuid":"092ed04c0e80b017fe6b4a84a1e03598277e8150"},"cell_type":"markdown","source":"`Explain new observations`\n\nWe'll create a variable called observation that contains our ith observation in the test dataset."},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"25a536b4dca09a1fd957dec0d1f5e18b2b45eb33"},"cell_type":"code","source":"i = 2\nX_observation = X_test.iloc[[i], :]\nX_observation","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"62593f88f2c9cf8840de094b8ebdfb4113ba7bc9"},"cell_type":"markdown","source":"Compare every model predictions for a specific client "},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"bb39000a5d870d2d394d3d5e7d533bc9628542d5"},"cell_type":"code","source":"print(f\"\"\"\\\n* True label: {y_test.iloc[i]}\n* LR: {lr_model.predict_proba(X_observation)[0]}\n* DT: {dt_model.predict_proba(X_observation)[0]}\n* RF: {rf_model.predict_proba(X_observation)[0]}\n\"\"\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33d36b689b2d256a94c2c3315926abc5cd2cd14e"},"cell_type":"markdown","source":"So for this particular client,its true lable is 0 i.e he/she didnt subscribe, Random Forest seems to be the predicting with more confidence than any other model "},{"metadata":{"_uuid":"4dea6796de58b172bfdb211598573741153f0ed2"},"cell_type":"markdown","source":"Let's convert our observation to lime format and convert it to a numpy array."},{"metadata":{"trusted":true,"_uuid":"f44f529ac84ff1a779899f06bcea59fea7d24e39"},"cell_type":"code","source":"observation = convert_to_lime_format(X_test.iloc[[i], :],categorical_names).values[0]\nobservation","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ecae4f1d79eda9131919a3eaf18a084c9b8c769e"},"cell_type":"markdown","source":"In order to explain a prediction, we use the explain_instance method on our explainer. This will generate new data with perturbated features around the observation and learn a local linear model. It needs to take:\n*  >our observation as a numpy array\n*  >a function that uses our model to predict probabilities given the data (in same format we've passed in our explainer). That means we cannot pass directly our rf_model.predict_proba because our pipeline expects string labels for categorical values. We will need to create a custom function rf_predict_proba that first converts back integer labels to strings and then calls rf_model.predict_proba.\n* >num_features: number of features to consider in explanation"},{"metadata":{"trusted":true,"_uuid":"72c214b4040fa910e82c78a13d5ff54b04f5ad3d"},"cell_type":"code","source":"# Let write a custom predict_proba functions for our models:\nfrom functools import partial\n\ndef custom_predict_proba(X, model):\n    X_str = convert_to_lime_format(X, categorical_names, col_names=X_train.columns, invert=True)\n    return model.predict_proba(X_str)\n\nlr_predict_proba = partial(custom_predict_proba, model=lr_model)\ndt_predict_proba = partial(custom_predict_proba, model=dt_model)\nrf_predict_proba = partial(custom_predict_proba, model=rf_model)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a3f08e8372296b03866ca1c76fe155fd8f77371"},"cell_type":"markdown","source":"Let's test our custom function to make sure it generates propabilities properly"},{"metadata":{"trusted":true,"_uuid":"ac7efc2fd776efafdee1146e74962ec6d038ed32"},"cell_type":"code","source":"explanation = explainer.explain_instance(observation, lr_predict_proba, num_features=5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1671927a7bafe035906ed02f2e94ebd9cc217441"},"cell_type":"markdown","source":"Now that we have generated our explanation, we have access to several representations. The most useful one when working in a notebook is show_in_notebook.\nOn the left it shows the list of probabilities for each class, here the model classified our observation as 0 (non subsribed) with a high probability.\n>If you set show_table=True, you will see the table with the most important features for this observation on the right."},{"metadata":{"trusted":true,"_uuid":"d2c763fba58f00830053eb2aa415bec9712c398a"},"cell_type":"code","source":"explanation.show_in_notebook(show_table=True, show_all=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bbdca6d36c409d985af40110e16b3b35837bdd09"},"cell_type":"markdown","source":"LIME is fitting a linear model on a local perturbated dataset. You can access the coefficients, the intercept and the R squared of the linear model by calling respectively .local_exp, .intercept and .score on your explanation."},{"metadata":{"trusted":true,"_uuid":"292eb5c9deae521508101d342578d03a46b6a6b2"},"cell_type":"code","source":"print(explanation.local_exp)\nprint(explanation.intercept)\nprint(explanation.score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b4f4522551fd30052dfbd2b1579429456b01e3f"},"cell_type":"markdown","source":"If your R-squared is low, the linear model that LIME fitted isn't a great approximation to your model, which means you should not rely too much on the explanation it provides."},{"metadata":{"trusted":true,"_uuid":"8f633462199982e6c67f297c7600a4b4adc22087"},"cell_type":"code","source":"explanation = explainer.explain_instance(observation, dt_predict_proba, num_features=5)\nexplanation.show_in_notebook(show_table=True, show_all=False)\nprint(explanation.score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aadff14793ae7acf306bcc8c1183eb535fbbcfc6"},"cell_type":"code","source":"explanation = explainer.explain_instance(observation, rf_predict_proba, num_features=5)\nexplanation.show_in_notebook(show_table=True, show_all=False)\nprint(explanation.score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f23ad8554ae63869c58a054aba3323c0ae2e1ea7"},"cell_type":"markdown","source":"Here we can see how each of features influence the model to choose and decide the probability score"},{"metadata":{"_uuid":"2a64ea7b23d941de213823ec3a705dd37d8c6606"},"cell_type":"markdown","source":"<a id='it'></a>\n## 4.2 Permutation Importance"},{"metadata":{"_uuid":"ff1e6f4410d2a992720333f66a08c9ab8338b059"},"cell_type":"markdown","source":"The idea of calculating feature_importances is simple, but great.\nSplitting down the idea into easy steps:\n1. train random forest model (assuming with right hyper-parameters)\n2. find prediction score of model (call it benchmark score)\n3. find prediction scores p more times where p is number of features, each time randomly shuffling the column of i(th) feature\n4. compare all p scores with benchmark score. If randomly shuffling some i(th) column is hurting the score, that means that our model is bad without that feature.\n5. remove the features that do not hurt the benchmark score and retrain the model with reduced subset of features."},{"metadata":{"_uuid":"fa95be99fc5daa65630ae8b906437aef12f5df32"},"cell_type":"markdown","source":"Permutation importance for Random forest from scratch "},{"metadata":{"trusted":true,"_uuid":"75f5d632f0338641ffada96d26fd65bddd370822"},"cell_type":"code","source":"# defining roc under the curve as scoring criteria (any other criteria can be used in a similar manner)\ndef score(x1,x2):\n    return metrics.auc(x1,x2)\n# defining feature importance function based on above logic\ndef feat_imp(m, x, y, small_good = True): \n\n     \"\"\"\n       m: random forest model\n       x: matrix of independent variables\n       y: output variable\n       small__good: True if smaller prediction score is better\n     \"\"\"  \n\n     score_list = {} \n\n     score_list['original'] = score(m.predict(x.values), y) \n\n     imp = {} \n\n     for i in range(len(x.columns)): \n\n            rand_idx = np.random.permutation(len(x)) # randomization\n\n            new_coli = x.values[rand_idx, i] \n\n            new_x = x.copy()            \n\n            new_x[x.columns[i]] = new_coli \n\n            score_list[x.columns[i]] = score(m.predict(new_x.values), y) \n\n            imp[x.columns[i]] = score_list['original'] - score_list[x.columns[i]] # comparison with benchmark\n\n     if small_good: \n          return sorted(imp.items(), key=lambda x: x[1]) \n\n     else: return sorted(imp.items(), key=lambda x: x[1], reverse=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ccb6bf437d98f654c5146555e7ca9fa9010b25b"},"cell_type":"markdown","source":"Suprisingly ELI5 also has an api for random forest model feature importance"},{"metadata":{"trusted":true,"_uuid":"f20d9e7803b10662c4b2aef0c88a20a987720b0c"},"cell_type":"code","source":"eli5.show_weights(rf_model.named_steps[\"model\"], \n                  feature_names=all_features)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e67ddbc4d162f3093d8974874a3dce5b8bc071a2"},"cell_type":"markdown","source":"We can explain roughly what our model seems to focus on mostly. We also get the standard deviation of feature importance accross the multiple trees in our ensemble."},{"metadata":{"_uuid":"9e8aa8f24f6cb3277e495c70a1adb8fa6838d0ae"},"cell_type":"markdown","source":"<a id='itw'></a>\n## 4.3 Removing Redundant Features"},{"metadata":{"trusted":true,"_uuid":"a83625940a4e6eed4bbe4590a3c0b7e27366b2a8"},"cell_type":"code","source":"from scipy.cluster import hierarchy as hc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0b2345289d582cf9be962f0ea863532444687d2"},"cell_type":"code","source":"corr = np.round(scipy.stats.spearmanr(X).correlation, 4)\ncorr_condensed = hc.distance.squareform(1-corr)\nz = hc.linkage(corr_condensed, method='average')\nfig = plt.figure(figsize=(16,10))\ndendrogram = hc.dendrogram(z, labels=X.columns, orientation='left', leaf_font_size=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53dacb8c56c9066c59b80e074a3d9756c0bb71b2"},"cell_type":"markdown","source":"We can remove  some of these related features to see if the model can be simplified without impacting the accuracy.This is called aggloremative clustering,which can be used to remove redudant features from you model and train  again with less features to see whether it will impact your accuracy"},{"metadata":{"_uuid":"cec3ce94ed0be472e4c9a53c98643447a47a05dc"},"cell_type":"markdown","source":"<a id='ts'></a>\n## 4.4 Partial Dependence Plots"},{"metadata":{"_uuid":"e129fc7b5621b3087b48826f5ffed698f5e9a19a"},"cell_type":"markdown","source":" Partial dependence plots that can be viewed as graphical representation of linear model coefficients, but can be extended to seemingly black box models also. The idea is to isolate the changes made in predictions to solely come from a specific feature. It is different than scatter plot of X vs. Y as scatter plot does not isolate the direct relationship of X vs. Y and can be affected by indirect relationships with other variables on which both X and Y depend."},{"metadata":{"_uuid":"9ca7ee31db10fbdc199a027094eddb001e525c19"},"cell_type":"markdown","source":"`Steps for PDP Plots"},{"metadata":{"_uuid":"e2dcf7fc986dac8596c329f0de04f07a99e457ba"},"cell_type":"markdown","source":"The steps to make PDP plot are as follows:\n1. Train a random forest model (let’s say F1…F4 are our features and Y is target variable. Suppose F1 is the most important feature). \n2. we are interested to explore the direct relationship of Y and F1\n3. replace column F1 with F1(A) and find new predictions for all observations. take mean of predictions. (call it base value)\n4. repeat step 3 for F1(B) … F1(E), i.e. for all distinct values of feature F1. \n5. PDP’s X-axis has distinct values of F1 and Y-axis is change in mean prediction for that F1 value from base value."},{"metadata":{"trusted":true,"_uuid":"04e8322732fe5da61e8cd323f476bbf5ff8ff8b5"},"cell_type":"markdown","source":"## MORE TO COME STAY TUNED "},{"metadata":{"trusted":true,"_uuid":"9a5c555e3c152cff626f6657214f9f9069409cd0"},"cell_type":"markdown","source":""},{"metadata":{"trusted":true,"_uuid":"9f509b689af101c9fcc3c90508de2a034cd172a8"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"14c6144c0e01d7513157e6ca3d839760f09efad1"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}