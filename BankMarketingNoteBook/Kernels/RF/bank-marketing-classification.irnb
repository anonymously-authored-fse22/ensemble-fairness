{"cells":[{"metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","trusted":true},"cell_type":"code","source":"library(readr)\nlibrary(tidyverse)\nlibrary(GGally)\nlibrary(glmnet)\nlibrary(Matrix)\nlibrary(randomForest)\nset.seed(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this project, we look at the \"Bank Marketing\" UCI dataset (individual lelve). The classification goal is to predict if the client will subscribe a bank term deposit (variable y - yes or no) using 20 predictors.\n\nWe compare 6 different models to predict our binary outcome: logistic regression, lasso, ridge, logistic with forward selection, random forrest, and bagging. These 6 model are good candidates to compare to each other when predicting a binary outcome with both categorical and quantitative predictors. We start by showing some exploratory data analysis, then we compute the null error rate, and then last we use our models. We cross validate all of the regression models, as well as compare the importance the variables between each of our models.\n\n**###Exploratory data analysis**"},{"metadata":{"trusted":true},"cell_type":"code","source":"set.seed(1)\ndata.df <- read_delim(\"../input/bank-additional-full.csv\", \n    \";\", escape_double = FALSE, trim_ws = TRUE)\ndata.df <- data.df %>%\n  mutate(y = ifelse(y==\"yes\", 1, 0))\ndim(data.df)\nsummary(data.df)\nnames(data.df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Respone variable: y - has the client subscribed a term deposit? (binary: \"yes\",\"no\"). Let's calculate the null rate."},{"metadata":{"trusted":true},"cell_type":"code","source":"table(data.df$y)\nwith(data.df, table(y))\nnull.err = length(data.df$y[data.df$y==1]) / length(data.df$y)\nnull.err","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at some graphs"},{"metadata":{},"cell_type":"markdown","source":"Marital: It doesn't seem like there is a big difference between the no and yes for marital status. "},{"metadata":{"trusted":true},"cell_type":"code","source":"mytable <- table(data.df$marital, data.df$y)\ntab <- as.data.frame(prop.table(mytable, 2))\ncolnames(tab) <-  c(\"marital\", \"y\", \"perc\")\n\nggplot(data = tab, aes(x = marital, y = perc, fill = y)) + \n  geom_bar(stat = 'identity', position = 'dodge', alpha = 2/3) + \n    xlab(\"Marital\")+\n    ylab(\"Percent\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The more educated people are the more they will have a yes for bank deposit.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"mytable <- table(data.df$education, data.df$y)\ntab <- as.data.frame(prop.table(mytable, 2))\ncolnames(tab) <-  c(\"education\", \"y\", \"perc\")\n\nggplot(data = tab, aes(x = education, y = perc, fill = y)) + \n  geom_bar(stat = 'identity', position = 'dodge', alpha = 2/3) + \n    xlab(\"Education\")+\n    ylab(\"Percent\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"May seems to be the time when they do the most calls for marketing deposit.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"mytable <- table(data.df$month, data.df$y)\ntab <- as.data.frame(prop.table(mytable, 2))\ncolnames(tab) <-  c(\"month\", \"y\", \"perc\")\n\nggplot(data = tab, aes(x = month, y = perc, fill = y)) + \n  geom_bar(stat = 'identity', position = 'dodge', alpha = 2/3) + \n    xlab(\"Month\")+\n    ylab(\"Percent\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Successful marketing have a higher median and quartile ranges of the duration of call. Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. However, I included duration in my predictive model anyway for this project"},{"metadata":{"trusted":true},"cell_type":"code","source":"ggplot(data.df, aes(x=as.factor(y), y=duration)) +\n  geom_boxplot(fill='#A4A4A4', color=\"black\")+\n  theme_classic()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.df %>%\n  ggplot() +\n  geom_point(aes(x=age, y=cons.price.idx, color=as.factor(y))) + ylab(\"consumer price index (monthly)\")\ndata.df %>%\n  ggplot() +\n  geom_point(aes(x=duration, y=nr.employed, color=as.factor(y))) + ylab(\"number of employees - quarterly indicator\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**###Logistic regression**"},{"metadata":{},"cell_type":"markdown","source":"Missing values for duration were filtered out (last contact duration, in seconds (numeric)) because if duration=0 then y=\"no\" (no call was made). Thus, it doesn't make sense to have 0 second duration. \nI also filtered out education illiterate, and default yes because they only have 1 observation each. We can't predict these situations if they happen to be in the test data but not the train data. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.df <- read_delim(\"../input/bank-additional-full.csv\", \n    \";\", escape_double = FALSE, trim_ws = TRUE)\ndata.df <- data.df %>%\n  dplyr::select(-pdays, - loan) %>%\n  filter(duration != 0, education != \"illiterate\", default != \"yes\") %>%\n  mutate(y = ifelse(y==\"yes\", 1, 0))\n\n#Create test train dataset\nset.seed(1)\n(num<-nrow(data.df))\ntrain<-sample(1:num,num/2,rep=F)\ntrain.df<-data.df[train,]\ntest.df<-data.df[-train,]\n\ndim(train.df)\ndim(test.df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logistic regression with all variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"mod.log <- glm(y ~ ., data = train.df, family=binomial)\nsummary(mod.log)\nprobs <- predict(mod.log,data=train.df,type=\"response\")\nhead(probs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot the probabilities"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.df <- mutate(train.df,prob.log = probs)\nnames(train.df)\n\ntrain.df %>% \n  ggplot()+\n  geom_jitter(aes(duration,y),\n              height=.1,size=.1,color=\"blue\")+\n  geom_point(aes(duration,prob.log),height=.1,size=.1,color=\"red\") +\n  geom_hline(yintercept = 1,color=\"black\")+\n  geom_hline(yintercept = 0,color=\"black\")+\n  ggtitle(\"y by duration (train df)\") \n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Error rate\n\nIt seems logistic regression with all predictors did pretty badly with an error rate higher than the null rate. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"thresh.pred <-.5\nprobs <- predict(mod.log,newdata=test.df,type=\"response\")\n\ntest.df<-test.df %>% \n  mutate(pred.log= ifelse(probs < thresh.pred ,0,1))\nwith(test.df, table(y, pred.log))\n(err.log <- with(test.df, mean(!y == pred.log))) ##test error rate\nc(err.log, null.err)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems logistic regression with all predictors did a little bit better than the null rate. \n\nGrab the statistically significant variables. "},{"metadata":{"trusted":true},"cell_type":"code","source":"pvals <- summary(mod.log)$coefficients[,4]\nsigPVals <- data.frame(pvals<0.05)\nsigPVals$var = rownames(sigPVals)\nsigPVals.df <- sigPVals %>% \n  filter(pvals...0.05 == TRUE) %>%\n  dplyr::select(-`pvals...0.05`)\nsigPVals.df$var","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's use all the significant variables only"},{"metadata":{"trusted":true},"cell_type":"code","source":"mod.log2 <- glm(y ~ job + default + contact + month + day_of_week + duration + campaign + poutcome + emp.var.rate + cons.price.idx + cons.conf.idx + nr.employed, data = train.df, family=binomial)\nsummary(mod.log2)\nprobs2 <- predict(mod.log2,data=train.df,type=\"response\")\nhead(probs2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"thresh.pred <-.5\nprobs2 <- predict(mod.log2,newdata=test.df,type=\"response\")\n\ntest.df<-test.df %>% \n  mutate(pred.log2= ifelse(probs2 < thresh.pred ,0,1))\nwith(test.df, table(y, pred.log2))\n(err.log2 <- with(test.df, mean(!y == pred.log2))) ##test error rate\nc(err.log, err.log2, null.err)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems using significant variables produces better error rate. It looks like we overfit the initial logistic model."},{"metadata":{},"cell_type":"markdown","source":"**###Ridge model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"mat <- model.matrix(y ~ .+0,data=data.df)\ndim(mat)\n\n(num<-nrow(data.df))\ntrain<-sample(1:num,num/2,rep=F)\n\nx.train<-mat[train,]\ny.train <- as.matrix(data.df[train,19])\nx.test<-mat[-train,]\ny.test <- as.matrix(data.df[-train,19])\n\ndim(x.train)\ndim(y.train)\ndim(x.test)\ndim(y.test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mod.ridge.cv <- cv.glmnet(x.train,y.train,alpha=0,intercept=T, family=\"binomial\")\nlambda.ridge <- mod.ridge.cv$lambda.min\n\nmod.ridge <- glmnet(x.train,y.train,alpha=0,intercept=T, lambda=lambda.ridge, family=\"binomial\")\npred.ridge <- predict(mod.ridge,newx=x.test,type = \"class\")\ntable(y.test, pred.ridge)\n\n(err.ridge <- mean(y.test!=pred.ridge))\n\n#str(mod.ridge)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lasso model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"mod.lasso.cv <- cv.glmnet(x.train,y.train,alpha=1,intercept=T, family=\"binomial\")\nlambda.lasso <- mod.lasso.cv$lambda.min\n\nmod.lasso <- glmnet(x.train,y.train,alpha=1,intercept=T, lambda=lambda.lasso, family=\"binomial\")\npred.lasso <- predict(mod.lasso,newx=x.test,type = \"class\")\ntable(y.test, pred.lasso)\n\n(err.lasso <- mean(y.test!=pred.lasso))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"betaVals<-mod.lasso$beta\ndim(betaVals)\nprint(data.matrix(betaVals))\nbetaNonZero <- betaVals >0\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some important variables chosen by Lasso are cons.price.idx, monthmar, poutcomesuccess."},{"metadata":{"trusted":true},"cell_type":"code","source":"numLambda<-10\nlambda.grid<-10^seq(-2,2,length=numLambda)\nmod.lasso<-glmnet(x.train,y.train,alpha=1,intercept=F,lambda=lambda.grid)\nplot(mod.lasso)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set.seed(1)\ndata.df$job = as.factor(data.df$job)\ndata.df$marital = as.factor(data.df$marital)\ndata.df$education = as.factor(data.df$education)\ndata.df$default = as.factor(data.df$default)\ndata.df$housing = as.factor(data.df$housing)\ndata.df$contact = as.factor(data.df$contact)\ndata.df$month = as.factor(data.df$month)\ndata.df$day_of_week = as.factor(data.df$day_of_week)\ndata.df$poutcome = as.factor(data.df$poutcome)\n\np <- ncol(data.df)-1\n\n(num<-nrow(data.df))\n\n\ntrain<-sample(1:num,num/2,rep=F)\ntrain.df<-data.df[train,]\ntest.df<-data.df[-train,]\ndata.rf <- randomForest(as.factor(y)~.,\n                        data=train.df, ##use all the predictors\n                        mtry=p/3, ##use 1/3 the predictors\n                        ntree=500,\n                        importance=T) ## 500 trees\n#data.bag\n#plot(data.bag)\npreds.rf <- predict(data.rf,newdata=test.df)\nwith(test.df,table(y,preds.rf))\n(err.rf <- with(test.df,mean(y!=preds.rf)))\n  \nvarImpPlot(data.rf)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**###Bagging**"},{"metadata":{"trusted":true},"cell_type":"code","source":"p <- ncol(train.df)-1\n#(num<-nrow(data.df))\nset.seed(1)\n\n\ntrain<-sample(1:num,num/2,rep=F)\ntrain.df<-data.df[train,]\ntest.df<-data.df[-train,]\ndata.bag <- randomForest(as.factor(y)~.,\n                         data=train.df, ##use all the predictors\n                         mtry=p, ##use all the predictors\n                         ntree=500,\n                         importance=T) ## 500 trees\n#data.bag\n#plot(data.bag)\npreds.bag <- predict(data.bag,newdata=test.df)\nwith(test.df,table(y,preds.bag))\n(err.bag <- with(test.df,mean(y!=preds.bag)))\n\nvarImpPlot(data.bag)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**###Compare error rate**"},{"metadata":{},"cell_type":"markdown","source":"It seems random forest and bagging are best at predicting the success of bank marketing calls.\n\nIt is also interesting to see that Lasso and Random Forest have different choices of variables.\nLasso found the month (march) and the p-outcome success, and the housing unknown to have the highest coefficients, which is different from both bagging, and random forrest. Furthermore, duration, nr.employed, and euribor3m are some of the most important variables in the bagging and random forrest models, but do not have high coefficients in the lasso model. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nnull.err\nerr.log\nerr.log2\nerr.lasso\nerr.ridge\nerr.rf\nerr.bag\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"}},"nbformat":4,"nbformat_minor":1}